{"cells":[{"cell_type":"markdown","source":["#**PHASE D'AMORCE**"],"metadata":{"id":"jmtbvkjFFu9I"}},{"cell_type":"markdown","metadata":{"id":"tLABAWSWT_kg"},"source":["## **INITIALISATION**"]},{"cell_type":"code","source":["# Montage de Google Drive dans l'environnement Colab afin d'accéder aux fichiers stockés sur le cloud.\n","# La fonction 'drive.mount()' permet d'établir une connexion avec le Google Drive de l'utilisateur\n","# et de le rendre accessible sous le chemin '/content/drive/' dans Colab.\n","#\n","# 'folder_path' définit le chemin d'accès au dossier spécifique sur Google Drive où les fichiers\n","# nécessaires à l'analyse seront stockés ou récupérés. Ce chemin sera utilisé pour lire et écrire des fichiers.\n","from google.colab import drive\n","drive.mount('/content/drive/')\n","folder_path = '/content/drive/MyDrive/COLAB_NLP/'"],"metadata":{"id":"uDivY5Y63fLA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741769173466,"user_tz":-60,"elapsed":26253,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"3fae12a1-9ba7-4e73-d1bd-732b3a8eda41"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"code","source":["PX_PER_TOPIC = 60      # Hauteur en pixels par topic\n","DPI = 72*4              # Résolution : 300 dpi\n","FIGURE_WIDTH_INCH = 6.30 # A4 AVEC 2.5CM DE MARGES #4.02 UN GALLIMARD AVEC 2.4CM DE MARGES # Largeur fixe (en pouces) que vous souhaitez"],"metadata":{"id":"Kz3aCJkxkSdX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if '/content/drive/' not in folder_path:\n","    !conda install matplotlib -y"],"metadata":{"id":"NOrMxpJRDpSC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib\n","from matplotlib import font_manager, rcParams\n","\n","matplotlib.rcParams['font.size'] = 10.5\n","\n","# Spécifiez le chemin vers le fichier de police\n","font_path = folder_path + \"Roboto/XCharter-Roman.otf\"\n","font_manager.fontManager.addfont(font_path)\n","xcharter_font = font_manager.FontProperties(fname=font_path)\n","matplotlib.rcParams['font.family'] = xcharter_font.get_name()"],"metadata":{"id":"3hO-36U0IBcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wqt0GBn7Vpj0"},"outputs":[],"source":["# Actuellement, seuls les langues suivantes sont prises en charge :\n","# français, anglais, espagnol, allemand, catalan, chinois, danois, japonais, slovaque, et ukrainien.\n","# Codes linguistiques correspondants : fr, en, es, de, ca, zh, da, ja, sl, uk.\n","# Attention : tout changement de langue oblige à relancer la fonction \"download_things\" ci-dessous.\n","language = 'en'"]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IUBcYK8jGcQE","executionInfo":{"status":"ok","timestamp":1741711606734,"user_tz":-60,"elapsed":10969,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"e2e66125-a806-4871-bdd7-90ee381ff64f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n","Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m57.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scipy, gensim\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.14.1\n","    Uninstalling scipy-1.14.1:\n","      Successfully uninstalled scipy-1.14.1\n","Successfully installed gensim-4.3.3 scipy-1.13.1\n"]}]},{"cell_type":"code","source":["!pip install gensim"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GOZphfig2U_Q","executionInfo":{"status":"ok","timestamp":1741769226998,"user_tz":-60,"elapsed":9731,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"f2f3a7ba-5eea-41b3-fc45-9d188bbb2dd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting gensim\n","  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.11/dist-packages (from gensim) (1.26.4)\n","Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n","  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n","Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m81.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: scipy, gensim\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.14.1\n","    Uninstalling scipy-1.14.1:\n","      Successfully uninstalled scipy-1.14.1\n","Successfully installed gensim-4.3.3 scipy-1.13.1\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EtUDkn0mNFAF","executionInfo":{"status":"ok","timestamp":1741769248254,"user_tz":-60,"elapsed":21253,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ba0b0025-ed33-4468-fdba-58d7a7bfe8fc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Vérification de seaborn…\n","✔ seaborn est prête.\n","Vérification de sklearn…\n","✔ sklearn est prête.\n","Vérification de scipy…\n","✔ scipy est prête.\n","Vérification de spacy…\n","✔ spacy est prête.\n","Vérification de pyate…\n","✔ pyate est prête.\n","Vérification de bs4…\n","✔ bs4 est prête.\n","Vérification de unidecode…\n","✔ unidecode est prête.\n","Vérification de charset_normalizer…\n","✔ charset_normalizer est prête.\n","Vérification de datasketch…\n","✔ datasketch est prête.\n","Vérification de tslearn…\n","✔ tslearn est prête.\n","Vérification de ortools…\n","✔ ortools est prête.\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n","\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n","If you are in a Jupyter or Colab notebook, you may need to restart Python in\n","order to load all the package's dependencies. You can do this by selecting the\n","'Restart kernel' or 'Restart runtime' option.\n"]},{"output_type":"stream","name":"stderr","text":["Exception ignored on calling ctypes callback function: <function ThreadpoolController._find_libraries_with_dl_iterate_phdr.<locals>.match_library_callback at 0x7f1c571b6840>\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1005, in match_library_callback\n","    self._make_controller_from_path(filepath)\n","  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 1175, in _make_controller_from_path\n","    lib_controller = controller_class(\n","                     ^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/threadpoolctl.py\", line 114, in __init__\n","    self.dynlib = ctypes.CDLL(filepath, mode=_RTLD_NOLOAD)\n","                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/ctypes/__init__.py\", line 376, in __init__\n","    self._handle = _dlopen(self._name, mode)\n","                   ^^^^^^^^^^^^^^^^^^^^^^^^^\n","OSError: /usr/local/lib/python3.11/dist-packages/scipy.libs/libscipy_openblas-c128ec02.so: cannot open shared object file: No such file or directory\n"]}],"source":["nlp_pipeline = None\n","file_to_run = f\"{folder_path}lexical_topic_modeling_backend.ipynb\"\n","%run \"$file_to_run\""]},{"cell_type":"markdown","metadata":{"id":"uPy-S4HFDH1P"},"source":["## **DÉFINITION DES PARAMÈTRES**"]},{"cell_type":"code","source":["# **grammatical_classes** : list[str], optionnel\n","#     Une liste de classes grammaticales à conserver lors de la construction des vecteurs TF-IDF.\n","#     Les classes doivent correspondre aux catégories grammaticales reconnues par spaCy.\n","#     Par défaut, les classes `['NOUN', 'PROPN', 'VERB']` sont utilisées, ciblant les noms communs,\n","#     noms propres et verbes.\n","#\n","# ### Classes grammaticales reconnues par spaCy :\n","# ------------------------------------------\n","# - 'NOUN' : Noms communs\n","# - 'PROPN' : Noms propres\n","# - 'VERB' : Verbes\n","# - 'ADJ' : Adjectifs\n","# - 'ADV' : Adverbes\n","# - 'PRON' : Pronoms\n","# - 'DET' : Déterminants (articles)\n","# - 'ADP' : Prépositions ou postpositions\n","# - 'CONJ' : Conjonctions de coordination\n","# - 'CCONJ' : Conjonctions de coordination (UD)\n","# - 'SCONJ' : Conjonctions de subordination (UD)\n","# - 'AUX' : Verbes auxiliaires (ex. \"être\", \"avoir\")\n","# - 'PART' : Particules\n","# - 'INTJ' : Interjections\n","# - 'NUM' : Nombres\n","# - 'SYM' : Symboles\n","# - 'X' : Autres (inclassables)\n","#\n","# ### Retourne :\n","# ---------\n","# **tuple** :\n","#     Un tuple contenant les résultats de la vectorisation TF-IDF :\n","#     - **tfidf_vectorizer** : L'objet `TfidfVectorizer` utilisé pour la vectorisation.\n","#     - **tfidf** : Une matrice sparse TF-IDF des documents vectorisés.\n","#     - **tfidf_feature_names** : La liste des unigrams sélectionnés comme caractéristiques.\n","#     - **tokenized_documents** : Une version tokenisée et filtrée des documents d'entrée, limitée\n","#       aux classes grammaticales spécifiées.\n","#\n","# ### Exemple :\n","# ---------\n","# Pour effectuer une vectorisation TF-IDF sur les catégories grammaticales 'NOUN' et 'PROPN' :\n","# grammatical_classes = ['NOUN', 'PROPN']\n","grammatical_classes = ['NOUN', 'PROPN', 'VERB']"],"metadata":{"id":"q-C1iPquO3Th"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CZJeY61n_92x"},"outputs":[],"source":["# La variable `threshold` fixe le seuil minimal d'articles requis pour qu'un journal\n","# soit inclus dans certaines analyses spécifiques :\n","# - Les journaux ayant un nombre d'articles inférieur à ce seuil ne seront pas pris\n","#   en compte dans les visualisations portant sur les journaux (exemple : graphiques\n","#   comparant des volumes de publications ou des analyses statistiques spécifiques).\n","# - Cela n'exclut pas ces journaux du corpus pour d'autres types d'analyse, comme le topic modeling,\n","#   où tous les articles restent intégrés indépendamment de leur origine.\n","#\n","# Ce seuil permet de concentrer les analyses et tests statistiques sur des journaux\n","# suffisamment représentatifs, en évitant que des titres marginalement présents\n","# introduisent du bruit dans les résultats.\n","threshold = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OqQlVOQP-0D4"},"outputs":[],"source":["# Ces paramètres s'appliquent principalement aux corpus issus d'Europresse.\n","# Par exemple, les articles contenant moins de 500 caractères ou plus de 100 000 caractères\n","# sont considérés comme suspects et sont donc supprimés du corpus.\n","#\n","# Ces valeurs doivent être ajustées en fonction du type de corpus analysé,\n","# car elles peuvent varier significativement selon les sources et les objectifs de l'étude.\n","#\n","# Pour les corpus provenant d'Europresse, l'algorithme élimine systématiquement les articles\n","# dont le nombre de caractères est inférieur à `minimum_caracters_nb_by_document` ou supérieur à\n","# `maximum_caracters_nb_by_document`. Cette approche repose sur trois considérations :\n","#\n","# 1. **Suspicion de contenu non standard** : Un article comportant moins de 500 caractères ou\n","#    plus de 100 000 caractères s'écarte des normes usuelles. Ces cas extrêmes suggèrent des anomalies\n","#    (ex. : méta-données mal extraites, textes hors contexte ou dégradés).\n","#\n","# 2. **Analyse thématique limitée** : Un article très court (moins de 100 ou 200 caractères,\n","#    soit environ vingt mots) ne fournit pas suffisamment de matière pour une analyse lexicale\n","#    ou thématique pertinente. Cela revient à une situation comparable au rejet d'une analyse\n","#    du chi² pour des croisements avec des effectifs insuffisants.\n","#\n","# 3. **Limitation technique de l’algorithme de NER (Reconnaissance d’Entités Nommées)** :\n","#    Les outils de NER utilisés (par exemple, spaCy, Stanford NER, etc.) peuvent présenter\n","#    des contraintes de longueur. Au-delà de 100 000 caractères, l’algorithme risque de ne plus\n","#    pouvoir traiter correctement le document, entraînant un blocage technique. Le seuil\n","#    maximum de 100 000 caractères vise donc également à préserver la faisabilité et la\n","#    qualité de l’analyse NER.\n","minimum_caracters_nb_by_document = 300\n","maximum_caracters_nb_by_document = 100000000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wYtY3pyB-_rT"},"outputs":[],"source":["# Ce paramètre détermine si les doublons doivent être conservés ou non.\n","\n","\n","# - À False : les doublons sont conservés.\n","# - À True : les doublons sont supprimés.\n","#\n","# La valeur True est particulièrement pertinente pour les corpus issus d'Europresse.\n","# Elle permet de gérer plusieurs problèmes liés à la collecte des données :\n","# - Élimination des doublons potentiels causés par le fonctionnement spécifique d'Europresse.\n","# - Suppression des recouvrements partiels entre différents fichiers d'articles,\n","#   évitant ainsi les répétitions accidentelles.\n","# - Identification et retrait des articles produits par simple copié/collé,\n","#   grâce à un algorithme qui résiste aux variations mineures. Ainsi, les articles\n","#   n'apportant aucune information substantiellement nouvelle sont exclus du corpus.\n","#\n","# Toutefois, conserver les doublons (valeur False) peut également être justifié,\n","# car ces répétitions reflètent une certaine dynamique médiatique. Dans ce cas,\n","# le paramètre devra rester à False pour intégrer ces éléments au corpus.\n","go_remove_duplicates = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-KjNjyJ_P2D"},"outputs":[],"source":["# La variable `web_paper_differentiation` détermine si les versions papier et web\n","\n","# d'une même revue doivent être fusionnées ou conservées distinctes :\n","# - `False` : les versions papier et web sont fusionnées en un seul ensemble.\n","# - `True` : les versions papier et web sont traitées comme des entités distinctes.\n","#\n","# Ce choix dépend de la finalité de l'analyse :\n","# - Si l'objectif est de considérer un média dans son ensemble, sans distinction\n","#   entre ses formats, laissez cette variable à `False`.\n","# - Si l'on souhaite analyser les spécificités des contenus selon leur support\n","#   (papier ou web), définissez cette variable à `True`.\n","web_paper_differentiation = False"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hXT9yj4Z_e1R"},"outputs":[],"source":["# La variable `source_type` indique l'origine du corpus et ajuste le traitement des données en conséquence.\n","#\n","# Valeurs possibles pour `source_type` :\n","# - \"europresse\" : le corpus provient de la plateforme Europresse.\n","#   - Des prétraitements spécifiques sont appliqués pour gérer les particularités de cette source,\n","#     telles que le format des fichiers ou les métadonnées propres à Europresse.\n","# - \"istex\" : le corpus provient de la plateforme ISTEX.\n","#   - Des étapes adaptées à la structure des métadonnées et des documents fournis par ISTEX sont appliquées.\n","# - \"csv\" : le corpus provient d'un fichier CSV ou d'autres formats standards.\n","#   - Le séparateur du fichier doit être une virgule ou un point-virgule. Le fichier doit également\n","#     contenir a minima deux colonnes : une colonne \"text\" et une colonne \"date\".\n","#\n","# Le choix de `source_type` influence directement les étapes de prétraitement et d'analyse des données.\n","source_type = 'csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gWkHUM-O-ywf"},"outputs":[],"source":["# La variable `base_name` correspond au nom du corpus. Elle doit être modifiée de manière à\n","\n","# décrire précisément le corpus analysé, en suivant des bonnes pratiques de nommage.\n","# Exemple : \"ukraine_russie__presse_francilienne__fev2022_feb2023\".\n","#\n","# Le contenu de `base_name` doit être présent dans les noms des fichiers situés dans le dossier \"DATA\"\n","# et utilisés pour l'analyse. L'algorithme est conçu pour gérer des corpus répartis sur plusieurs fichiers :\n","# il suffit que tous les fichiers concernés contiennent le texte exact de `base_name` dans leur nom.\n","#\n","# Exemple de fichiers conformes :\n","# - \"ukraine_russie__presse_francilienne__fev2022_mars2024__feb2022_sep2022.HTML\"\n","# - \"ukraine_russie__presse_francilienne__fev2022_feb2023__oct2022_feb2023.HTML\"\n","#\n","# Ces fichiers seront inclus dans l'analyse dès lors qu'ils partagent le même `base_name`.\n","#\n","# De plus, les résultats de l'analyse seront stockés dans un dossier spécifique dont le chemin\n","# est défini par la variable `RESULTS_PATH`. Ce chemin inclut le `base_name` pour assurer une\n","# organisation cohérente des résultats.\n","base_name = 'good_swift'\n","results_path = folder_path + \"RESULTS_\" + base_name + \"/\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bgooqvcWUFJj"},"outputs":[],"source":["# Cette fonction crée un dossier de résultats basé sur le nom spécifié dans `base_name`.\n","\n","# Elle vérifie si le dossier existe déjà, et si ce n'est pas le cas, elle le crée.\n","# Le nom du dossier sera préfixé par \"RESULTS_\" et inclura le contenu de `base_name`.\n","# Elle définit également le nom du fichier CSV où les résultats seront sauvegardés.\n","create_results_folder(base_name)# CA PEUT PASSER DANS LA LECTURE DES DOCUMENTS"]},{"cell_type":"markdown","metadata":{"id":"llQhJTcDDNdk"},"source":["## **PRÉPARATION DES DOCUMENTS**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":138,"referenced_widgets":["6d411e848df049acba679d8f6ac021f4","b951c23a57b24298bbfc84a946219bd5","ff08f96f6c68498699fa5aa86e5c3f50","6c798065d3d34f33a5cb4ef3a86352a3","813a0096c0a14544bf8587197047d519","a24d702ff6494dac8b6ed48a464c7e06","40bbe11df03b4da3b011249554f5a4af","96e1596583854dd28f6f45d8018127b0","8e0f122d9b38442586646c7d54f2b905","c4f4c50162da4955884c79c8bf78db39","5945ec1d12e84726bb59acd843647bba"]},"id":"1ABNfjfms9Pk","outputId":"f339d61d-d32e-40f5-80bf-a24f394b14a4","executionInfo":{"status":"ok","timestamp":1741769666853,"user_tz":-60,"elapsed":30914,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["DOCUMENTS PROCESSÉS:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d411e848df049acba679d8f6ac021f4"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["[*] Taille du fichier CSV = 503.82 Mo\n","[*] Le fichier est > 200 Mo : lecture directe en UTF-8 (séparateur ';')\n","\n","\n","128812 documents\n"]}],"source":["# Initialisation des variables pour stocker les documents, les objets BeautifulSoup associés,\n","# et les métadonnées extraites des différentes sources de données (europresse, CSV, ISTEX).\n","#\n","# - 'documents' : liste qui contiendra les textes extraits des documents.\n","#    - Pour 'europresse', ce seront les textes nettoyés extraits des articles HTML.\n","#    - Pour 'csv', ce seront les contenus des colonnes 'text' ou 'description' des fichiers CSV filtrés par le nombre de caractères.\n","#    - Pour 'istex', ce seront les contenus texte extraits des fichiers `.txt` associés aux fichiers `.json` correspondants.\n","documents = []\n","\n","# - 'all_soups' : liste qui contiendra les objets BeautifulSoup pour chaque document.\n","#    Cela est utilisé pour manipuler et analyser la structure HTML des documents extraits, en particulier pour 'europresse',\n","#    où chaque document est transformé en un objet BeautifulSoup afin de procéder à des nettoyages supplémentaires (par exemple, suppression de certains éléments HTML).\n","all_soups = []\n","\n","# - 'columns_dict' : dictionnaire qui stocke les métadonnées liées à chaque document.\n","#    - Pour 'csv', cela contiendra les autres colonnes du fichier (autres que 'text' et 'description') que l'on souhaite conserver comme métadonnées.\n","#    - Pour 'istex', ce dictionnaire contiendra des champs supplémentaires extraits des fichiers JSON associés à chaque document, tels que 'doi', 'journal', 'date', etc.\n","columns_dict = {}\n","\n","# La fonction 'meta_load_documents()' est appelée pour charger et traiter les données des documents à partir de la source spécifiée\n","# (définie par 'source_type'). En fonction de cette source, les données sont extraites et stockées dans les structures ci-dessus pour un traitement ultérieur.\n","meta_load_documents()"]},{"cell_type":"code","source":["# Supposons que les variables soient déjà définies :\n","# documents = [...]\n","# all_soups = [...]\n","# columns_dict = {...}\n","\n","# On détermine les indices des documents \"valides\"\n","valid_indices = []\n","for i, doc in enumerate(documents):\n","    # Tokenisation basique\n","    tokens = doc.split()\n","    # Comptage des mots uniques\n","    if len(set(tokens)) >= 10:\n","        valid_indices.append(i)\n","\n","# On reconstruit la liste documents et all_soups\n","documents = [documents[i] for i in valid_indices]\n","if len(all_soups) > 0:\n","    all_soups = [all_soups[i] for i in valid_indices]\n","\n","# On reconstruit les listes dans columns_dict en fonction des indices conservés\n","for key in columns_dict:\n","    columns_dict[key] = [columns_dict[key][i] for i in valid_indices]"],"metadata":{"id":"GqN-BwsDh9AU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(documents))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y_aLyg0wiVAk","executionInfo":{"status":"ok","timestamp":1741769675216,"user_tz":-60,"elapsed":4,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"9390af94-a79c-454f-bdd8-850d77f43243"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["128623\n"]}]},{"cell_type":"markdown","source":["#**PHASE DE PRODUCTION DES MATRICES H ET W**"],"metadata":{"id":"KYISqZLGF6Wn"}},{"cell_type":"markdown","metadata":{"id":"y__k8itnLupq"},"source":["##**LEMMATISATION ET VECTORISATION**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZPAL-JPMyec","colab":{"base_uri":"https://localhost:8080/","height":85,"referenced_widgets":["d29f1fc02d964c8e8600308bf6d09dec","0dc48acef57f428f972e75ebfa50338e","6a52320dd96d4e0c89f9b16d75772ff0","e10268dc4c3e4c4e978c057af32147c2","5245b6f1cf074219aba13aadb968246f","bbdfb3a58d8f4388bfd1b771bab3bb9b","257f2a9bb9a7453dbb59546de453ad76","a5ac99fc9e7747808608e466b3fb6172","0c4c95e25a114e10beeab3b484b61cd3","c998fc39c42743e38f86e2b5ebae80d9","ff9c6da282f442359847c81b69cb8243"]},"outputId":"f8129932-ab39-4c6a-ab92-968a9cccc697","executionInfo":{"status":"ok","timestamp":1741770903565,"user_tz":-60,"elapsed":1198143,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["gpu_activated True\n","Utilisation de 1 processus parallèles pour spaCy.\n"]},{"output_type":"display_data","data":{"text/plain":["DOCUMENTS PROCESSÉS:   0%|          | 0/128623 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d29f1fc02d964c8e8600308bf6d09dec"}},"metadata":{}}],"source":["# La fonction process_documents traite une liste de documents en effectuant les opérations suivantes :\n","\n","\n","# 1. Chargement d'un modèle linguistique spaCy en fonction de la langue spécifiée.\n","# 2. Pour chaque document, elle effectue :\n","#    - La lemmatisation des tokens (avec normalisation des noms propres via unidecode).\n","#    - L'extraction des étiquettes de parties du discours (POS).\n","#    - La création de listes de mots lemmatisés et normalisés pour chaque phrase.\n","# 3. Les résultats sont stockés dans quatre listes :\n","#    - 'documents_lemmatized' (pour les n-grams),\n","#    - 'all_tab_pos' (mots et leurs POS par document),\n","#    - 'sentences_norms' (phrases normalisées),\n","#    - 'all_sentence_pos' (mots et leurs POS par phrase).\n","# 4. Un suivi de la progression est effectué via tqdm, et des erreurs sont loggées sans interrompre l'exécution.\n","documents_lemmatized = []\n","all_tab_pos = []\n","sentences_norms = []\n","all_sentence_pos = []\n","process_documents(documents)"]},{"cell_type":"markdown","metadata":{"id":"6xQNj6K3ZYL2"},"source":["##**TF-IDF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZFa9S_AOMZl","colab":{"base_uri":"https://localhost:8080/","height":81,"referenced_widgets":["6ed24d4ab2b34844ac740f610899f716","bdd47d4986834f368168d34776965aa5","2e43333161254325a19e8285edbb93ce","8247fe49192a47c88da052388b309c9a","993e44343c394e46996719dc596d9468","77968bf9e2d245959a8baaa74ac969ce","c0501456c0034c30984d2f2b2e6b0195","284ec8e15a2a4c839f436d8d20c8268c","6f477fbb55f140e9856f82f4d38ed312","634033e7fbe345c58d7dfd276a2f2789","2b5fb742bc2f42fcbdaa176eb3194b62","d95819ac1f024d1f825f373852b4f848","f2cb881e86a84b9d97fdd96ebd8509c8","72bf281e72104988b8af544dcef9cbe5","592368db4fcb41849664dc13fe64be77","556e919d627c4e13a3e4192a85cdf6e4","d10a1681a9e44f499be3a68f9e638f94","9a91c1a4c25049049eca8b933b191941","199d5c7aad2b4945b43d4459d594a2c7","660a0dda97ca416e8b97ce48adf066ed","0b05464c072c435fa7dc4d80172a633e","b6bf255c06a9413d8060331dddf1ccf5"]},"outputId":"068e9204-fc00-457b-9e22-03e1fd3f819a","executionInfo":{"status":"ok","timestamp":1741770965558,"user_tz":-60,"elapsed":61988,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}}},"outputs":[{"output_type":"display_data","data":{"text/plain":["Mise à jour des unigrams:   0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ed24d4ab2b34844ac740f610899f716"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Filtrage des stopwords:   0%|          | 0/128623 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d95819ac1f024d1f825f373852b4f848"}},"metadata":{}}],"source":["# Effectue une vectorisation TF-IDF des documents, en se concentrant sur des unigrams correspondant\n","# aux classes grammaticales spécifiées.\n","#\n","# Cette fonction applique une sélection basée sur les classes grammaticales définies par `grammatical_classes`\n","# afin de limiter la construction des vecteurs TF-IDF aux unigrams pertinents. Cela permet de réduire\n","# le bruit en excluant les mots non pertinents (par exemple, les déterminants ou les particules)\n","# et d'améliorer la qualité des résultats analytiques.\n","unigrams = {}\n","tfidf_vectorizer, tfidf, tfidf_feature_names, tokenized_documents, tfidf_transformer = go_tfidf_vectorization(grammatical_classes)"]},{"cell_type":"markdown","metadata":{"id":"RP_k5UM8lyID"},"source":["##**NMF**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QGZjdjA6WMUO","colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["261f2cae5c8a4622b7d4ad0d206dc27f","c69192f9e8af45bda4e4fdcd7010da96","966f1728dd0440fba83052119019a784","b262477dba9c4aef9cf44db10544ca4f","10ca8c888aab4e32a0f2937c3feeaaf0","300611a2ba2e420b9811e3221f5d3300","2ea5ee687ed947c2b92135749ed74b39","e5c9192050e440f0ba82630612efe4cd","072cce547935406fb348e5d49f8d216b","f21e890af6fc4c449429189c9ce8c44f","34b6531480074258ae4491bfa645de64"]},"executionInfo":{"status":"ok","timestamp":1741771117812,"user_tz":-60,"elapsed":152243,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"4920f716-9c43-4f99-dcdf-b742fc99950b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["PROCESSUS DES TOPICS:   0%|          | 0/4 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"261f2cae5c8a4622b7d4ad0d206dc27f"}},"metadata":{}}],"source":["# Initialisation des variables globales utilisées pour stocker les résultats de la factorisation NMF et les données associées.\n","\n","# all_nmf_H : Dictionnaire global où les matrices H (composantes thématiques) générées par le modèle NMF\n","# pour chaque nombre de topics sont stockées. La clé correspond au nombre de topics (e.g., 5, 10, etc.),\n","# et la valeur est la matrice H générée pour ce nombre de topics.\n","all_nmf_H = {}\n","\n","# all_nmf_W : Dictionnaire global où les matrices W (représentant les documents dans l'espace des topics)\n","# sont stockées. Chaque clé correspond à un nombre de topics, et chaque valeur est la matrice W associée.\n","all_nmf_W = {}\n","\n","# coherence_scores : Dictionnaire global associant à chaque nombre de topics un score de cohérence\n","# calculé par une métrique de type “fenêtre glissante” (par exemple c_npmi ou c_uci).\n","# Nous utilisons ici NMF, car il est fréquent qu'un même document soit associé à plusieurs topics.\n","# Dans cette situation, mesurer la qualité à l'échelle d'un document entier (doc-based) pourrait\n","# masquer des co-occurrences thématiques plus fines. Une approche en fenêtre glissante s'avère\n","# donc plus appropriée pour évaluer la cohérence locale des mots-clés liés à chaque topic.\n","coherence_scores = {}\n","\n","# Appel de la fonction determine_nmf avec une liste de nombres de topics à tester ([10, 15]),\n","# en spécifiant les nouveaux paramètres alpha_W, alpha_H et l1_ratio.\n","# La fonction accepte également n_top_words et window_size si besoin (avec des valeurs par défaut de 15 et 100).\n","# Exemple d’appel pour tester respectivement 10 et 15 topics :\n","# determine_nmf([5, 7, 10, 12, 15, 20], alpha_W=0.3, alpha_H=0.3, l1_ratio=0.0, n_top_words=15, window_size=100)\n","nmf_models = determine_nmf([7, 12, 15, 20], alpha_W=0.0, alpha_H=0.0, l1_ratio=0.0)"]},{"cell_type":"code","source":["print(coherence_scores) # IL FAUT LES METTRE SUR LE DISQUE"],"metadata":{"id":"0HFAjz00hQv4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1741771995816,"user_tz":-60,"elapsed":24,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"3c656b77-0271-46fa-9477-e65a7eb5270a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{7: 0.1367817414650049, 12: 0.15575413559944443, 15: 0.16931919902449516, 20: 0.15814851867080068}\n"]}]},{"cell_type":"code","source":["{7: 0.13518116928951168, 12: 0.12654753655246923, 15: 0.1666228032688502, 20: 0.15475903898431692}\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BVOFG0n9cMK7","executionInfo":{"status":"ok","timestamp":1741771997365,"user_tz":-60,"elapsed":17,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"b87bd636-3cd5-4e03-eb99-99bb77fb4b5b"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{7: 0.13518116928951168,\n"," 12: 0.12654753655246923,\n"," 15: 0.1666228032688502,\n"," 20: 0.15475903898431692}"]},"metadata":{},"execution_count":32}]},{"cell_type":"code","source":["# Ce script est conçu pour aider les analystes à extraire des phrases représentatives\n","# après l'entraînement d'un modèle NMF. L’objectif global est de sélectionner,\n","# pour chaque thème (ou topic) identifié par le NMF, un ensemble de phrases\n","# pertinentes et peu redondantes. Pour cela, le code met à jour des listes de\n","# termes unigrams, génère une matrice TF-IDF de l’ensemble des phrases, puis\n","# utilise les scores NMF pour repérer les phrases les plus discriminantes de chaque\n","# topic, en filtrant celles qui sont trop similaires entre elles. Les sorties,\n","# notamment les phrases et leurs scores de pertinence, permettent ensuite\n","# de réaliser des synthèses qualitatives sur les topics détectés.\n","extract_relevant_sentences(nmf_models)"],"metadata":{"id":"R_DwxX4qYdQD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Ce code enregistre sur le disque l'esnsemble des matrices H (topics-termes) et\n","# des matrices W (documents-topics) sur le disque. Il y a donc une matrice H et\n","# une matrice W par configuration (nombre de topics). Ce sont les seuls objets\n","# dont nous aurons besoin pour les analyses, en plus des données du corpus.\n","with open(results_path + base_name + '_RAW/all_nmf_W.pkl', 'wb') as f:\n","    pickle.dump(all_nmf_W, f)\n","\n","with open(results_path + base_name + '_RAW/all_nmf_H.pkl', 'wb') as f:\n","    pickle.dump(all_nmf_H, f)"],"metadata":{"id":"eCDROO8sYdNd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#**PHASE D'ANALYSES**"],"metadata":{"id":"HMoaOSrjGMRN"}},{"cell_type":"markdown","source":["## **RÉCUPÉRATION DES MATRICES H ET W ENREGISTRÉES SUR LE DISQUE**"],"metadata":{"id":"5Wft1gUeCar-"}},{"cell_type":"code","source":["# Récupération des matrices W et H depuis le disque.\n","with open(results_path + base_name + '_RAW/all_nmf_W.pkl', 'rb') as f:\n","    all_nmf_W = pickle.load(f)\n","\n","with open(results_path + base_name + '_RAW/all_nmf_H.pkl', 'rb') as f:\n","    all_nmf_H = pickle.load(f)"],"metadata":{"id":"gpGLrUbJlZ33","colab":{"base_uri":"https://localhost:8080/","height":216},"executionInfo":{"status":"error","timestamp":1741727225821,"user_tz":-60,"elapsed":177,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"outputId":"a1c528b5-d366-4d98-e396-63ce214790c9"},"execution_count":null,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/COLAB_NLP/RESULTS_good_swift/good_swift_RAW/all_nmf_W.pkl'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-3e164e015b80>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Récupération des matrices W et H depuis le disque.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbase_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_RAW/all_nmf_W.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mall_nmf_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults_path\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbase_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_RAW/all_nmf_H.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/COLAB_NLP/RESULTS_good_swift/good_swift_RAW/all_nmf_W.pkl'"]}]},{"cell_type":"markdown","metadata":{"id":"JhzO_mUDmG5y"},"source":["##**EXTRACTION DES PHRASES CARACTÉRISTIQUES**\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1caSJnkIgKq8"},"outputs":[],"source":["# 'original_labels' est un dictionnaire qui associe à chaque configuration du nombre de thèmes\n","# une liste de libellés descriptifs pour chacun des thèmes identifiés par le modèle NMF.\n","#\n","# Clés du dictionnaire :\n","# - Les clés sont des entiers représentant le nombre de thèmes dans une configuration donnée (par exemple, 7 ou 12).\n","#\n","# Valeurs du dictionnaire :\n","# - Les valeurs sont des listes de chaînes de caractères, où chaque chaîne est un libellé descriptif pour un thème spécifique.\n","# - La longueur de chaque liste doit correspondre exactement au nombre de thèmes de la configuration (c'est-à-dire, la clé du dictionnaire).\n","#\n","# Comment remplir 'original_labels' :\n","# - Pour chaque configuration du nombre de thèmes que vous avez générée avec NMF, créez une entrée dans le dictionnaire.\n","# - Assurez-vous que les thèmes (libellés) sont ordonnés de la même manière que les indices des thèmes dans la matrice W de NMF.\n","# - Fournissez des descriptions claires et concises pour chaque thème afin de faciliter l'interprétation des résultats.\n","#\n","# Utilisation dans la fonction :\n","# - 'original_labels' est utilisé pour remplacer les indices numériques des thèmes par des libellés textuels lors de la génération des graphiques.\n","# - Cela permet d'améliorer la lisibilité des cartes de chaleur en affichant des noms de thèmes significatifs sur l'axe des ordonnées.\n","topic_labels_by_config = {15:\n","                            [\"Au-delà du small talk : l’art de créer du lien par la conversation\",\n","                            \"Streaming zen : self-care et mindfulness face aux trolls\",\n","                            \"Consentement dans l’intimité naissante : gérer le rejet\",\n","                            \"Aimer son corps : de la critique à la gratitude\",\n","                            \"Le plaisir de soi : dépasser la honte, cultiver l’extase\",\n","                            \"De la scène aux causes : l'engagement des stars\",\n","                            \"Liberté hardcore : l'émancipation par l'audace\",\n","                            \"Fantasmes : de l'introspection au dépassement des tabous\",\n","                            \"Cannabis : de l'éveil à la réinvention\",\n","                            \"Le camming en duo : entre vie personnelle et professionnelle\",\n","                            \"L’art de la communication sexuelle : de la sécurité à la guérison\",\n","                            \"Le contenu pour adultes : entre spontanéité et élaboration\",\n","                            \"Kink et BDSM : quand le jeu devient thérapie\",\n","                            \"Désir et excitation : harmoniser les écarts\",\n","                            \"L'industrie pour adultes : relations intimes et compétition\"]\n","}"]},{"cell_type":"code","source":["write_documents_infos()"],"metadata":{"id":"CHxO_MFwYd52"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hYC2GAShpeHU"},"source":["##**DYNAMIQUE DES TOPICS**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RRwDK00oJjds","executionInfo":{"status":"ok","timestamp":1739878654428,"user_tz":-60,"elapsed":30942,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"625cdf53-7223-414e-a97f-dfd925beb10c"},"outputs":[{"output_type":"stream","name":"stderr","text":["CONFIGURATIONS PROCESSÉES: 100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:15<00:00, 15.64s/it]\n","CONFIGURATIONS PROCESSÉES: 100%|████████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:14<00:00, 14.88s/it]\n"]}],"source":["# Le paramètre `sigma` détermine le niveau de lissage de la distribution des scores des topics.\n","# Lorsque `sigma='auto'`, la fonction tente de déterminer automatiquement un lissage \"optimal\"\n","# en se basant sur la moyenne des écarts-types des valeurs dans chaque période temporelle.\n","# Cependant, cette approche n'est pas toujours idéale, et il peut être nécessaire d'expérimenter\n","# avec différentes valeurs de `sigma`. Si `sigma=1`, aucun lissage n'est appliqué, laissant\n","# les valeurs brutes du DataFrame intactes.\n","#\n","# Il est important de noter qu'il n'existe pas de lissage \"optimal\" universel. Le lissage permet\n","# de rendre les dynamiques des topics plus intelligibles visuellement, en supprimant les variations\n","# erratiques et en montrant les tendances générales des topics au fil du temps. Par exemple,\n","# cela peut aider à mettre en évidence un topic dominant au début ou à la fin de la période analysée.\n","#\n","# Cependant, un lissage trop fort peut masquer des variations significatives au sein des données\n","# et donner une impression erronée de stabilité. Une fois lissée, il devient plus difficile\n","# de distinguer si une zone de forte intensité est due à une présence soutenue du topic tout au\n","# long de la période ou à un événement ponctuel particulièrement intense.\n","for apply_norm in [True, False]:\n","    create_chrono_topics(sigma='auto', apply_normalizations=apply_norm)"]},{"cell_type":"markdown","source":["##**DYNAMIQUE DE GROUPES**\n","\n"],"metadata":{"id":"ekryLTvSM_P1"}},{"cell_type":"code","source":["# Le paramètre `sigma` détermine le niveau de lissage de la distribution des scores des journaux ou autres entités au fil du temps.\n","# Lorsque `sigma='auto'`, la fonction tente de déterminer automatiquement un lissage \"optimal\" basé sur la moyenne\n","# des écarts-types des valeurs dans chaque période. Cependant, cette méthode n'est pas toujours efficace, et il\n","# peut être utile d'expérimenter avec différentes valeurs de `sigma`.\n","#\n","# Par exemple, si `sigma=1`, aucun lissage n'est appliqué et les valeurs brutes sont utilisées, laissant ainsi\n","# apparaître toutes les fluctuations, même mineures. Si `sigma=30`, cela signifie que l'on applique un fort lissage\n","# qui va aplanir considérablement les données, en supprimant une bonne partie des variations quotidiennes et\n","# en montrant plutôt des tendances à plus long terme. Plus `sigma` est grand, plus la courbe est \"étalée\" et\n","# lissée, ce qui permet de voir des tendances générales, mais au détriment de la détection des pics ponctuels.\n","#\n","# Il est important de souligner qu'il n'existe pas de lissage \"optimal\" universel. Le lissage permet de rendre\n","# les dynamiques de la publication des journaux plus intelligibles en supprimant les fluctuations erratiques\n","# et en exposant les tendances générales des journaux au fil du temps. Par exemple, cela permet de visualiser\n","# les périodes où certains journaux dominent la couverture médiatique.\n","#\n","# Cependant, un lissage trop important peut atténuer des variations cruciales dans les données et nuire à\n","# l'analyse des événements ponctuels ou des périodes d'intensité médiatique accrue. Une fois lissée,\n","# il devient difficile de savoir si une zone de forte intensité est le résultat d'une couverture continue\n","# ou d'un événement médiatique exceptionnellement intense.\n","#\n","# Le paramètre `group_column` est pertinent uniquement lorsque `source_type='csv'`. Dans ce cas, il s'agit du\n","# nom (string) d'une colonne catégorielle sur laquelle regrouper les données, similaire à la colonne \"journal\"\n","# dans le cas de Istex ou de Europresse. Par exemple, si votre CSV contient une colonne \"publisher\" qui répertorie le nom de\n","# l'éditeur ou de la source du document, vous pouvez utiliser `group_column='publisher'` pour agréger\n","# temporellement les données par éditeur. De même, si vous avez une colonne \"theme\" pour catégoriser\n","# les articles par sujet, `group_column='theme'` permettra de visualiser l'évolution temporelle\n","# par sujet. Si `group_column` n'est pas fourni ou n'existe pas dans `columns_dict`, un message\n","# d'erreur est affiché et la fonction s'interrompt. Ce mécanisme permet de créer une agrégation temporelle\n","# par catégorie, facilitant l'analyse par groupes définis (par exemple, par titre de publication, thème, pays, etc.).\n","for apply_norm in [False, True]:\n","    create_chrono_group_column(group_column='channel_title', sigma='auto', apply_normalizations=apply_norm)"],"metadata":{"id":"4gc1HIh2oQyY","executionInfo":{"status":"ok","timestamp":1739878654442,"user_tz":-60,"elapsed":7,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"acf113f4-669f-44f9-c39e-25e2f2929aa4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["La colonne 'channel_title' n'existe pas dans columns_dict. Vérifiez les colonnes disponibles.\n","La colonne 'channel_title' n'existe pas dans columns_dict. Vérifiez les colonnes disponibles.\n"]}]},{"cell_type":"markdown","metadata":{"id":"6QTDTgkFatMT"},"source":["## **SPATIALISATION DES TOPICS PAR PCA**"]},{"cell_type":"code","source":["# Cette fonction vise à analyser et visualiser les relations entre topics en appliquant une PCA sur les matrices\n","# produites par la factorisation NMF (matrices W et H), chaque projection ayant un objectif distinct :\n","#\n","# 1) **PCA sur W** :\n","#    - La matrice W décrit comment les topics sont distribués à travers l'ensemble des documents.\n","#    - En appliquant une PCA sur W, nous cherchons à rapprocher les topics qui partagent une distribution\n","#      similaire sur les documents, c'est-à-dire ceux qui apparaissent de manière conjointe dans les mêmes\n","#      ensembles documentaires. Cela fournit une vision \"macro\" des relations entre topics dans l'espace documentaire.\n","#\n","# 2) **PCA sur H** :\n","#    - La matrice H décrit comment les mots contribuent à la définition de chaque topic.\n","#    - En appliquant une PCA sur H, nous cherchons à rapprocher les topics ayant un vocabulaire similaire,\n","#      c'est-à-dire ceux qui partagent des mots-clés ou des caractéristiques linguistiques proches.\n","#      Cette analyse met en évidence des proximités sémantiques entre les topics.\n","#\n","# Ces deux visualisations permettent d'explorer les relations entre topics sous deux angles complémentaires :\n","# - La PCA sur W éclaire les co-apparitions thématiques dans les documents.\n","# - La PCA sur H révèle les similitudes lexicales ou sémantiques entre topics.\n","#\n","# En somme, cette étape enrichit l'interprétation des topics NMF en offrant une vue synthétique de leurs\n","# relations documentaires et lexicales.\n","plot_pca('W')\n","plot_pca('H')"],"metadata":{"id":"-P1ytfCYH0Zl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **ANALYSE DE SENTIMENTS**"],"metadata":{"id":"bPgCltOL9NtP"}},{"cell_type":"code","source":["# Cette fonction effectue une analyse de sentiments sur une liste de documents textuels, en s’appuyant\n","# sur un modèle BERT multilingue (nlptown/bert-base-multilingual-uncased-sentiment). Elle parcourt chaque texte,\n","# le tokenize et le passe au pipeline d’analyse afin d’obtenir une note d’étoiles (de 1 à 5) reflétant\n","# le sentiment.\n","#\n","# Attention : par défaut, le modèle BERT utilisé est limité à 512 tokens. Si le texte dépasse cette limite,\n","# il sera automatiquement tronqué aux 512 premiers tokens par le tokenizer et le pipeline. Par conséquent,\n","# seuls ces 512 premiers tokens seront pris en compte dans le calcul du sentiment, ce qui peut potentiellement\n","# biaiser l’analyse pour les textes très longs. Si un traitement plus complet est nécessaire, il conviendra\n","# de segmenter le texte en plusieurs morceaux de taille inférieure ou égale à 512 tokens et d’analyser\n","# chaque segment individuellement.\n","#\n","# Après l’analyse, la fonction calcule une moyenne des scores des documents, transforme les dates,\n","# puis génère des graphiques de sentiment dans le temps et sous forme de cartes thermiques (heatmaps)\n","# en fonction de différents paramètres.\n","process_sentiments()"],"metadata":{"id":"mNMRH_5cqxB6","executionInfo":{"status":"error","timestamp":1739878726218,"user_tz":-60,"elapsed":68448,"user":{"displayName":"Jérémie Garrigues","userId":"14912292466158867073"}},"colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bb120e9f-82fb-49b3-f554-f861abfe194c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/opt/anaconda3/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["device mps\n"]},{"output_type":"stream","name":"stderr","text":["Processing Documents:   1%|▌                                                                                  | 1399/224579 [01:05<2:16:11, 27.31it/s]"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[94], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Cette fonction effectue une analyse de sentiments sur une liste de documents textuels, en s’appuyant\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# sur un modèle BERT multilingue (nlptown/bert-base-multilingual-uncased-sentiment). Elle parcourt chaque texte,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# le tokenize et le passe au pipeline d’analyse afin d’obtenir une note d’étoiles (de 1 à 5) reflétant\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# puis génère des graphiques de sentiment dans le temps et sous forme de cartes thermiques (heatmaps)\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# en fonction de différents paramètres.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m process_sentiments()\n","File \u001b[0;32m/var/folders/p9/kh9swkjd54j2bq_3yl5sy2gh0000gn/T/ipykernel_7648/2991090316.py:27\u001b[0m, in \u001b[0;36mprocess_sentiments\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m sentiment_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m                               model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m                               tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     22\u001b[0m                               truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m                               max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     24\u001b[0m                               device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Imaginons que 'documents' est votre tableau de textes\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m [analyze_sentiment(doc, sentiment_pipeline) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(documents, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuropresse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m soup \u001b[38;5;129;01min\u001b[39;00m all_soups:\n","File \u001b[0;32m/var/folders/p9/kh9swkjd54j2bq_3yl5sy2gh0000gn/T/ipykernel_7648/2991090316.py:27\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m sentiment_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     20\u001b[0m                               model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     21\u001b[0m                               tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[1;32m     22\u001b[0m                               truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m                               max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m,\n\u001b[1;32m     24\u001b[0m                               device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Imaginons que 'documents' est votre tableau de textes\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m sentiments \u001b[38;5;241m=\u001b[39m [analyze_sentiment(doc, sentiment_pipeline) \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m tqdm(documents, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing Documents\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m source_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuropresse\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m soup \u001b[38;5;129;01min\u001b[39;00m all_soups:\n","File \u001b[0;32m/var/folders/p9/kh9swkjd54j2bq_3yl5sy2gh0000gn/T/ipykernel_7648/3332776885.py:5\u001b[0m, in \u001b[0;36manalyze_sentiment\u001b[0;34m(text, sentiment_pipeline)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_sentiment\u001b[39m(text, sentiment_pipeline):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m      3\u001b[0m         \u001b[38;5;66;03m# Analyse de sentiments directement sur le texte complet,\u001b[39;00m\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;66;03m# en demandant explicitement la troncation à 512 tokens\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m         result \u001b[38;5;241m=\u001b[39m sentiment_pipeline(text, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m512\u001b[39m)\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:156\u001b[0m, in \u001b[0;36mTextClassificationPipeline.__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;03mClassify the text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;03m    If `top_k` is used, one such dictionary is returned per label.\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    155\u001b[0m inputs \u001b[38;5;241m=\u001b[39m (inputs,)\n\u001b[0;32m--> 156\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# TODO try and retrieve it in a nicer way from _sanitize_parameters.\u001b[39;00m\n\u001b[1;32m    158\u001b[0m _legacy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m kwargs\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1257\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1250\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1251\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1254\u001b[0m         )\n\u001b[1;32m   1255\u001b[0m     )\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1257\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1264\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1262\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1264\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1265\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/base.py:1164\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1162\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1163\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1164\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward(model_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mforward_params)\n\u001b[1;32m   1165\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1166\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/pipelines/text_classification.py:187\u001b[0m, in \u001b[0;36mTextClassificationPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(model_forward)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m    186\u001b[0m     model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1695\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1687\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1688\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1689\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1690\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1691\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1692\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1693\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1695\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert(\n\u001b[1;32m   1696\u001b[0m     input_ids,\n\u001b[1;32m   1697\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1698\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1699\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1700\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1701\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1702\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1703\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1704\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1705\u001b[0m )\n\u001b[1;32m   1707\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   1709\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(pooled_output)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m   1142\u001b[0m     embedding_output,\n\u001b[1;32m   1143\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m   1144\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1145\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m   1146\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m   1147\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m   1148\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m   1149\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1150\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1151\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1152\u001b[0m )\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    695\u001b[0m         hidden_states,\n\u001b[1;32m    696\u001b[0m         attention_mask,\n\u001b[1;32m    697\u001b[0m         layer_head_mask,\n\u001b[1;32m    698\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    699\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    700\u001b[0m         past_key_value,\n\u001b[1;32m    701\u001b[0m         output_attentions,\n\u001b[1;32m    702\u001b[0m     )\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:584\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    574\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    581\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 584\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention(\n\u001b[1;32m    585\u001b[0m         hidden_states,\n\u001b[1;32m    586\u001b[0m         attention_mask,\n\u001b[1;32m    587\u001b[0m         head_mask,\n\u001b[1;32m    588\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    589\u001b[0m         past_key_value\u001b[38;5;241m=\u001b[39mself_attn_past_key_value,\n\u001b[1;32m    590\u001b[0m     )\n\u001b[1;32m    591\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:514\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    505\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    506\u001b[0m     hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    512\u001b[0m     output_attentions: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    513\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 514\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself(\n\u001b[1;32m    515\u001b[0m         hidden_states,\n\u001b[1;32m    516\u001b[0m         attention_mask,\n\u001b[1;32m    517\u001b[0m         head_mask,\n\u001b[1;32m    518\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    519\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    520\u001b[0m         past_key_value,\n\u001b[1;32m    521\u001b[0m         output_attentions,\n\u001b[1;32m    522\u001b[0m     )\n\u001b[1;32m    523\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    524\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n","File \u001b[0;32m/opt/anaconda3/lib/python3.11/site-packages/transformers/models/bert/modeling_bert.py:439\u001b[0m, in \u001b[0;36mBertSdpaSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;66;03m# We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[38;5;66;03m# The tgt_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# a causal mask in case tgt_len == 1.\u001b[39;00m\n\u001b[1;32m    435\u001b[0m is_causal \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cross_attention \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m tgt_len \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    437\u001b[0m )\n\u001b[0;32m--> 439\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mscaled_dot_product_attention(\n\u001b[1;32m    440\u001b[0m     query_layer,\n\u001b[1;32m    441\u001b[0m     key_layer,\n\u001b[1;32m    442\u001b[0m     value_layer,\n\u001b[1;32m    443\u001b[0m     attn_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m    444\u001b[0m     dropout_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m,\n\u001b[1;32m    445\u001b[0m     is_causal\u001b[38;5;241m=\u001b[39mis_causal,\n\u001b[1;32m    446\u001b[0m )\n\u001b[1;32m    448\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    449\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mreshape(bsz, tgt_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","source":["##**FORÊTS ALÉATOIRES AVEC ANALYSE DES RÉSIDUS NORMALISÉS**"],"metadata":{"id":"OO-u3ChN4fno"}},{"cell_type":"code","source":["create_box_plots(group_column='channel_id')"],"metadata":{"id":"tELAclevPNiV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["random_forests_residuals_analysis(group_column='channel_id')"],"metadata":{"id":"mj373B1wAfWF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"vFT_cp3Bd-Vl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"CtJaMURad-S-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ad2-cMA0d-Qn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"f7GCclCvd-OE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"JLNDddPFzYlQ"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"6d411e848df049acba679d8f6ac021f4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b951c23a57b24298bbfc84a946219bd5","IPY_MODEL_ff08f96f6c68498699fa5aa86e5c3f50","IPY_MODEL_6c798065d3d34f33a5cb4ef3a86352a3"],"layout":"IPY_MODEL_813a0096c0a14544bf8587197047d519"}},"b951c23a57b24298bbfc84a946219bd5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a24d702ff6494dac8b6ed48a464c7e06","placeholder":"​","style":"IPY_MODEL_40bbe11df03b4da3b011249554f5a4af","value":"DOCUMENTS PROCESSÉS: 100%"}},"ff08f96f6c68498699fa5aa86e5c3f50":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_96e1596583854dd28f6f45d8018127b0","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8e0f122d9b38442586646c7d54f2b905","value":1}},"6c798065d3d34f33a5cb4ef3a86352a3":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c4f4c50162da4955884c79c8bf78db39","placeholder":"​","style":"IPY_MODEL_5945ec1d12e84726bb59acd843647bba","value":" 1/1 [01:09&lt;00:00, 30.80s/it]"}},"813a0096c0a14544bf8587197047d519":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a24d702ff6494dac8b6ed48a464c7e06":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"40bbe11df03b4da3b011249554f5a4af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"96e1596583854dd28f6f45d8018127b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8e0f122d9b38442586646c7d54f2b905":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c4f4c50162da4955884c79c8bf78db39":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5945ec1d12e84726bb59acd843647bba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d29f1fc02d964c8e8600308bf6d09dec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0dc48acef57f428f972e75ebfa50338e","IPY_MODEL_6a52320dd96d4e0c89f9b16d75772ff0","IPY_MODEL_e10268dc4c3e4c4e978c057af32147c2"],"layout":"IPY_MODEL_5245b6f1cf074219aba13aadb968246f"}},"0dc48acef57f428f972e75ebfa50338e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bbdfb3a58d8f4388bfd1b771bab3bb9b","placeholder":"​","style":"IPY_MODEL_257f2a9bb9a7453dbb59546de453ad76","value":"DOCUMENTS PROCESSÉS: 100%"}},"6a52320dd96d4e0c89f9b16d75772ff0":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a5ac99fc9e7747808608e466b3fb6172","max":128623,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0c4c95e25a114e10beeab3b484b61cd3","value":128623}},"e10268dc4c3e4c4e978c057af32147c2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c998fc39c42743e38f86e2b5ebae80d9","placeholder":"​","style":"IPY_MODEL_ff9c6da282f442359847c81b69cb8243","value":" 128623/128623 [19:50&lt;00:00, 131.43it/s]"}},"5245b6f1cf074219aba13aadb968246f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bbdfb3a58d8f4388bfd1b771bab3bb9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"257f2a9bb9a7453dbb59546de453ad76":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a5ac99fc9e7747808608e466b3fb6172":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c4c95e25a114e10beeab3b484b61cd3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c998fc39c42743e38f86e2b5ebae80d9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff9c6da282f442359847c81b69cb8243":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ed24d4ab2b34844ac740f610899f716":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_bdd47d4986834f368168d34776965aa5","IPY_MODEL_2e43333161254325a19e8285edbb93ce","IPY_MODEL_8247fe49192a47c88da052388b309c9a"],"layout":"IPY_MODEL_993e44343c394e46996719dc596d9468"}},"bdd47d4986834f368168d34776965aa5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_77968bf9e2d245959a8baaa74ac969ce","placeholder":"​","style":"IPY_MODEL_c0501456c0034c30984d2f2b2e6b0195","value":"Mise à jour des unigrams: 100%"}},"2e43333161254325a19e8285edbb93ce":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_284ec8e15a2a4c839f436d8d20c8268c","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6f477fbb55f140e9856f82f4d38ed312","value":3}},"8247fe49192a47c88da052388b309c9a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_634033e7fbe345c58d7dfd276a2f2789","placeholder":"​","style":"IPY_MODEL_2b5fb742bc2f42fcbdaa176eb3194b62","value":" 3/3 [00:56&lt;00:00, 18.07s/it]"}},"993e44343c394e46996719dc596d9468":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77968bf9e2d245959a8baaa74ac969ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c0501456c0034c30984d2f2b2e6b0195":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"284ec8e15a2a4c839f436d8d20c8268c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f477fbb55f140e9856f82f4d38ed312":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"634033e7fbe345c58d7dfd276a2f2789":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b5fb742bc2f42fcbdaa176eb3194b62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d95819ac1f024d1f825f373852b4f848":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f2cb881e86a84b9d97fdd96ebd8509c8","IPY_MODEL_72bf281e72104988b8af544dcef9cbe5","IPY_MODEL_592368db4fcb41849664dc13fe64be77"],"layout":"IPY_MODEL_556e919d627c4e13a3e4192a85cdf6e4"}},"f2cb881e86a84b9d97fdd96ebd8509c8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d10a1681a9e44f499be3a68f9e638f94","placeholder":"​","style":"IPY_MODEL_9a91c1a4c25049049eca8b933b191941","value":"Filtrage des stopwords: 100%"}},"72bf281e72104988b8af544dcef9cbe5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_199d5c7aad2b4945b43d4459d594a2c7","max":128623,"min":0,"orientation":"horizontal","style":"IPY_MODEL_660a0dda97ca416e8b97ce48adf066ed","value":128623}},"592368db4fcb41849664dc13fe64be77":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b05464c072c435fa7dc4d80172a633e","placeholder":"​","style":"IPY_MODEL_b6bf255c06a9413d8060331dddf1ccf5","value":" 128623/128623 [00:01&lt;00:00, 116810.84it/s]"}},"556e919d627c4e13a3e4192a85cdf6e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d10a1681a9e44f499be3a68f9e638f94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a91c1a4c25049049eca8b933b191941":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"199d5c7aad2b4945b43d4459d594a2c7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"660a0dda97ca416e8b97ce48adf066ed":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0b05464c072c435fa7dc4d80172a633e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6bf255c06a9413d8060331dddf1ccf5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"261f2cae5c8a4622b7d4ad0d206dc27f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c69192f9e8af45bda4e4fdcd7010da96","IPY_MODEL_966f1728dd0440fba83052119019a784","IPY_MODEL_b262477dba9c4aef9cf44db10544ca4f"],"layout":"IPY_MODEL_10ca8c888aab4e32a0f2937c3feeaaf0"}},"c69192f9e8af45bda4e4fdcd7010da96":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_300611a2ba2e420b9811e3221f5d3300","placeholder":"​","style":"IPY_MODEL_2ea5ee687ed947c2b92135749ed74b39","value":"PROCESSUS DES TOPICS: 100%"}},"966f1728dd0440fba83052119019a784":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5c9192050e440f0ba82630612efe4cd","max":4,"min":0,"orientation":"horizontal","style":"IPY_MODEL_072cce547935406fb348e5d49f8d216b","value":4}},"b262477dba9c4aef9cf44db10544ca4f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f21e890af6fc4c449429189c9ce8c44f","placeholder":"​","style":"IPY_MODEL_34b6531480074258ae4491bfa645de64","value":" 4/4 [02:24&lt;00:00, 45.35s/it]"}},"10ca8c888aab4e32a0f2937c3feeaaf0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"300611a2ba2e420b9811e3221f5d3300":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ea5ee687ed947c2b92135749ed74b39":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e5c9192050e440f0ba82630612efe4cd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"072cce547935406fb348e5d49f8d216b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f21e890af6fc4c449429189c9ce8c44f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34b6531480074258ae4491bfa645de64":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}