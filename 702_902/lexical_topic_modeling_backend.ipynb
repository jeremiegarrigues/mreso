{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"5WnTr61urNKj"},"outputs":[],"source":["def is_module_installed(module_name):\n","    \"\"\"\n","    Vérifie si un module Python est déjà installé.\n","    Retourne True s'il est installé, False sinon.\n","    \"\"\"\n","    return importlib.util.find_spec(module_name) is not None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tXExUQn2jHbJ"},"outputs":[],"source":["def download_things():\n","    global nlp_pipeline\n","\n","    if '/content/drive/' in folder_path:\n","        # Définition des bibliothèques à vérifier\n","        libraries = {\n","            # Bibliothèque  : (nom_module_import, commande_pip)\n","            'seaborn': ('seaborn', 'pip install -q seaborn'),\n","            'sklearn': ('sklearn', 'pip install -U -q scikit-learn'),\n","            'scipy': ('scipy', 'pip install -U -q scipy'),\n","            'spacy': ('spacy', 'pip install -q spacy'),\n","            'pyate': ('pyate', 'pip install -q pyate'),\n","            'bs4': ('bs4', 'pip install -q beautifulsoup4'),\n","            'unidecode': ('unidecode', 'pip install -q unidecode'),\n","            'charset_normalizer': ('charset_normalizer', 'pip install -q charset-normalizer'),\n","            'datasketch': ('datasketch', 'pip install -q datasketch'),\n","            'tslearn': ('tslearn', 'pip install -q tslearn'),\n","            'ortools': ('ortools', 'pip install -q ortools')  # <-- Ajout de OR-Tools\n","        }\n","    else:\n","        # Définition des bibliothèques à vérifier\n","        libraries = {\n","            'gensim': ('gensim', 'conda install gensim -y'),\n","            'torch': ('torch', 'conda install pytorch torchvision torchaudio -c pytorch-nightly -c conda-forge -y'),\n","            'transformers': ('transformers', 'conda install -c conda-forge transformers -y'),\n","            'seaborn': ('seaborn', 'conda install -y seaborn'),\n","            'sklearn': ('sklearn', 'conda install -y scikit-learn'),\n","            'scipy': ('scipy', 'conda install -y scipy'),\n","            'spacy': ('spacy', 'conda install -y spacy'),\n","            'bs4': ('bs4', 'conda install -y beautifulsoup4'),\n","            'unidecode': ('unidecode', 'conda install -y unidecode'),\n","            'charset_normalizer': ('charset_normalizer', 'conda install -y -c conda-forge charset-normalizer'),\n","            'datasketch': ('datasketch', 'conda install -y -c conda-forge datasketch'),\n","            'tslearn': ('tslearn', 'conda install -y -c conda-forge tslearn'),\n","            'ortools': ('ortools', 'conda install -y -c conda-forge ortools')  # <-- Ajout de OR-Tools\n","        }\n","\n","\n","    # Installation conditionnelle et import pour chaque bibliothèque\n","    for lib_key, (module_name, install_cmd) in libraries.items():\n","        print(f\"Vérification de {lib_key}…\")\n","        if not is_module_installed(module_name):\n","            print(f\"{module_name} n'est pas installé. Installation en cours… {install_cmd}\")\n","            subprocess.run(install_cmd.split(), stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n","        # Import effectif après installation (ou si déjà installé)\n","        importlib.import_module(module_name)\n","        print(f\"✔ {lib_key} est prête.\")\n","\n","    # Installation du modèle spaCy selon la langue\n","    if language == 'fr':\n","        !python -m spacy download fr_core_news_sm --quiet\n","    elif language == 'en':\n","        !python -m spacy download en_core_web_sm --quiet\n","    elif language == 'es':\n","        !python -m spacy download es_core_news_sm --quiet\n","    elif language == 'de':\n","        !python -m spacy download de_core_news_sm --quiet\n","    elif language == 'ca':\n","        !python -m spacy download ca_core_news_sm --quiet\n","    elif language == 'zh':\n","        !python -m spacy download zh_core_web_sm --quiet\n","    elif language == 'da':\n","        !python -m spacy download da_core_news_sm --quiet\n","    elif language == 'ja':\n","        !python -m spacy download ja_core_news_sm --quiet\n","    elif language == 'sl':\n","        !python -m spacy download sl_core_news_sm --quiet\n","    elif language == 'uk':\n","        !python -m spacy download uk_core_news_sm --quiet\n","\n","\n","    try:\n","        if language == 'fr':\n","            nlp_pipeline = spacy.load('fr_core_news_sm', disable=['ner'])\n","        elif language == 'en':\n","            nlp_pipeline = spacy.load('en_core_web_sm', disable=['ner'])\n","        elif language == 'es':\n","            nlp_pipeline = spacy.load('es_core_news_sm', disable=['ner'])\n","        elif language == 'de':\n","            nlp_pipeline = spacy.load('de_core_news_sm', disable=['ner'])\n","        elif language == 'ca':\n","            nlp_pipeline = spacy.load('ca_core_news_sm', disable=['ner'])\n","        elif language == 'zh':\n","            nlp_pipeline = spacy.load('zh_core_web_sm', disable=['ner'])\n","        elif language == 'da':\n","            nlp_pipeline = spacy.load('da_core_news_sm', disable=['ner'])\n","        elif language == 'ja':\n","            nlp_pipeline = spacy.load('ja_core_news_sm', disable=['ner'])\n","        elif language == 'sl':\n","            nlp_pipeline = spacy.load('sl_core_news_sm', disable=['ner'])\n","        elif language == 'uk':\n","            nlp_pipeline = spacy.load('uk_core_web_sm', disable=['ner'])\n","        else:\n","            raise ValueError(f\"Modèle non supporté pour la langue {language}\")\n","    except Exception as e:\n","        raise\n","\n","    # Optionnel : évite les avertissements de multiprocessing pour spaCy\n","    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""]},{"cell_type":"code","source":["# =====================\n","# Standard Libraries\n","# =====================\n","import contextlib\n","import copy\n","import csv\n","import gc\n","import html\n","import importlib\n","import io\n","import itertools\n","import json\n","import locale\n","import logging\n","import math\n","import multiprocessing\n","from multiprocessing import cpu_count\n","import os\n","import pickle\n","import random\n","import re\n","import string\n","import subprocess\n","import time\n","\n","from collections import Counter, defaultdict\n","from datetime import datetime, timedelta\n","from functools import partial\n","from math import sqrt\n","from urllib import request\n","import spacy\n","\n","# Cette fonction semble être un appel interne (pas un import),\n","# on la laisse à la suite des imports standards si c’est nécessaire\n","download_things()\n","\n","# =====================\n","# Data Handling & Analysis\n","# =====================\n","import numpy as np\n","import pandas as pd\n","import psutil\n","import requests\n","import statsmodels.api as sm\n","from ortools.linear_solver import pywraplp\n","\n","from charset_normalizer import from_path\n","from dateutil.parser import parse\n","from gensim.corpora import Dictionary\n","from gensim.models import CoherenceModel\n","from scipy import stats\n","from scipy.cluster.hierarchy import dendrogram, linkage\n","from scipy.ndimage import gaussian_filter\n","from scipy.spatial import distance\n","from scipy.spatial.distance import cosine, pdist\n","from statsmodels.stats.outliers_influence import variance_inflation_factor\n","\n","\n","# =====================\n","# NLP & Text Processing\n","# =====================\n","import torch\n","import unidecode\n","from bs4 import BeautifulSoup\n","from pyate import cvalues\n","from pyate.term_extraction_pipeline import TermExtractionPipeline\n","from spacy.language import Language\n","from spacy.tokens import Doc\n","\n","\n","# =====================\n","# Machine Learning\n","# =====================\n","from sklearn.decomposition import NMF, PCA\n","from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n","from sklearn.feature_extraction.text import (\n","    CountVectorizer,\n","    TfidfTransformer,\n","    TfidfVectorizer\n",")\n","from sklearn.inspection import permutation_importance\n","from sklearn.metrics import roc_auc_score\n","from sklearn.metrics.pairwise import (\n","    cosine_distances,\n","    cosine_similarity,\n","    manhattan_distances\n",")\n","from sklearn.model_selection import (\n","    StratifiedKFold,\n","    cross_val_score\n",")\n","from sklearn.preprocessing import label_binarize\n","\n","\n","# =====================\n","# Transformers & NLP Models\n","# =====================\n","from transformers import (\n","    AutoTokenizer,\n","    AutoModelForSequenceClassification,\n","    CamembertTokenizer,\n","    pipeline\n",")\n","\n","\n","# =====================\n","# Visualisation\n","# =====================\n","import matplotlib.colors as mcolors\n","import matplotlib.dates as mdates\n","import matplotlib.pyplot as plt\n","import matplotlib.transforms as mtransforms\n","import seaborn as sns\n","from matplotlib.cm import ScalarMappable\n","from matplotlib.colors import LinearSegmentedColormap, Normalize\n","from matplotlib.gridspec import GridSpec\n","\n","\n","# =====================\n","# Divers\n","# =====================\n","if '/content/drive/' in folder_path:\n","    from tqdm.notebook import tqdm, trange\n","else:\n","    from tqdm import tqdm, trange\n","\n","from datasketch import MinHash, MinHashLSH\n","from joblib import Parallel, delayed\n","from tslearn.metrics import dtw_path_from_metric\n","from dateutil import parser"],"metadata":{"id":"P49onC8jM0IS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def plot_clustered_heatmap(\n","    pivot_df,\n","    num_topic=20,\n","    scale_axis=1,\n","    safe_pad_px=5  # marge de sécurité supplémentaire (en pixels) de chaque côté\n","):\n","\n","    # -----------------------------\n","    # 1) Normalisation min–max\n","    # -----------------------------\n","    df_scaled = pivot_df.apply(\n","        lambda x: (x - x.min()) / (x.max() - x.min()),\n","        axis=scale_axis\n","    )\n","\n","    # Nettoyage des noms de colonnes (optionnel)\n","    df_scaled.columns = (\n","        df_scaled.columns\n","        .str.split(':')\n","        .str[0]\n","        .str.strip()\n","    )\n","\n","    if scale_axis == 1:\n","        df_scaled = df_scaled.T\n","\n","    # 2) Ordre des lignes et des colonnes\n","    row_order = (\n","        df_scaled\n","        .sum(axis=1)\n","        .sort_values()\n","        .index\n","    )\n","    col_order = (\n","        df_scaled\n","        .sum(axis=0)\n","        .sort_values()\n","        .index\n","    )\n","    df_scaled = df_scaled.loc[row_order, col_order]\n","\n","    # ======================================================\n","    # PASSE 1 : figure temporaire pour mesurer\n","    # ======================================================\n","    temp_height_inch = 6\n","    temp_fig, temp_ax = plt.subplots(\n","        figsize=(FIGURE_WIDTH_INCH, temp_height_inch),\n","        dpi=DPI\n","    )\n","\n","    sns.heatmap(\n","        df_scaled,\n","        ax=temp_ax,\n","        cbar=False,\n","        cmap=\"YlGnBu\",\n","        square=False,\n","        linewidths=1,\n","        linecolor='white'\n","    )\n","    temp_ax.tick_params(\n","        axis='x',\n","        pad=2,\n","        length=0,\n","        labeltop=True,   # labels en haut\n","        labelbottom=True,\n","        top=True,\n","        bottom=True,\n","        labelrotation=90\n","    )\n","    temp_ax.tick_params(axis='y', pad=2, length=0)\n","\n","    temp_fig.canvas.draw()\n","    renderer = temp_fig.canvas.get_renderer()\n","\n","    # 1) Mesure bounding box Xlabels\n","    xlabels = temp_ax.get_xticklabels()\n","    xlabels_bboxes = [lbl.get_window_extent(renderer=renderer) for lbl in xlabels]\n","    max_label_height_px = max(bbox.height for bbox in xlabels_bboxes) if xlabels_bboxes else 0\n","\n","    # 2) Mesure bounding box Ylabels\n","    ylabels = temp_ax.get_yticklabels()\n","    ylabels_bboxes = [lbl.get_window_extent(renderer=renderer) for lbl in ylabels]\n","    max_label_width_px = max(bbox.width for bbox in ylabels_bboxes) if ylabels_bboxes else 0\n","\n","    plt.close(temp_fig)\n","\n","    # ======================================================\n","    # CALCUL DES DIMENSIONS FINALES\n","    # ======================================================\n","    nb_lignes = len(df_scaled)\n","\n","    # Hauteur stricte occupée par les lignes de la heatmap\n","    heatmap_height_px = nb_lignes * PX_PER_TOPIC\n","\n","    # Hauteur totale : zone heatmap + marge X en haut et en bas + safe_pad\n","    # Ici, on ajoute safe_pad_px en haut ET safe_pad_px en bas\n","    # => total = heatmap + 2*(labelsX + safe_pad)\n","    total_height_px = heatmap_height_px + 2 * (max_label_height_px + safe_pad_px)\n","    figure_height_inch = total_height_px / DPI\n","\n","    # Largeur : on garde FIGURE_WIDTH_INCH, mais on doit\n","    # réserver de la place à gauche pour Y + safe_pad (et éventuellement à droite).\n","    total_width_px = FIGURE_WIDTH_INCH * DPI\n","\n","    # Marge gauche en pixels = (largeurY + safe_pad)\n","    margin_left_px = max_label_width_px + safe_pad_px\n","    # Marge droite en pixels : par exemple on en met 0 ou un safe_pad_px\n","    margin_right_px = safe_pad_px  # si tu veux vraiment 0, tu mets 0\n","\n","    # Conversion en fraction de la largeur totale\n","    left_margin_fraction  = margin_left_px / float(total_width_px)\n","    right_margin_fraction = 1.0 - (margin_right_px / float(total_width_px))\n","\n","    # Marge bas = (hauteurX + safe_pad)\n","    # Marge haut = idem\n","    margin_bottom_px = max_label_height_px + safe_pad_px\n","    margin_top_px    = max_label_height_px + safe_pad_px\n","\n","    bottom_margin_fraction = margin_bottom_px / float(total_height_px)\n","    top_margin_fraction    = 1.0 - (margin_top_px / float(total_height_px))\n","\n","    # ======================================================\n","    # PASSE 2 : figure finale\n","    # ======================================================\n","    fig = plt.figure(\n","        figsize=(FIGURE_WIDTH_INCH, figure_height_inch),\n","        dpi=DPI\n","    )\n","    ax = sns.heatmap(\n","        df_scaled,\n","        cbar=False,\n","        square=False,\n","        cmap=\"YlGnBu\",\n","        linewidths=1,\n","        linecolor='white'\n","    )\n","    ax.set_aspect(\"auto\")\n","\n","    ax.tick_params(\n","        axis='x',\n","        pad=2,\n","        length=0,\n","        labeltop=True,\n","        labelbottom=True,\n","        top=True,\n","        bottom=True,\n","        labelrotation=90\n","    )\n","    ax.tick_params(axis='y', pad=2, length=0)\n","\n","    # Ajustement manuel des marges\n","    fig.subplots_adjust(\n","        left=left_margin_fraction,\n","        right=right_margin_fraction,\n","        bottom=bottom_margin_fraction,\n","        top=top_margin_fraction\n","    )\n","\n","    # Nettoyage\n","    ax.set_xlabel(\"\")\n","    ax.set_ylabel(\"\")\n","    plt.title(\"\")\n","\n","    # ======================================================\n","    # Sauvegarde\n","    # ======================================================\n","    if scale_axis == 1:\n","        output_filename = (\n","            f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\"\n","            f\"{base_name}_random_forests_residual_analysis_topic_normalized_heatmap_{num_topic}tc_\"\n","            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n","            f\"_correlationaveragech.png\"\n","        )\n","    else:\n","        output_filename = (\n","            f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\"\n","            f\"{base_name}_random_forests_residual_analysis_group_normalized_heatmap_{num_topic}tc_\"\n","            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n","            f\"_correlationaveragech.png\"\n","        )\n","\n","    plt.savefig(\n","        output_filename,\n","        dpi=DPI,\n","        pad_inches=0,      # pas d'espace supplémentaire autour\n","        bbox_inches=None\n","    )\n","\n","    plt.close(fig)"],"metadata":{"id":"IQvwxAFguEeo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def random_forests_residuals_analysis(group_column=None):\n","    if group_column == None:\n","        print('group_column est None')\n","        return\n","\n","    if not os.path.exists(f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\"):\n","        os.makedirs(f\"{results_path}{base_name}_RANDOM_FORESTS_RESIDUALS_ANALYSIS/\")\n","\n","\n","    for num_topic in all_nmf_W:\n","        rows = []\n","        W_matrix = all_nmf_W[20]\n","\n","        # On itère sur les \"lignes\" de la matrice, i.e. chaque article\n","        for doc_idx, topic_scores in enumerate(W_matrix):\n","            # === Récupération du nom du journal selon la source ===\n","            # doc_idx (au lieu de num_article)\n","            if source_type == 'europresse':\n","                header = all_soups[doc_idx].header\n","                journal_text = extract_information(header, '.rdp__DocPublicationName')\n","                journal_text = normalize_journal(journal_text)\n","\n","            elif source_type == 'istex':\n","                journal_text = columns_dict['journal'][doc_idx]\n","\n","            elif source_type == 'csv':\n","                if group_column not in columns_dict:\n","                    print(f\"La colonne '{group_column}' n'a pas été trouvée dans le fichier CSV.\")\n","                    return\n","                journal_text = columns_dict[group_column][doc_idx]\n","\n","            # Construire un dictionnaire pour cette ligne\n","            row_dict = {\n","                'doc_idx': doc_idx,\n","                'Journal': journal_text\n","            }\n","            # Ajouter les scores de tous les topics\n","            # topic_scores est un array de taille k\n","            for t_idx, score in enumerate(topic_scores):\n","                row_dict[f'Topic{t_idx}'] = score\n","\n","            rows.append(row_dict)\n","\n","\n","        from collections import Counter\n","\n","        # 1) Compter la fréquence de chaque journal dans rows\n","        journal_counts = Counter(row['Journal'] for row in rows)\n","\n","        # 2) Filtrer : ne conserver que les rows dont le journal apparaît au moins threshold fois\n","        rows = [row for row in rows if journal_counts[row['Journal']] >= threshold]\n","\n","        unique_journals = {row['Journal'] for row in rows}\n","\n","        if len(unique_journals) == 1:\n","            print(\"Il n'y a qu'un seul groupe\")\n","            return\n","\n","        df_wide = pd.DataFrame(rows)\n","\n","        k = W_matrix.shape[1]  # nombre de topics\n","\n","        residuals_df = df_wide.copy()  # On duplique pour y stocker les résidus\n","\n","        for j in tqdm(range(k), desc='RÉGRESSIONS : ANALYSE DES RÉSIDUS'):\n","            # Nom de la colonne cible\n","            col_target = f'Topic{j}'\n","\n","            # Features = tous les topics sauf le j-ème\n","            feature_cols = [f'Topic{x}' for x in range(k) if x != j]\n","\n","            X = df_wide[feature_cols]\n","            y = df_wide[col_target]\n","\n","            # Entraîner un modèle de régression\n","            rf = RandomForestRegressor(n_estimators=20, random_state=42)\n","            rf.fit(X, y)\n","\n","            # Prédire\n","            y_pred = rf.predict(X)\n","\n","            # Calcul du résidu \"brut\"\n","            resid = y - y_pred\n","\n","            # Stocker le résidu brut (optionnel)\n","            residuals_df[f'Resid_Topic{j}'] = resid\n","\n","            # Standardisation (z-scoring) du résidu\n","            mu = resid.mean()\n","            sigma = resid.std()  # ou np.std(resid, ddof=1) pour l'échantillon\n","            if sigma == 0:\n","                # Éventuellement, gérer le cas où le résidu est toujours identique (très rare)\n","                resid_z = resid  # ou resid_z = 0\n","            else:\n","                resid_z = (resid - mu) / sigma\n","\n","            # Stocker le résidu normalisé\n","            residuals_df[f'ResidZ_Topic{j}'] = resid_z\n","\n","        # 1) Préparer un dictionnaire de renommage\n","        rename_map = {}\n","        for j in range(k):\n","            old_col = f\"ResidZ_Topic{j}\"\n","            # Récupérer le vrai nom du topic\n","            # Exemple : \"Politique\", \"Économie\", etc.\n","            real_name = topic_labels_by_config[num_topic][j]\n","            new_col = f\"ResidZ_{real_name}\"\n","            rename_map[old_col] = new_col\n","\n","        # 2) Renommer les colonnes dans un nouveau DataFrame\n","        residuals_df_renamed = residuals_df.rename(columns=rename_map)\n","\n","        # 3) Faire le melt : on sélectionne les nouvelles colonnes 'Resid_<topic_name>'\n","        value_vars_list = list(rename_map.values())  # ex: ['Resid_Politique', 'Resid_Sport', ...]\n","\n","        table_resid = residuals_df_renamed.melt(\n","            id_vars=['Journal'],         # on garde la colonne 'Journal' telle quelle\n","            value_vars=value_vars_list,  # on fait fondre les colonnes résidu renommées\n","            var_name='Topic',            # le nom de la colonne contenant l'ancien nom de variable\n","            value_name='Resid'           # la valeur numérique du résidu\n","        )\n","\n","        # Maintenant, 'Topic' sera de la forme 'Resid_<NomDuTopic>'\n","        # On peut, si on veut, enlever le préfixe 'Resid_' pour un affichage plus clair :\n","        table_resid['Topic'] = table_resid['Topic'].str.replace('ResidZ_', '', regex=False)\n","\n","        # 4) Calculer la moyenne des résidus par (Journal, Topic), puis faire un pivot\n","        pivot = table_resid.groupby(['Journal','Topic'])['Resid'].mean().unstack(fill_value=0)\n","\n","        # ===================================================================\n","        # Exemple d’utilisation pour générer deux heatmaps :\n","        #   - l’une avec normalisation min-max par ligne\n","        #   - l’autre avec normalisation min-max par colonne\n","        # ===================================================================\n","\n","        for num_topic in all_nmf_W:\n","            # Heatmap avec normalisation par colonne\n","            plot_clustered_heatmap(\n","                pivot_df=pivot,\n","                scale_axis=0,\n","                num_topic=num_topic\n","            )\n","\n","            # Heatmap avec normalisation par ligne\n","            plot_clustered_heatmap(\n","                pivot_df=pivot,\n","                scale_axis=1,\n","                num_topic=num_topic\n","            )"],"metadata":{"id":"XZArllw9IRLO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def go_tfidf_vectorization_sentences(\n","    gclasses,\n","    count_vectorizer,       # CountVectorizer déjà \"fit\"\n","    tfidf_transformer,      # TfidfTransformer (ou pipeline) déjà \"fit\"\n","    all_sentence_pos        # liste de données (phrases + infos POS, etc.) à tokeniser\n","):\n","    # Sans `tokenize_and_stem`\n","    tokenized_documents = []\n","    for atb in all_sentence_pos:\n","        tokenized_document = [t[0] for t in atb if t[0] in unigrams]\n","        tokenized_documents.append(tokenized_document)\n","\n","    # 4) Filtrer les stop words via spaCy\n","    spacy_stopwords = nlp_pipeline.Defaults.stop_words\n","    # On itère avec tqdm sur tokenized_documents\n","    filtered_docs = []\n","    for doc in tokenized_documents:\n","        filtered_doc = [token for token in doc if token.lower() not in spacy_stopwords]\n","        filtered_docs.append(filtered_doc)\n","    tokenized_documents = filtered_docs\n","\n","    # Exemple : transformation batch pour profiter de tqdm (optionnel)\n","    word_count = count_vectorizer.transform(tokenized_documents)\n","\n","    # Transformation TF-IDF\n","    X_sentences = tfidf_transformer.transform(word_count)\n","\n","    # 7) Retourner la matrice TF-IDF et la version tokenisée\n","    return X_sentences"],"metadata":{"id":"oJdSGESIXdYQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def write_sentences_results(topic_num, final_top_ngrams_per_topic):\n","    with open(\n","        f\"{results_path}{base_name}_EXPLORE_TOPICS/\"\n","        f\"{base_name}_topic_modeling_sentences_{topic_num}tc_\"\n","        f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","        f\"{go_remove_duplicates}dup.csv\",\n","        \"w\",\n","        encoding='utf-8'\n","    ) as file_object:\n","        writer = csv.writer(file_object)\n","\n","        # Écrire les en-têtes si nécessaire\n","        headers = []\n","        for i in range(len(final_top_ngrams_per_topic)):\n","            headers.extend([f'{i}_sentences', f'{i}_scores'])\n","        writer.writerow(headers)\n","\n","        # Écrire les données\n","        for i in range(20):\n","            row = []\n","            for sub_array in final_top_ngrams_per_topic:\n","                if i < len(sub_array):\n","                    row.extend(sub_array[i])\n","                else:\n","                    row.extend(('', ''))\n","            writer.writerow(row)"],"metadata":{"id":"F6llBCJlXdVp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def extract_relevant_sentences(nmf_models):\n","    for num_topic in nmf_models:\n","        X_sentences = go_tfidf_vectorization_sentences(grammatical_classes, tfidf_vectorizer, tfidf_transformer, all_sentence_pos)\n","\n","        score_phrases = nmf_models[num_topic].transform(X_sentences)\n","\n","        final_top_ngrams_per_topic = []\n","\n","        top_n = 20\n","        candidate_size = 100  # on récupère 30 phrases candidates au lieu de 5\n","        similarity_threshold = 0.8  # seuil de similarité au-dessus duquel on considère que c’est “trop proche”\n","\n","        n_topics = nmf_models[num_topic].n_components\n","\n","        for topic_idx in range(n_topics):\n","            topic_scores = score_phrases[:, topic_idx]\n","            # Tri décroissant\n","            top_indices_candidate = np.argsort(topic_scores)[::-1][:candidate_size]\n","\n","            # On extrait les vecteurs TF-IDF correspondants\n","            # (On suppose X_sentences est la matrice TF-IDF de toutes les phrases)\n","            candidate_vectors = X_sentences[top_indices_candidate]\n","            candidate_phrases = [sentences_norms[i] for i in top_indices_candidate]\n","\n","            # Filtrage de similarité\n","            selected_indices = []\n","            for i, vec_i in enumerate(candidate_vectors):\n","                # On calcule la similarité de cette phrase avec celles déjà retenues\n","                # (on compare le vecteur i avec les vecteurs des phrases déjà sélectionnées)\n","                if not selected_indices:\n","                    selected_indices.append(i)\n","                    continue\n","\n","                # Comparaison avec chaque phrase déjà incluse\n","                is_similar_to_selected = False\n","                for j in selected_indices:\n","                    # Similarité cosinus entre le vecteur i et le vecteur j\n","                    sim_ij = cosine_similarity(vec_i, candidate_vectors[j])\n","                    # sim_ij est une matrice 1x1, il faut extraire la valeur\n","                    if sim_ij[0, 0] >= similarity_threshold:\n","                        is_similar_to_selected = True\n","                        break\n","\n","                if not is_similar_to_selected:\n","                    selected_indices.append(i)\n","\n","                # Si on a assez de phrases “différentes”, on arrête\n","                if len(selected_indices) >= top_n:\n","                    break\n","\n","            sub_array = []\n","        #   print(f\"\\n=== Topic {topic_idx} ===\")\n","            for idx_in_candidates in selected_indices[:top_n]:\n","            #print(\"•\", candidate_phrases[idx_in_candidates])\n","\n","                phrase_brute = candidate_phrases[idx_in_candidates]\n","                score_value = topic_scores[top_indices_candidate[idx_in_candidates]]\n","\n","                sub_array.append((phrase_brute, round(score_value, 4)))  # arrondi ou non\n","\n","            final_top_ngrams_per_topic.append(sub_array)\n","\n","\n","        write_sentences_results(num_topic, final_top_ngrams_per_topic)"],"metadata":{"id":"Qoa0SYGuXdNN"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3kdQJK-l_Fto"},"outputs":[],"source":["def detecter_date(chaine, jour_en_premier=True):\n","    try:\n","        return parse(chaine, dayfirst=jour_en_premier)\n","    except ValueError:\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"21wCcYU5_HUj"},"outputs":[],"source":["def formater_date(date):\n","    return date.strftime('%d/%m/%Y')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8MZQ4gTe_IhB"},"outputs":[],"source":["def formater_liste_dates(liste_dates, jour_en_premier=True):\n","    return [formater_date(detecter_date(date_str, jour_en_premier)) for date_str in liste_dates if detecter_date(date_str, jour_en_premier)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QsZyVjo9WtG7"},"outputs":[],"source":["def truncate_texts(texts, max_length=30):\n","    # 1. Vérifier si tous les textes contiennent \":\"\n","    if all(':' in text for text in texts):\n","        # Si oui, on ne garde que la partie avant le premier \":\" (trim)\n","        return [text.split(':', 1)[0].strip() for text in texts]\n","\n","    # 2. Sinon, on applique la logique de troncature initiale\n","    truncated_texts = []\n","    for text in texts:\n","        if len(text) <= max_length:\n","            truncated_texts.append(text)\n","            continue\n","\n","        # Trouve le dernier espace avant max_length\n","        last_space_index = text.rfind(' ', 0, max_length)\n","        if last_space_index == -1:\n","            # S'il n'y a pas d'espace, on coupe jusqu'à max_length et on ajoute \"...\"\n","            truncated_texts.append(text[:max_length] + \"...\")\n","        else:\n","            # Sinon, on coupe jusqu'au dernier espace et on ajoute \"...\"\n","            truncated_texts.append(text[:last_space_index] + \"...\")\n","\n","    return truncated_texts"]},{"cell_type":"code","source":["def solve_label_placement_matplotlib_2passes(\n","    ax,\n","    positions_and_labels,\n","    x_min, x_max, y_min, y_max,\n","    offsets_x, offsets_y,\n","    possible_ha=('left','center','right'),\n","    possible_va=('bottom','center','top')\n","):\n","    \"\"\"\n","    -------------------------------------------------------------------------\n","    SOLVEUR EN DEUX PASSES (méthode \"lexicographique\") en MILP :\n","      1) Minimiser la somme des distances (distance * 100000).\n","      2) À distance minimale égale, maximiser le nombre de labels\n","         en (ha='center', va='center').\n","\n","    positions_and_labels : liste de ((x_i, y_i), label_text).\n","    x_min, x_max, y_min, y_max : cadre à ne pas dépasser.\n","    offsets_x, offsets_y : listes des offsets qu'on souhaite tester.\n","\n","    Retourne : [(i, X, Y, ha, va, bbox, distance_reelle), ...]\n","       - i = indice du label\n","       - (X, Y) = position choisie\n","       - ha, va = alignements\n","       - bbox = (xmin, xmax, ymin, ymax)\n","       - distance_reelle = distance euclidienne (en \"data coords\") entre\n","         (x_i, y_i) et (X, Y)\n","    -------------------------------------------------------------------------\n","    \"\"\"\n","\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    #  1) GÉNÉRATION DE TOUTES LES POSITIONS CANDIDATES\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    candidate_positions = []\n","    for i, ((x_i, y_i), label_text) in enumerate(positions_and_labels):\n","        cands_for_i = []\n","        for dx in offsets_x:\n","            for dy in offsets_y:\n","                for ha in possible_ha:\n","                    for va in possible_va:\n","                        # Coordonnées où on place le label\n","                        X = x_i + dx\n","                        Y = y_i + dy\n","\n","                        # On mesure la bounding box (en coords data)\n","                        bbx_min, bbx_max, bby_min, bby_max = bounding_box_with_patch(\n","                            ax, label_text, X, Y, ha=ha, va=va,\n","                            bbox_style=dict(facecolor='white', edgecolor='black', alpha=0.1, boxstyle='square,pad=0.0')\n","                        )\n","\n","                        # On vérifie qu'elle reste dans le cadre\n","                        EPS = 1e-9\n","                        if (\n","                            bbx_min >= x_min - EPS and\n","                            bbx_max <= x_max + EPS and\n","                            bby_min >= y_min - EPS and\n","                            bby_max <= y_max + EPS\n","                        ):\n","                            # Calcul de la distance * 100000\n","                            dist = math.dist((x_i, y_i), (X, Y))\n","                            dist_int = int(round(dist * 100000))\n","\n","                            # Stockage\n","                            bbox_tup = (bbx_min, bbx_max, bby_min, bby_max)\n","                            cands_for_i.append((X, Y, ha, va, bbox_tup, dist_int))\n","        candidate_positions.append(cands_for_i)\n","\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    #  2) PREMIÈRE PASSE : MINIMISER LA SOMME DES DISTANCES (MILP)\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","    # Création du solveur MILP (CBC, par exemple)\n","    # - vous pouvez aussi essayer \"SCIP\", \"BOP\", etc. si installés\n","    solver_1 = pywraplp.Solver.CreateSolver('CBC')\n","    # Optionnel : on peut paramétrer, ex. solver_1.SetNumThreads(multiprocessing.cpu_count())\n","\n","    # Variables booléennes z1_{i,p}\n","    z1_vars = {}\n","    for i, cands_i in enumerate(candidate_positions):\n","        for p, cand_p in enumerate(cands_i):\n","            z1_vars[(i, p)] = solver_1.BoolVar(f\"z1_{i}_{p}\")\n","\n","    # Contrainte : un seul candidat p par label i\n","    for i, cands_i in enumerate(candidate_positions):\n","        solver_1.Add(\n","            sum(z1_vars[(i, p)] for p in range(len(cands_i))) == 1\n","        )\n","\n","    # Contrainte de non-chevauchement\n","    # On suppose que 'overlap(bbox1, bbox2)' renvoie True si overlap\n","    for i, cands_i in enumerate(candidate_positions):\n","        for p, cand_p in enumerate(cands_i):\n","            bbox_p = cand_p[4]\n","            for j in range(i+1, len(candidate_positions)):\n","                for q, cand_q in enumerate(candidate_positions[j]):\n","                    bbox_q = cand_q[4]\n","                    if overlap(bbox_p, bbox_q):\n","                        solver_1.Add(z1_vars[(i, p)] + z1_vars[(j, q)] <= 1)\n","\n","    # Création de l'objectif : somme des distances\n","    distance_expr_1 = solver_1.Sum(\n","        cand_p[5] * z1_vars[(i,p)]\n","        for i, cands_i in enumerate(candidate_positions)\n","        for p, cand_p in enumerate(cands_i)\n","    )\n","\n","    # Minimiser la somme des distances\n","    solver_1.Minimize(distance_expr_1)\n","\n","    # Résolution de la première passe\n","    status_1 = solver_1.Solve()\n","    if status_1 != pywraplp.Solver.OPTIMAL and status_1 != pywraplp.Solver.FEASIBLE:\n","        print(\"[2passes] Aucune solution lors de la première passe.\")\n","        return []\n","\n","    # Récupération de la valeur de l'objectif (distance totale)\n","    dist_min_float = solver_1.Objective().Value()\n","    dist_min_int = int(round(dist_min_float))\n","\n","    # Calcul \"à la main\" de sum_of_dist_int depuis la solution\n","    sum_of_dist_int = 0\n","    for i, cands_i in enumerate(candidate_positions):\n","        for p, cand_p in enumerate(cands_i):\n","            if z1_vars[(i,p)].solution_value() > 0.5:\n","                sum_of_dist_int += cand_p[5]\n","\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    #  3) DEUXIÈME PASSE : distance = dist_min, maximiser #center\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","\n","    # Nouveau solveur\n","    solver_2 = pywraplp.Solver.CreateSolver('CBC')\n","    # solver_2.SetNumThreads(multiprocessing.cpu_count())  # si vous voulez multi-threads\n","\n","    # Variables booléennes z2_{i,p}\n","    z2_vars = {}\n","    for i, cands_i in enumerate(candidate_positions):\n","        for p, cand_p in enumerate(cands_i):\n","            z2_vars[(i, p)] = solver_2.BoolVar(f\"z2_{i}_{p}\")\n","\n","    # Contrainte : un seul candidat p par label\n","    for i, cands_i in enumerate(candidate_positions):\n","        solver_2.Add(\n","            sum(z2_vars[(i, p)] for p in range(len(cands_i))) == 1\n","        )\n","\n","    # Même contrainte de non-chevauchement\n","    for i, cands_i in enumerate(candidate_positions):\n","        for p, cand_p in enumerate(cands_i):\n","            bbox_p = cand_p[4]\n","            for j in range(i+1, len(candidate_positions)):\n","                for q, cand_q in enumerate(candidate_positions[j]):\n","                    bbox_q = cand_q[4]\n","                    if overlap(bbox_p, bbox_q):\n","                        solver_2.Add(z2_vars[(i,p)] + z2_vars[(j,q)] <= 1)\n","\n","    # Expression de la distance totale en 2ᵉ passe\n","    distance_expr_2 = solver_2.Sum(\n","        cand_p[5] * z2_vars[(i,p)]\n","        for i, cands_i in enumerate(candidate_positions)\n","        for p, cand_p in enumerate(cands_i)\n","    )\n","\n","    # On fixe la distance = dist_min_int\n","    #   Selon les besoins, on peut autoriser un +/- 1e-9 si nécessaire.\n","    solver_2.Add(distance_expr_2 == dist_min_int)\n","\n","    # On veut maximiser le nombre de \"center\"\n","    #   On compte 1 si ha == 'center', +1 si va == 'center'\n","    #   => total = sum over i,p of center_score(i,p)*z2_{i,p}\n","    center_expr_terms = []\n","    for i, cands_i in enumerate(candidate_positions):\n","        for p, cand_p in enumerate(cands_i):\n","            (_, _, ha, va, _, _) = cand_p\n","            center_score = 0\n","            if ha == 'center':\n","                center_score += 1\n","            if va == 'center':\n","                center_score += 1\n","\n","            if center_score > 0:\n","                # contribution = center_score * z2_{i,p}\n","                center_expr_terms.append(center_score * z2_vars[(i,p)])\n","\n","    center_expr = solver_2.Sum(center_expr_terms)\n","\n","    # Objectif 2 : maximiser la somme des \"center scores\"\n","    solver_2.Maximize(center_expr)\n","\n","    # Résolution de la 2ᵉ passe\n","    status_2 = solver_2.Solve()\n","    if status_2 != pywraplp.Solver.OPTIMAL and status_2 != pywraplp.Solver.FEASIBLE:\n","        print(\"[2passes] Aucune solution en 2e passe.\")\n","        return []\n","\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    #  4) EXTRACTION FINALE DE LA SOLUTION\n","    # ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n","    solution = []\n","    for i, cands_i in enumerate(candidate_positions):\n","        for p, cand_p in enumerate(cands_i):\n","            if z2_vars[(i,p)].solution_value() > 0.5:\n","                (X, Y, ha, va, bbox_tup, dist_int) = cand_p\n","                dist_reelle = dist_int / 100000.0\n","                solution.append((i, X, Y, ha, va, bbox_tup, dist_reelle))\n","                break  # on a trouvé le p sélectionné pour ce i\n","\n","    return solution"],"metadata":{"id":"EZTO-yL0hOQZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def overlap(bbox1, bbox2):\n","    \"\"\"\n","    Teste si deux bounding boxes se chevauchent strictement.\n","    bbox = (xmin, xmax, ymin, ymax)\n","    \"\"\"\n","    return not (\n","        bbox1[1] < bbox2[0] or  # bbox1.xmax < bbox2.xmin\n","        bbox1[0] > bbox2[1] or  # bbox1.xmin > bbox2.xmax\n","        bbox1[3] < bbox2[2] or  # bbox1.ymax < bbox2.ymin\n","        bbox1[2] > bbox2[3]     # bbox1.ymin > bbox2.ymax\n","    )"],"metadata":{"id":"teQHi2cMNizM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def bounding_box_with_patch(ax,\n","                            label_text,\n","                            x, y,\n","                            ha='left', va='center',\n","                            bbox_style=None):\n","    \"\"\"\n","    Crée *temporairement* un texte invisible,\n","    avec EXACTEMENT le bbox dict(...) que vous utiliserez pour l’affichage,\n","    puis récupère la bbox de ce patch, en coordonnées data (ax).\n","\n","    Retourne (xmin, xmax, ymin, ymax).\n","    \"\"\"\n","\n","    # 1) On crée un objet Text, invisible (color='none'),\n","    #    MAIS avec le même bbox que l'affichage final\n","    t = ax.text(\n","        x, y, label_text,\n","        ha=ha, va=va,\n","        color='none',\n","        bbox=dict(facecolor='white', edgecolor='black', alpha=0.1, boxstyle='square,pad=0.0')\n","    )\n","\n","    # 2) Forcer le dessin pour que le patch soit calculé\n","    ax.figure.canvas.draw()\n","\n","    # 3) Récupérer la bounding box du patch (le cadre gris)\n","    patch = t.get_bbox_patch()\n","    if patch is not None:\n","        bbox = patch.get_window_extent()\n","    else:\n","        # fallback, au cas où (rare)\n","        bbox = t.get_window_extent()\n","\n","    # 4) Convertir la bbox en coords \"data\"\n","    bbox_data = bbox.transformed(ax.transData.inverted())\n","\n","    # 5) Supprimer le texte temporaire\n","    t.remove()\n","\n","    # 6) Retour (xmin, xmax, ymin, ymax)\n","    return (bbox_data.x0, bbox_data.x1, bbox_data.y0, bbox_data.y1)"],"metadata":{"id":"cDddeTP1NlAa"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bHy81u94W7OV"},"outputs":[],"source":["def plot_pca(matrix_type='W'):\n","    \"\"\"\n","    all_nmf_H : dict[ int -> ndarray ]\n","        Dictionnaire où all_nmf_H[topic_count] est une matrice H (shape = (k, m))\n","        k = nombre de topics, m = taille du vocabulaire.\n","    all_nmf_W : iterable\n","        Liste (ou clés du dict) indiquant les différents topic_count disponibles.\n","    \"\"\"\n","\n","    if not os.path.exists(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\"):\n","        os.makedirs(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\")\n","\n","    for topic_count in all_nmf_H:\n","        if matrix_type == 'W':\n","            M = all_nmf_W[topic_count].T  # Matrice (k x m)\n","        else:\n","            M = all_nmf_H[topic_count]\n","\n","        # ---------------------------\n","        # 1) Normalisation L2 par topic (chaque ligne)\n","        # ---------------------------\n","        # On calcule la norme L2 de chaque ligne (axis=1)\n","        norms = np.linalg.norm(M, axis=1, keepdims=True)\n","        # Pour éviter la division par zéro si une ligne est totalement nulle\n","        norms[norms == 0] = 1e-16\n","\n","        M_norm = M / norms  # Division élément par élément\n","\n","        # ---------------------------\n","        # 2) PCA sur H normalisé\n","        # ---------------------------\n","        pca = PCA(n_components=2)\n","        pca_result = pca.fit_transform(M_norm)  # shape = (k, 2)\n","\n","\n","        # 2) Préparation de la figure/axe\n","        fig, ax = plt.subplots(\n","            figsize=(FIGURE_WIDTH_INCH, FIGURE_WIDTH_INCH),\n","            dpi=DPI\n","        )\n","\n","        # 3) Construire la liste (positions, labels)\n","        labels = [f'{i}' for i in range(len(pca_result))]\n","        truncated_texts = truncate_texts(topic_labels_by_config[topic_count])\n","\n","        # On associe chaque point PCA à un label\n","        positions_and_labels = [\n","            (tuple(coords), truncated_texts[int(lbl)])\n","            for coords, lbl in zip(pca_result, labels)\n","        ]\n","\n","        # 4) Calculer le min/max pour x et y (cadre à ne pas dépasser)\n","        all_x = [pos_lbl[0][0] for pos_lbl in positions_and_labels]\n","        all_y = [pos_lbl[0][1] for pos_lbl in positions_and_labels]\n","        x_min, x_max = min(all_x), max(all_x)\n","        y_min, y_max = min(all_y), max(all_y)\n","\n","        ax.set_xlim(x_min, x_max)\n","        ax.set_ylim(y_min, y_max)\n","        ax.autoscale(False)\n","\n","        # 5) Paramètres pour solve_label_placement_matplotlib\n","        # 1) Calcul de l'étendue (range) de l'axe\n","        range_x = x_max - x_min\n","        range_y = y_max - y_min\n","\n","        # 2) Choix du nombre d'offsets\n","        num_offsets = 11\n","\n","        # 3) Génération des pourcentages entre -5% et +5% (en 10 pas)\n","        #    => np.linspace(-0.05, 0.05, num_offsets)\n","        #    sera par exemple [-0.05, -0.0388, ..., 0.05]\n","\n","        percentages_x = np.linspace(-0.05, 0.05, num_offsets)\n","        percentages_y = np.linspace(-0.05, 0.05, num_offsets)\n","\n","        # 4) Conversion de ces pourcentages en offsets dans les coordonnées du graphique\n","        offsets_x = [0] #[p * range_x for p in percentages_x]\n","        offsets_y = [0] #[p * range_y for p in percentages_y]\n","\n","        possible_ha = ['left', 'center', 'right']\n","        possible_va = ['bottom', 'center', 'top']\n","\n","        # 6) Appel du solveur (qui va mesurer la bbox via ax)\n","        solution = solve_label_placement_matplotlib_2passes(\n","            ax=ax,\n","            positions_and_labels=positions_and_labels,\n","            x_min=x_min, x_max=x_max,\n","            y_min=y_min, y_max=y_max,\n","            offsets_x=offsets_x, offsets_y=offsets_y,\n","            possible_ha=possible_ha, possible_va=possible_va\n","        )\n","\n","        # 7) Affichage de la solution\n","        for (i, X, Y, ha, va, bbox, cost) in solution:\n","            (ox, oy), text_label = positions_and_labels[i]\n","\n","            # Points d'origine (optionnel si on veut les voir en plus du scatter)\n","            ax.plot(ox, oy, color='red', marker='o', alpha=0.5, markersize=10, markeredgewidth=0)\n","\n","            # Le label positionné\n","            ax.text(X, Y, text_label, ha=ha, va=va,\n","                    bbox=dict(facecolor='white', edgecolor='black', alpha=0.1, boxstyle='square,pad=0.0'))\n","\n","            # Une flèche qui relie le point d'origine au label\n","            ax.annotate(\n","                \"\",\n","                xy=(ox, oy),\n","                xytext=(X, Y),\n","                arrowprops=dict(arrowstyle=\"->\", color='black', alpha=0.2)\n","            )\n","\n","        explained_var_ratio = pca.explained_variance_ratio_\n","\n","        manual_tick_placement_continuous(\n","            ax=ax,\n","            xmin=x_min,\n","            xmax=x_max,\n","            spacing_factor_min=1.02,\n","            spacing_factor_max=1.2,\n","            step=0.001\n","        )\n","        manual_tick_placement_continuous_Y(\n","            ax=ax,\n","            ymin=y_min,\n","            ymax=y_max,\n","            spacing_factor_min=1.02,\n","            spacing_factor_max=1.2,\n","            step=0.001\n","        )\n","\n","        # 15) Labels des axes, etc.\n","        plt.xlabel(\n","            f'Facteur 1 - Variance expliquée={explained_var_ratio[0]*100:.2f}%',\n","            labelpad=35\n","        )\n","        plt.ylabel(\n","            f'Facteur 2 - Variance expliquée={explained_var_ratio[1]*100:.2f}%',\n","            labelpad=34\n","        )\n","\n","        # Supprimer la bordure du haut et de droite\n","        ax.spines['top'].set_visible(False)\n","        ax.spines['right'].set_visible(False)\n","\n","        # Ligne horizontale y=0\n","        plt.axhline(0, color='black', linewidth=1, alpha=0.3)\n","        # Ligne verticale x=0\n","        plt.axvline(0, color='black', linewidth=1, alpha=0.3)\n","\n","        # On désactive la grille\n","        plt.grid(False)\n","\n","        class_suffix = \"_\".join(grammatical_classes)\n","\n","        if not os.path.exists(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\"):\n","            os.makedirs(f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\")\n","\n","        # 9) Afficher la figure\n","        plt.savefig(\n","            f\"{results_path}{base_name}_L2_{matrix_type}_PCA_PLOTS/\"\n","            f\"{base_name}_{matrix_type.lower()}_pca_plot_{topic_count}tc_l2_{class_suffix}_\"\n","            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","            f\"{go_remove_duplicates}dup.png\",\n","            dpi=DPI,\n","            bbox_inches='tight',\n","            pad_inches=0\n","        )\n","        plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cYht2nb3KBA3"},"outputs":[],"source":["def is_overlapping(text, other_texts, tolerance=0.0, buffer=0.0):\n","    \"\"\"\n","    Vérifie si un texte se chevauche avec d'autres, avec une tolérance très permissive.\n","\n","    Args:\n","        text: L'objet texte à tester.\n","        other_texts: Liste des objets textes existants.\n","        tolerance: Proportion de tolérance (plus grand = plus tolérant).\n","        buffer: Distance minimale entre les boîtes pour ignorer un chevauchement léger.\n","\n","    Returns:\n","        bool: True si chevauchement significatif, sinon False.\n","    \"\"\"\n","    bbox = text.get_window_extent(renderer=plt.gcf().canvas.get_renderer())\n","    bbox_data = bbox.transformed(plt.gca().transData.inverted())  # Conversion en coordonnées data\n","\n","    for other in other_texts:\n","        other_bbox = other.get_window_extent(renderer=plt.gcf().canvas.get_renderer())\n","        other_bbox_data = other_bbox.transformed(plt.gca().transData.inverted())\n","\n","        # Calcul des dimensions avec \"buffer\" pour agrandir légèrement les boîtes existantes\n","        bbox_data_inflated = [\n","            bbox_data.xmin - buffer, bbox_data.xmax + buffer,\n","            bbox_data.ymin - buffer, bbox_data.ymax + buffer\n","        ]\n","        other_bbox_data_inflated = [\n","            other_bbox_data.xmin - buffer, other_bbox_data.xmax + buffer,\n","            other_bbox_data.ymin - buffer, other_bbox_data.ymax + buffer\n","        ]\n","\n","        # Vérifier le chevauchement agrandi\n","        overlap_x = max(0, min(bbox_data_inflated[1], other_bbox_data_inflated[1]) -\n","                           max(bbox_data_inflated[0], other_bbox_data_inflated[0]))\n","        overlap_y = max(0, min(bbox_data_inflated[3], other_bbox_data_inflated[3]) -\n","                           max(bbox_data_inflated[2], other_bbox_data_inflated[2]))\n","\n","        # Surface d'intersection\n","        overlap_area = overlap_x * overlap_y\n","\n","        # Aire minimale de chevauchement tolérée\n","        area_threshold = tolerance * (bbox_data.width * bbox_data.height)\n","\n","        # Ignorer les chevauchements inférieurs à la tolérance\n","        if overlap_area > area_threshold:\n","            return True\n","    return False"]},{"cell_type":"code","source":["def tokenize_and_stem(args):\n","    atb, unigrams = args\n","    tokenized_sents = []\n","    for t in atb:\n","        if t[0] in unigrams:\n","            tokenized_sents.append(t[0])\n","\n","    return tokenized_sents"],"metadata":{"id":"ZPrd5G30IMuS"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvNMOepHffb9"},"outputs":[],"source":["def go_tfidf_vectorization(gclasses):\n","    # 1) Traiter les classes grammaticales définies globalement\n","    #    On ajoute tqdm pour visualiser l'avancement\n","    global unigrams\n","\n","    for grammatical_class in tqdm(gclasses, desc=\"Mise à jour des unigrams\"):\n","        unigrams = update_candidates_for_unigram(grammatical_class, unigrams)\n","\n","    tokenized_documents = []\n","    for atb in all_tab_pos:\n","        tokenized_document = [t[0] for t in atb if t[0] in unigrams]\n","        tokenized_documents.append(tokenized_document)\n","\n","    # 4) Suppression des stop words via le pipeline spaCy\n","    spacy_stopwords = nlp_pipeline.Defaults.stop_words\n","    # Si on veut voir la progression ici, on peut boucler :\n","    filtered_docs = []\n","    for doc in tqdm(tokenized_documents, desc=\"Filtrage des stopwords\"):\n","        filtered_docs.append(\n","            [token for token in doc if token.lower() not in spacy_stopwords]\n","        )\n","    tokenized_documents = filtered_docs\n","\n","    # 5) Création des vecteurs TF-IDF\n","    #    Pour avoir une barre de progression, on peut découper manuellement en batches.\n","    #    Toutefois, si la liste n'est pas trop grosse, on peut juste faire fit_transform d'un coup.\n","\n","    def identity_analyzer(tokens):\n","        return tokens\n","\n","    count_vectorizer = CountVectorizer(analyzer=identity_analyzer, lowercase=False)\n","\n","    # Si vous voulez découper en batches pour CountVectorizer.fit_transform,\n","    # il faut recourir à un autre mécanisme (car le fit_transform standard ne propose pas de batch).\n","    # Par défaut, on fait donc un fit_transform \"classique\" :\n","    word_count = count_vectorizer.fit_transform(tokenized_documents)\n","\n","    tfidf_transformer = TfidfTransformer(norm=None, sublinear_tf=False, smooth_idf=True)\n","    X = tfidf_transformer.fit_transform(word_count)\n","\n","    tfidf_feature_names = count_vectorizer.get_feature_names_out()\n","\n","    return count_vectorizer, X, tfidf_feature_names, tokenized_documents, tfidf_transformer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MHHdg_Ocffb9"},"outputs":[],"source":["def write_unigrams_results(nb_words, tfidf_feature_names, nmf_H):\n","    tab_words_nmf = []\n","    for topic_idx, topic in enumerate(nmf_H):\n","      subtab_words_nmf = []\n","      for i in topic.argsort()[:-nb_words - 1:-1]:\n","        subtab_words_nmf.append([(tfidf_feature_names[i]), topic[i]])\n","\n","      tab_words_nmf.append(subtab_words_nmf)\n","\n","\n","    new_tab_words_nmf = []\n","    for t in tab_words_nmf:\n","      sorted_t = sorted(t, key = lambda x: (-x[1]))\n","\n","      new_tab_words_nmf.append(sorted_t)\n","\n","\n","\n","    max_rows_nb = 0\n","    for to in new_tab_words_nmf:\n","      if len(to) > max_rows_nb:\n","        max_rows_nb = len(to)\n","\n","\n","    if not os.path.exists(f\"{results_path}{base_name}_EXPLORE_TOPICS/\"):\n","        os.makedirs(f\"{results_path}{base_name}_EXPLORE_TOPICS/\")\n","\n","    class_suffix = \"_\".join(grammatical_classes)\n","    with open(\n","        f\"{results_path}{base_name}_EXPLORE_TOPICS/\"\n","        f\"{base_name}_{len(nmf_H)}tc_topic_modeling_unigrams_{class_suffix}_\"\n","        f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","        f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp.csv\",\n","        \"w\",\n","        encoding=\"utf-8\"\n","    ) as file_object:\n","        writer = csv.writer(file_object)\n","\n","        i = 0\n","        while i < max_rows_nb:\n","            new_row = \"\"\n","            for to in new_tab_words_nmf:\n","                if i < len(to):\n","                    if len(new_row) > 0:\n","                        new_row = new_row + \",\" + (to[i][0]) + \",\" + str(to[i][1])\n","                    else:\n","                        new_row = (to[i][0]) + \",\" + str(to[i][1])\n","\n","            file_object.write(new_row)\n","            file_object.write(\"\\n\")\n","\n","            i += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9ferTRgwzYDN"},"outputs":[],"source":["def determine_nmf(topic_list, alpha_W, alpha_H, l1_ratio, n_top_words=15, window_size=100):\n","    \"\"\"\n","    Entraîne un modèle NMF pour chaque nombre de topics dans topic_list\n","    et calcule la métrique de cohérence (type sliding window) c_npmi.\n","\n","    Paramètres\n","    ----------\n","    topic_list : list\n","        Liste des nombres de topics à tester (ex. [5, 10, 15]).\n","    n_top_words : int\n","        Nombre de mots que l’on va extraire pour chaque topic (top words).\n","    window_size : int\n","        Taille de la fenêtre de co-occurrence pour la cohérence (sliding window).\n","\n","    Retour\n","    ------\n","    None. (Les résultats sont directement enregistrés dans les dictionnaires globaux.)\n","    \"\"\"\n","\n","    # On indique qu’on va modifier à ces variables globales\n","    global all_nmf_H, all_nmf_W\n","    global coherence_scores\n","\n","    # 1. Construire le dictionary Gensim à partir des documents tokenisés\n","    dictionary = Dictionary(tokenized_documents)\n","    # Optionnel : filtrer les tokens trop rares ou trop fréquents\n","    # dictionary.filter_extremes(no_below=5, no_above=0.5)\n","\n","    # 2. Pour chaque valeur de topics dans topic_list, on entraîne un modèle NMF\n","    nmf_models = {}\n","    for num_topic in tqdm(topic_list, desc=\"PROCESSUS DES TOPICS\"):\n","        nmf_model = NMF(\n","            n_components=num_topic,\n","            random_state=1,\n","            max_iter=10000,\n","            alpha_W=alpha_W,      # remplace alpha=0.2\n","            alpha_H=alpha_H,      # idem\n","            l1_ratio=l1_ratio,  # Ratio proche de 0 => plus de L2\n","            init='nndsvd'\n","        ).fit(tfidf)\n","\n","        nmf_W = nmf_model.transform(tfidf)\n","        nmf_H = nmf_model.components_\n","\n","        # Stocker les matrices W et H dans les dictionnaires globaux\n","        all_nmf_W[num_topic] = nmf_W\n","        all_nmf_H[num_topic] = nmf_H\n","\n","        # Initialiser pour ce num_topic\n","        all_topics_and_scores_by_document = {}\n","\n","        # 3. Extraire les top words de chaque topic\n","        topic_words = []\n","        for t in range(num_topic):\n","            # Trouver les indices des \"top n_top_words\" en ordre décroissant\n","            top_word_indexes = nmf_H[t].argsort()[:-n_top_words-1:-1]\n","            # Récupérer les mots associés à ces indices\n","            words_for_topic_t = [tfidf_feature_names[idx] for idx in top_word_indexes]\n","            topic_words.append(words_for_topic_t)\n","\n","        # 4. Calculer la cohérence par fenêtre glissante\n","        coherence_model = CoherenceModel(\n","            topics=topic_words,\n","            texts=tokenized_documents,\n","            dictionary=dictionary,\n","            coherence='c_npmi',   # ou 'c_uci' si souhaité\n","            window_size=window_size\n","        )\n","        coherence_score = coherence_model.get_coherence()\n","        coherence_scores[num_topic] = coherence_score\n","\n","        nmf_models[num_topic] = nmf_model\n","\n","    # 7. Fonctions de post-traitement (optionnelles)\n","    write_topics_unigrams()\n","\n","    return nmf_models"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uwmnFsq1yawa"},"outputs":[],"source":["def process_documents(documents):\n","    # Calcul du nombre de cœurs disponibles\n","    gpu_activated = spacy.prefer_gpu()  # True si GPU détecté, sinon None\n","\n","    print('gpu_activated', gpu_activated)\n","\n","    # n_process=1 si on a un GPU, sinon on utilise tous les CPU\n","    n_process = 1 if gpu_activated else multiprocessing.cpu_count()\n","\n","    print(f\"Utilisation de {n_process} processus parallèles pour spaCy.\")\n","\n","    # On s'assure de (ré)initialiser les tableaux globaux si nécessaire\n","    global documents_lemmatized, all_tab_pos, sentences_norms, all_sentence_pos\n","    documents_lemmatized = []\n","    all_tab_pos = []\n","    all_sentence_pos = []\n","    sentences_norms = []\n","\n","    # Préparer une barre de progression\n","    pbar = tqdm(total=len(documents), desc='DOCUMENTS PROCESSÉS')\n","\n","    # Traitement en parallèle avec nlp.pipe\n","    try:\n","        # Par défaut, spaCy divise en batch de ~1000 tokens.\n","        # On peut ajuster batch_size si besoin (ex: batch_size=20 ou 50).\n","        for spacy_doc in nlp_pipeline.pipe(documents, n_process=n_process, batch_size=20):\n","            doc_for_ngrams = ''\n","            tab_pos = []\n","\n","            for sent in spacy_doc.sents:\n","                sentence_pos = []\n","                norms = []\n","                lemmes = []\n","\n","                for token in sent:\n","                    pos = token.pos_\n","                    lemma = token.lemma_.lower()\n","\n","                    # Exemple: unidecode si c'est un PROPN\n","                    if pos == 'PROPN':\n","                        lemma = unidecode.unidecode(lemma)\n","\n","                    if lemma not in [\" \", \"\\n\", \"\\t\"]:\n","                        doc_for_ngrams += lemma + ' '\n","                        tab_pos.append([lemma, pos])\n","                        sentence_pos.append([lemma, pos])\n","                        lemmes.append(lemma)\n","                        norms.append(token.norm_)\n","\n","                sentences_norms.append(\" \".join(norms))\n","\n","                all_sentence_pos.append(sentence_pos)\n","\n","            documents_lemmatized.append(doc_for_ngrams)\n","            all_tab_pos.append(tab_pos)\n","\n","            pbar.update(1)\n","\n","    except Exception as e:\n","        print(f\"Erreur lors du traitement des documents : {e}\")\n","\n","    pbar.close()\n","\n","    # Écriture des résultats sur disque (ou autre)\n","    write_raw_documents()\n","    write_lemmatized_documents()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TsIGROXDJ5e8"},"outputs":[],"source":["def extract_and_convert_date(date_str):\n","    try:\n","        return parser.parse(date_str)\n","    except (parser.ParserError, ValueError):\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DAw3i_UqzYDO"},"outputs":[],"source":["def write_raw_documents():\n","    if not os.path.exists(f\"{results_path}{base_name}_RAW/\"):\n","            os.makedirs(f\"{results_path}{base_name}_RAW/\")\n","\n","    with open(f\"{results_path}{base_name}_RAW/raw_documents.txt\", \"w\", encoding='utf-8') as file_object:\n","        for dfn in documents:\n","            file_object.write(dfn + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X6QrgrEG_2zh"},"outputs":[],"source":["def write_lemmatized_documents():\n","    with open(f\"{results_path}{base_name}_RAW/{base_name}_lemmatized_documents.txt\",\n","              \"w\",\n","              encoding='utf-8') as file_object:\n","        for dfn in documents_lemmatized:\n","            file_object.write(dfn + '\\n')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3l4UeX3yzYDO"},"outputs":[],"source":["def extract_information(header, selector):\n","    elements = header.select(selector)\n","    if elements:\n","        return \"////\".join([get_text_from_tag(el).replace(';', ',') for el in elements])\n","    else:\n","        return \"N/A\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ikKezsdQzYDO"},"outputs":[],"source":["def get_text_from_tag(tag):\n","    return ''.join(tag.strings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oryU-8PozYDO"},"outputs":[],"source":["def normalize_journal(t):\n","    t = t.strip()\n","\n","    # Supprimer tout ce qui se trouve entre parenthèses (y compris les parenthèses)\n","    t = re.sub(r'\\(.*?\\)', '', t)\n","\n","    # Supprimer tout ce qui se trouve après la première virgule\n","    t = re.sub(r',.*', '', t)\n","\n","    # Supprimer tout ce qui se trouve après le premier tiret précédé d'un espace\n","    t = re.sub(r' -.*', '', t)\n","\n","    # Supprimer tout ce qui suit trois espaces vides ou plus\n","    t = re.sub(r' {3,}.*', '', t)\n","\n","    if not web_paper_differentiation:\n","        # Supprimer les préfixes \"www.\"\n","        t = re.sub(r'^www\\.', '', t)\n","\n","        # Supprimer les extensions de domaine\n","        t = re.sub(r'(\\.\\w{2,3})+$', '', t)\n","\n","    # Trim le texte\n","    t = t.strip()\n","\n","    return t.lower()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GEWRP5HBzYDO"},"outputs":[],"source":["def extract_date_info(date_text, language='fr'):\n","    if language == 'fr':\n","        regex = \"([1-3]?[0-9]\\\\s(janvier|février|mars|avril|mai|juin|juillet|août|septembre|octobre|novembre|décembre)\\\\s20[0-2][0-9])\"\n","    elif language == 'en':\n","        regex = \"([1-3]?[0-9]\\\\s(January|February|March|April|May|June|July|August|September|October|November|December)\\\\s20[0-2][0-9])\"\n","\n","    date_text_clean = re.search(regex, date_text)\n","    return date_text_clean.group() if date_text_clean else date_text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d2u6MilUzYDO"},"outputs":[],"source":["def normalise_date(date_text):\n","    # Dictionnaire combiné des mois en anglais, français, espagnol et allemand avec leurs variations\n","    month_dict = {\n","        # Mois en anglais\n","        'january': '01', 'jan': '01',\n","        'february': '02', 'feb': '02',\n","        'march': '03', 'mar': '03',\n","        'april': '04', 'apr': '04',\n","        'may': '05',\n","        'june': '06', 'jun': '06',\n","        'july': '07', 'jul': '07',\n","        'august': '08', 'aug': '08',\n","        'september': '09', 'sep': '09', 'sept': '09',\n","        'october': '10', 'oct': '10',\n","        'november': '11', 'nov': '11',\n","        'december': '12', 'dec': '12',\n","        # Mois en français\n","        'janvier': '01', 'janv.': '01', 'janv': '01',\n","        'février': '02', 'févr.': '02', 'févr': '02', 'fevrier': '02', 'fevr': '02',\n","        'mars': '03',\n","        'avril': '04', 'avr.': '04', 'avr': '04',\n","        'mai': '05',\n","        'juin': '06',\n","        'juillet': '07', 'juil.': '07', 'juil': '07',\n","        'août': '08', 'aout': '08', 'aôut': '08',\n","        'septembre': '09', 'sept.': '09', 'sept': '09',\n","        'octobre': '10', 'oct.': '10', 'oct': '10',\n","        'novembre': '11', 'nov.': '11', 'nov': '11',\n","        'décembre': '12', 'déc.': '12', 'déc': '12', 'decembre': '12', 'dec': '12',\n","        # Mois en espagnol\n","        'enero': '01', 'ene.': '01', 'ene': '01',\n","        'febrero': '02', 'feb.': '02', 'feb': '02',\n","        'marzo': '03', 'mar.': '03', 'mar': '03',\n","        'abril': '04', 'abr.': '04', 'abr': '04',\n","        'mayo': '05', 'may.': '05', 'may': '05',\n","        'junio': '06', 'jun.': '06', 'jun': '06',\n","        'julio': '07', 'jul.': '07', 'jul': '07',\n","        'agosto': '08', 'ago.': '08', 'ago': '08',\n","        'septiembre': '09', 'sept.': '09', 'sep': '09', 'setiembre': '09', 'set.': '09', 'set': '09',\n","        'octubre': '10', 'oct.': '10', 'oct': '10',\n","        'noviembre': '11', 'nov.': '11', 'nov': '11',\n","        'diciembre': '12', 'dic.': '12', 'dic': '12',\n","        # Mois en allemand\n","        'januar': '01', 'jan.': '01', 'jan': '01',\n","        'februar': '02', 'feb.': '02', 'feb': '02',\n","        'märz': '03', 'maerz': '03', 'mär.': '03', 'marz': '03', 'mar.': '03', 'mar': '03',\n","        'april': '04', 'apr.': '04', 'apr': '04',\n","        'mai': '05',\n","        'juni': '06', 'jun.': '06', 'jun': '06',\n","        'juli': '07', 'jul.': '07', 'jul': '07',\n","        'august': '08', 'aug.': '08', 'aug': '08',\n","        'september': '09', 'sept.': '09', 'sep': '09', 'sept': '09',\n","        'oktober': '10', 'okt.': '10', 'okt': '10',\n","        'november': '11', 'nov.': '11', 'nov': '11',\n","        'dezember': '12', 'dez.': '12', 'dez': '12'\n","    }\n","\n","    # Nettoyer le texte de la date\n","    date_text = date_text.lower().strip()\n","\n","    # Liste unifiée des formats de dates à essayer\n","    date_formats = [\n","        # Exemples : 19 de noviembre de 2021, 19 novembre 2021, 19 november 2021, 19. November 2021\n","        r\"(?:\\b\\w+\\b,\\s+)?(\\d{1,2})(?:\\.|\\s+de|\\s+)?\\s*([\\w\\.\\-]+)(?:\\s+de)?\\s+(\\d{4})\",\n","        # Exemples : noviembre 19, 2021, november 19, 2021\n","        r\"(?:\\b\\w+\\b,\\s+)?([\\w\\.\\-]+)\\s+(\\d{1,2}),?\\s+(\\d{4})\",\n","        # Formats numériques : 19/11/2021, 11/19/2021\n","        r\"(\\d{1,2})/(\\d{1,2})/(\\d{4})\",\n","        # Formats numériques avec tirets : 19-11-2021, 11-19-2021\n","        r\"(\\d{1,2})-(\\d{1,2})-(\\d{4})\",\n","        # Année en premier : 2021-11-19\n","        r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\",\n","        # Année en premier avec slash : 2021/11/19\n","        r\"(\\d{4})/(\\d{1,2})/(\\d{1,2})\",\n","        # Formats avec points : 19.11.2021\n","        r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\",\n","    ]\n","\n","    for pattern in date_formats:\n","        match = re.search(pattern, date_text, re.IGNORECASE)\n","        if match:\n","            groups = match.groups()\n","            # Déterminer l'ordre des éléments en fonction du motif\n","            if pattern.startswith(r\"(?:\\b\\w+\\b,\\s+)?(\\d{1,2})\"):\n","                # Motif : Jour [de] Mois [de] Année (ex : 19 de noviembre de 2021)\n","                day, month, year = groups\n","            elif pattern.startswith(r\"(?:\\b\\w+\\b,\\s+)?([\\w\\.\\-]+)\"):\n","                # Motif : Mois Jour, Année (ex : noviembre 19, 2021)\n","                month, day, year = groups\n","            elif pattern.startswith(r\"(\\d{1,2})/(\\d{1,2})/\"):\n","                # Motif : Numérique avec slash (ambigu)\n","                first, second, year = groups\n","                if int(first) > 12:\n","                    # Probablement Jour/Mois/Année\n","                    day, month = first, second\n","                elif int(second) > 12:\n","                    # Probablement Mois/Jour/Année\n","                    month, day = first, second\n","                else:\n","                    # Ambigu, par défaut Jour/Mois/Année\n","                    day, month = first, second\n","                day = day.zfill(2)\n","                month = month.zfill(2)\n","                return f\"{year}-{month}-{day}\"\n","            elif pattern.startswith(r\"(\\d{1,2})-(\\d{1,2})-\"):\n","                # Motif : Numérique avec tirets (ambigu)\n","                first, second, year = groups\n","                if int(first) > 12:\n","                    day, month = first, second\n","                elif int(second) > 12:\n","                    month, day = first, second\n","                else:\n","                    day, month = first, second\n","                day = day.zfill(2)\n","                month = month.zfill(2)\n","                return f\"{year}-{month}-{day}\"\n","            elif pattern.startswith(r\"(\\d{4})-(\\d{1,2})-(\\d{1,2})\"):\n","                # Motif : Année-Mois-Jour\n","                year, month, day = groups\n","            elif pattern.startswith(r\"(\\d{4})/(\\d{1,2})/(\\d{1,2})\"):\n","                # Motif : Année/Mois/Jour\n","                year, month, day = groups\n","            elif pattern.startswith(r\"(\\d{1,2})\\.(\\d{1,2})\\.(\\d{4})\"):\n","                # Motif : Jour.Mois.Année\n","                day, month, year = groups\n","            else:\n","                # Motif non reconnu\n","                continue\n","\n","            month = month.lower().replace('.', '').strip()\n","            day = day.zfill(2)\n","\n","            # Convertir le mois en chiffre\n","            if month.isdigit():\n","                month_num = month.zfill(2)\n","            elif month in month_dict:\n","                month_num = month_dict[month]\n","            else:\n","                print(f\"Attention, mois non reconnu : {month}\")\n","                continue\n","\n","            return f\"{year}-{month_num}-{day}\"\n","\n","    print('Attention, date non gérée :', date_text)\n","    # Retourner None si aucun format n'est reconnu\n","    return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4kkopFTdzYDP"},"outputs":[],"source":["def standardize_name(name):\n","    words = name.split()\n","    words.sort()\n","    return ' '.join(words)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iKrisLVXzYDP"},"outputs":[],"source":["def split_names(s):\n","    words = s.split()\n","    if len(words) == 4:\n","        first_name = ' '.join(words[:2])\n","        second_name = ' '.join(words[2:])\n","        return [first_name, second_name]\n","    elif len(words) == 6:\n","        first_name = ' '.join(words[0:2])\n","        second_name = ' '.join(words[2:4])\n","        third_name = ' '.join(words[4:6])\n","        return [first_name, second_name, third_name]\n","    elif len(words) == 8:\n","        first_name = ' '.join(words[0:2])\n","        second_name = ' '.join(words[2:4])\n","        third_name = ' '.join(words[4:6])\n","        fourth_name = ' '.join(words[4:6])\n","        return [first_name, second_name, third_name, fourth_name]\n","\n","    return [s]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JK_v1A9Geni3"},"outputs":[],"source":["def transform_text(text):\n","    text = text.replace('\\n', ' ')\n","    text = text.replace('\\t', ' ')\n","\n","   # text = re.sub(r'[-–—‑‒−]', ' ', text)\n","    text = re.sub(r'\\s+', ' ', text)\n","\n","    # écriture inclusive\n","    text = text.replace('(e)', '')\n","    text = text.replace('(E)', '')\n","    text = text.replace('.e.', '')\n","    text = text.replace('.E.', '')\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yjo8OSp5zYDP"},"outputs":[],"source":["def extract_names(line):\n","    if len(line) > 150:\n","        return None\n","\n","    # Supprimer tout ce qui est entre parenthèses\n","    line = re.sub(r'\\(.*?\\)', '', line)\n","\n","    # Ignorer les lignes qui contiennent des domaines ou \"N/A\"\n","    if re.search(r'(\\.fr|\\.com|n/a)', line):\n","        return None\n","\n","    line = re.sub(r'\\s?@\\w+', '', line)\n","    line = line.replace('.', '')\n","    line = line.replace('\"', '')\n","    line = line.replace('«', '')\n","    line = line.replace('»', '')\n","    line = re.sub(r'\\s+', ' ', line).strip()\n","\n","    # Si la ligne contient \"////\", supprimez tout ce qui est à gauche et \"////\" lui-même\n","    if \"////\" in line:\n","        line = line.split(\"////\")[1].strip()\n","\n","    line = line.replace(',', ', ')\n","    line = re.sub(r'\\s+', ' ', line).strip()\n","\n","    # Si la ligne contient des virgules ou \"et\", divisez la ligne et prenez les noms\n","\n","    names = []\n","    if len(line.split()) > 3:\n","        parts = re.split(',| et', line)\n","        for part in parts:\n","            names.extend(split_names(part.strip()))\n","    else:\n","        line = line.replace(',', '')\n","        names.extend(split_names(line.strip()))\n","\n","    return set(names)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JMywMyATQahL"},"outputs":[],"source":["def write_info_europresse(scores, article, actual_doc):\n","    header = article.header\n","\n","    # Extraire les informations (adaptez en fonction de vos fonctions extract_information, etc.)\n","    title_text = extract_information(header, '.titreArticle p')\n","    journal_text = extract_information(header, '.rdp__DocPublicationName')\n","    date_text = extract_information(header, '.DocHeader')\n","\n","    journal_text = normalize_journal(journal_text)\n","    date_text_clean = extract_date_info(date_text)\n","    normalized_date = normalise_date(date_text_clean)\n","\n","    if normalized_date is not None:\n","        date_normalized = normalized_date.replace(';', '').replace('&', '')\n","    else:\n","        date_normalized = date_text_clean\n","\n","    # Vérifier si le tableau scores n'est pas vide\n","    if scores.size > 0:\n","        max_topic_index = np.argmax(scores)\n","    else:\n","        max_topic_index = -1\n","\n","    # Calculer la clé pour récupérer le bon tableau de labels\n","    config_key = len(scores)\n","\n","    # Récupérer le label correspondant au lieu de l'indice\n","    if config_key in topic_labels_by_config and 0 <= max_topic_index < len(topic_labels_by_config[config_key]):\n","        main_topic_label = topic_labels_by_config[config_key][max_topic_index]\n","    else:\n","        main_topic_label = \"Unknown topic\"\n","\n","    # Convertir chaque score en chaîne\n","    scores_list = [str(score) for score in scores]\n","\n","    # Extraction des noms\n","    names_raw = extract_information(header, '.sm-margin-bottomNews').lower()\n","    names = extract_names(names_raw)\n","    if names:\n","        actual_names = [standardize_name(name) for name in names]\n","        filtered_names = [\n","            name for name in actual_names\n","            if not any(\n","                other_name != name\n","                and set(name.split()) < set(other_name.split())\n","                for other_name in actual_names\n","            )\n","        ]\n","        all_names = filtered_names\n","    else:\n","        all_names = None\n","\n","    chaine_authors = \"None\" if all_names is None else ', '.join(map(str, all_names))\n","\n","    # Retourner une liste plutôt qu'une chaîne\n","    # Remarquez qu'on place main_topic_label à la place de l'ancien max_topic_index\n","    return [\n","        title_text.replace(';', ''),\n","        chaine_authors,\n","        names_raw,\n","        str(len(actual_doc)),\n","        journal_text.replace(';', ''),\n","        date_normalized,\n","        main_topic_label   # Le label au lieu de l'indice\n","    ] + scores_list"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TMc5J9tR_24d"},"outputs":[],"source":["def write_info_another(scores, columns_dict, i, actual_doc):\n","    # Vérifier si le tableau scores n'est pas vide\n","    if scores.size > 0:\n","        max_topic_index = np.argmax(scores)  # indice de la valeur max\n","    else:\n","        max_topic_index = -1\n","\n","    # Préparer la liste des scores en chaînes de caractères\n","    scores_list = [str(s) for s in scores]\n","\n","    # Construire la ligne sous forme de liste\n","    row = []\n","    for key in columns_dict:\n","        row.append(str(columns_dict[key][i]))\n","\n","    # Ajouter nb_characters (en supposant que actual_doc est une chaîne)\n","    row.append(str(len(actual_doc)))\n","\n","    # --- Récupérer le label à la place de l'indice ---\n","    # La clé dans le dictionnaire : len(scores) + 1\n","    config_key = len(scores)\n","\n","    if config_key in topic_labels_by_config and 0 <= max_topic_index < len(topic_labels_by_config[config_key]):\n","        main_topic_label = topic_labels_by_config[config_key][max_topic_index]\n","    else:\n","        # Au cas où la clé ou l'indice n'existe pas dans le dictionnaire\n","        main_topic_label = \"Unknown topic\"\n","\n","    # Ajouter le label du sujet principal au lieu de l'indice\n","    row.append(main_topic_label)\n","\n","    # Ajouter les scores\n","    row.extend(scores_list)\n","\n","    return row"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SP8r-C6EzYDP"},"outputs":[],"source":["def remove_urls_hashtags_emojis_mentions_emails(text):\n","    # Supprimer les URLs\n","    text = re.sub(r'https?://\\S+', '', text)\n","\n","    # Supprimer les hashtags\n","   # text = re.sub(r'#\\w+', '', text)\n","\n","    # Supprimer les mentions\n","  #  text = re.sub(r'@\\w+', '', text)\n","\n","    # Supprimer les e-mails\n"," #   text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '', text)\n","\n","    # Supprimer les émojis\n","    emoji_pattern = re.compile(\"[\"\n","                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","                           u\"\\U0001F700-\\U0001F77F\"  # alchemical symbols\n","                           u\"\\U0001F780-\\U0001F7FF\"  # Geometric Shapes Extended\n","                           u\"\\U0001F800-\\U0001F8FF\"  # Supplemental Arrows-C\n","                           u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n","                           u\"\\U0001FA00-\\U0001FA6F\"  # Chess Symbols\n","                           u\"\\U0001FA70-\\U0001FAFF\"  # Symbols and Pictographs Extended-A\n","                           u\"\\U00002702-\\U000027B0\"  # Dingbats\n","                           u\"\\U000024C2-\\U0001F251\"\n","                           \"]+\", flags=re.UNICODE)\n"," #   text = emoji_pattern.sub(r'', text)\n","\n","    return text"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SjTq-_ZEzYDQ"},"outputs":[],"source":["def extract_info(topic_nums, article):\n","    header = article.header\n","\n","    date_text = extract_information(header, '.DocHeader')\n","    date_text_clean = extract_date_info(date_text)\n","    if normalise_date(date_text_clean) != None:\n","        date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n","    else:\n","        date_normalized = date_text_clean\n","\n","    topics_dict = dict(topic_nums)\n","\n","    return {date_normalized: topics_dict}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vB-tYyThzYDT"},"outputs":[],"source":["def aggregate_scores(articles_info):\n","    aggregated_scores = {}\n","\n","    for info in articles_info:\n","        for date, topics in info.items():\n","            if date not in aggregated_scores:\n","                aggregated_scores[date] = {}\n","\n","            for topic, score in topics.items():\n","                if topic not in aggregated_scores[date]:\n","                    aggregated_scores[date][topic] = 0\n","                aggregated_scores[date][topic] += score\n","\n","    return aggregated_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RvBhn9Z2zYDU"},"outputs":[],"source":["def create_chrono_topics(sigma, apply_normalizations=False):\n","    \"\"\"\n","    Fonction qui agrège les scores de topics par date,\n","    prépare le DataFrame final, puis appelle `plot_custom_heatmap`\n","    pour le tracé (avec clustering, colorbar, ticks, etc.),\n","    en s'assurant d'un alignement robuste entre topic_num et topic_label.\n","    \"\"\"\n","    # 1) Création du répertoire de sortie\n","    if not os.path.exists(f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS/\"):\n","        os.makedirs(f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS/\")\n","\n","    # 2) Pour chaque configuration (nombre de topics)\n","    for num_topic in tqdm(all_nmf_W, desc=\"CONFIGURATIONS PROCESSÉES\"):\n","        config_path = (\n","            f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS\"\n","        )\n","        if not os.path.exists(config_path):\n","            os.makedirs(config_path)\n","\n","        # --- 2.1) Extraction des infos articles selon la source ---\n","        if source_type == 'europresse':\n","            # On construit une liste d'objets \"articles_info\"\n","            articles_info = [\n","                extract_info(\n","                    # On crée un dictionnaire { \"0\": score_doc_i_topic_0, \"1\": score_doc_i_topic_1, ... }\n","                    {str(topic_num): all_nmf_W[num_topic][i, topic_num] for topic_num in range(all_nmf_W[num_topic].shape[1])},\n","                    article\n","                )\n","                for i, article in enumerate(all_soups)\n","            ]\n","        elif source_type in ['csv', 'istex']:\n","            articles_info = []\n","            all_dates = formater_liste_dates(columns_dict['date'])\n","            i = 0\n","            while i < len(all_dates):\n","                score_dict = {\n","                    str(topic_num): all_nmf_W[num_topic][i, topic_num]\n","                    for topic_num in range(all_nmf_W[num_topic].shape[1])\n","                }\n","                articles_info.append({all_dates[i]: score_dict})\n","                i += 1\n","\n","        # --- 2.2) Agrégation par date ---\n","        #     aggregate_scores() doit renvoyer un dict du type :\n","        #     {\n","        #        \"12/05/2020\": {\"0\": 0.43, \"1\": 0.12, ...},\n","        #        \"13/05/2020\": {\"0\": 0.22, \"1\": 0.61, ...},\n","        #         ...\n","        #     }\n","        aggregated_scores = aggregate_scores(articles_info)\n","\n","        renamed_aggregated_scores = {\n","            date: {\n","                topic_labels_by_config[num_topic][int(topic_str)]: score\n","                for topic_str, score in scores_dict.items()\n","            }\n","            for date, scores_dict in aggregated_scores.items()\n","        }\n","\n","        aggregated_scores = renamed_aggregated_scores\n","\n","        # --- 2.3) Calcul du nombre de documents par date ---\n","        doc_counts_by_date = defaultdict(int)\n","        for elt in articles_info:\n","            if isinstance(elt, dict):\n","                for date_str in elt.keys():\n","                    doc_counts_by_date[date_str] += 1\n","\n","        # --- 2.4) Filtrage des dates valides & conversion ---\n","        valid_dates = {}\n","        for date_str, score_dict in aggregated_scores.items():\n","            date_obj = extract_and_convert_date(date_str)\n","            if date_obj:\n","                # Format unifié \"jj/mm/YYYY\"\n","                valid_dates[date_obj.strftime('%d/%m/%Y')] = score_dict\n","\n","        # --- 2.5) Tri chronologique de ces dates ---\n","        aggregated_scores_sorted = {\n","            date: valid_dates[date]\n","            for date in sorted(valid_dates, key=lambda d: datetime.strptime(d, '%d/%m/%Y'))\n","        }\n","\n","        # -----------------------------------------------------------------------\n","        # --- 3) Construction d'un DataFrame \"long\" (date, topic_num, score) ---\n","        # -----------------------------------------------------------------------\n","        #  L'objectif : obtenir une table de la forme :\n","        #\n","        #       date         topic_num   score\n","        #     \"12/05/2020\"       0       0.43\n","        #     \"12/05/2020\"       1       0.12\n","        #     \"13/05/2020\"       0       0.22\n","        #     ...\n","        #\n","        #  afin de pouvoir ensuite fusionner sur un DataFrame de mapping\n","        #  (topic_num -> topic_label) et enfin faire un pivot.\n","        # -----------------------------------------------------------------------\n","\n","        rows = []\n","        for date_str, topics_dict in aggregated_scores_sorted.items():\n","            for topic_num_str, score in topics_dict.items():\n","                rows.append({\n","                    \"Date\": date_str,\n","                    \"Topic\": topic_num_str,\n","                    \"Score\": score\n","                })\n","\n","        df_long = pd.DataFrame(rows)  # \"long format\"\n","\n","        # --- 3.3) Pivot sur le label de topic (index) et la date (columns) ---\n","        #   Les valeurs sont \"Score\"\n","        df_pivoted = df_long.pivot(\n","            index=\"Topic\",\n","            columns=\"Date\",\n","            values=\"Score\"\n","        ).fillna(0)  # on remplit à 0 les absences\n","\n","        # ---------------------------------------------------------------------\n","        # --- 4) Optionnel : division par le nombre de documents (normalization)\n","        # ---------------------------------------------------------------------\n","        if apply_normalizations:\n","            for col in df_pivoted.columns:\n","                nb_docs = doc_counts_by_date.get(col, 1)  # éviter la division par 0\n","                df_pivoted[col] = df_pivoted[col] / nb_docs\n","\n","        # ---------------------------------------------------------------------\n","        # --- 5) Conversion des colonnes en datetime & tri chronologique\n","        # ---------------------------------------------------------------------\n","        df_pivoted.columns = pd.to_datetime(df_pivoted.columns, format='%d/%m/%Y', errors='coerce')\n","        # On enlève d'éventuels NaT si conversion ratée (ou on pourrait ignorer)\n","        df_pivoted = df_pivoted.loc[:, df_pivoted.columns.notna()]\n","\n","        # Tri des colonnes par ordre chronologique\n","        df_pivoted = df_pivoted.reindex(sorted(df_pivoted.columns), axis=1)\n","\n","        # ---------------------------------------------------------------------\n","        # --- 6) Réindexation sur la plage complète de dates\n","        #         (de la plus ancienne à la plus récente)\n","        # ---------------------------------------------------------------------\n","        if not df_pivoted.columns.empty:\n","            oldest_date = df_pivoted.columns.min()\n","            newest_date = df_pivoted.columns.max()\n","            date_range = pd.date_range(start=oldest_date, end=newest_date)\n","\n","            # On réindexe, et on remplit à 0 pour les dates manquantes\n","            df_pivoted = df_pivoted.reindex(columns=date_range, fill_value=0)\n","        else:\n","            # S'il n'y a pas de colonne (cas extrême), on ne fait rien\n","            date_range = []\n","\n","        # ---------------------------------------------------------------------\n","        # --- 7) Préparation pour la heatmap\n","        # ---------------------------------------------------------------------\n","        # df_pivoted est un DataFrame (topics_label en lignes, dates en colonnes)\n","        df_normalized = df_pivoted  # pour garder le même nom que l'ancien code\n","\n","        # Si sigma = 'auto', on le définit maintenant (pour plot_custom_heatmap)\n","        if sigma == 'auto':\n","            sigma = len(df_normalized.columns) / 15 if len(df_normalized.columns) > 0 else 1\n","\n","        # ---------------------------------------------------------------------\n","        # --- 8) Appel de plot_custom_heatmap ---\n","        # ---------------------------------------------------------------------\n","        plot_custom_heatmap(\n","            df=df_normalized,\n","            sigma=sigma,\n","            cmap=\"YlGnBu\",            # palette\n","            relative_normalizaton=apply_normalizations,  # normalisation interne\n","            with_colormap=False\n","        )\n","\n","        # ---------------------------------------------------------------------\n","        # --- 9) Sauvegarde de la figure ---\n","        # ---------------------------------------------------------------------\n","        class_suffix = \"_\".join(grammatical_classes)\n","        plt.savefig(\n","            f\"{results_path}{base_name}_TOPICS_DYNAMICS_HEATMAPS/\"\n","            f\"{base_name}_topics_dynamics_heatmap_{num_topic}tc_{apply_normalizations}n_{('auto' if sigma=='auto' else int(sigma))}s\"\n","            f\"_{class_suffix}_\"\n","            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","            f\"{go_remove_duplicates}dup_dtwcompletechdtw2.png\",\n","            dpi=DPI,\n","            bbox_inches='tight',\n","            pad_inches=0,\n","        )\n","        plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-emWCg7JUfTc"},"outputs":[],"source":["def correct_dates(dictionary):\n","    corrected_dict = {}\n","    for date, count in dictionary.items():\n","        if len(date) < 10:\n","            # Ajouter '0' au début de la date\n","            corrected_date = '0' + date\n","        else:\n","            corrected_date = date\n","        corrected_dict[corrected_date] = count\n","    return corrected_dict"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iiMYxaJsiVsF"},"outputs":[],"source":["def create_chrono_group_column(group_column, sigma, apply_normalizations=False):\n","    \"\"\"\n","    Prépare les données par groupe/colonne (journal ou autre) au fil du temps,\n","    puis crée la heatmap temporelle en utilisant `plot_custom_heatmap`,\n","    en s'appuyant sur la stratégie \"robuste\" (format long + pivot).\n","    \"\"\"\n","    # 1) Création du répertoire de sortie\n","    output_dir = f\"{results_path}{base_name}_GROUPS_DYNAMICS_HEATMAPS/\"\n","    if not os.path.exists(output_dir):\n","        os.makedirs(output_dir)\n","\n","    # ---------------------------------------------------------\n","    # 2) Agrégation des comptes (scores) en fonction de la source\n","    #    => On va créer un dict: date -> {groupe: count}\n","    # ---------------------------------------------------------\n","    aggregated_scores = {}\n","\n","    if source_type == 'europresse':\n","        for article in all_soups:\n","            header = article.header\n","            journal_text = extract_information(header, '.rdp__DocPublicationName')\n","            date_text = extract_information(header, '.DocHeader')\n","\n","            journal_text = normalize_journal(journal_text)\n","            date_text_clean = extract_date_info(date_text)\n","\n","            date_normalized = normalise_date(date_text_clean)\n","            if date_normalized is None:\n","                # Si on n'a pas pu normaliser, on prend la valeur brute\n","                date_normalized = date_text_clean\n","            else:\n","                # Nettoyage de base\n","                date_normalized = date_normalized.replace(';', '').replace('&', '')\n","\n","            if date_normalized not in aggregated_scores:\n","                aggregated_scores[date_normalized] = {}\n","            aggregated_scores[date_normalized].setdefault(journal_text, 0)\n","            aggregated_scores[date_normalized][journal_text] += 1\n","\n","    elif source_type == 'istex':\n","        for i in range(len(columns_dict['date'])):\n","            date_raw = columns_dict['date'][i]\n","            journal = columns_dict['journal'][i]\n","\n","            # Nettoyage basique\n","            date_normalized = date_raw.replace(';', '').replace('&', '')\n","\n","            if date_normalized not in aggregated_scores:\n","                aggregated_scores[date_normalized] = {}\n","            aggregated_scores[date_normalized].setdefault(journal, 0)\n","            aggregated_scores[date_normalized][journal] += 1\n","\n","    elif source_type == 'csv':\n","        # Vérification de la disponibilité des colonnes requises\n","        if 'date' not in columns_dict:\n","            print(\"La colonne 'date' n'existe pas dans columns_dict. Elle est requise.\")\n","            return\n","\n","        if group_column not in columns_dict:\n","            print(f\"La colonne '{group_column}' n'existe pas dans columns_dict. Vérifiez les colonnes disponibles.\")\n","            return\n","\n","        for i in range(len(columns_dict['date'])):\n","            date_raw = columns_dict['date'][i]\n","            group_value = columns_dict[group_column][i]\n","\n","            # Nettoyage\n","            date_normalized = date_raw.replace(';', '').replace('&', '')\n","\n","            if date_normalized not in aggregated_scores:\n","                aggregated_scores[date_normalized] = {}\n","            aggregated_scores[date_normalized].setdefault(group_value, 0)\n","            aggregated_scores[date_normalized][group_value] += 1\n","\n","    else:\n","        # Source non gérée\n","        return\n","\n","    # ---------------------------------------------------------\n","    # 3) Conversion des dates & tri chronologique\n","    #    => format \"jj/mm/YYYY\" pour éviter les ambiguïtés\n","    # ---------------------------------------------------------\n","    valid_dates = {}\n","    for date_str, score_dict in aggregated_scores.items():\n","        date_obj = extract_and_convert_date(date_str)\n","        if date_obj:\n","            valid_dates[date_obj.strftime('%d/%m/%Y')] = score_dict\n","\n","    # Tri chronologique des dates\n","    sorted_dates = sorted(valid_dates, key=lambda d: datetime.strptime(d, '%d/%m/%Y'))\n","    aggregated_scores_sorted = {\n","        date: valid_dates[date]\n","        for date in sorted_dates\n","    }\n","\n","    # ---------------------------------------------------------\n","    # 4) Calcul du nombre total d'articles par date (pour normaliser si besoin)\n","    # ---------------------------------------------------------\n","    aggregated_article_counts = {}\n","    for date, group_counts in aggregated_scores_sorted.items():\n","        aggregated_article_counts[date] = sum(group_counts.values())\n","\n","    # ---------------------------------------------------------\n","    # 5) Passage en \"long format\"\n","    #    => on obtient des lignes : [Date, Group, Count]\n","    # ---------------------------------------------------------\n","    rows = []\n","    for date, group_dict in aggregated_scores_sorted.items():\n","        for group_value, count in group_dict.items():\n","            rows.append({\n","                \"Date\": date,\n","                \"Group\": group_value,\n","                \"Count\": count\n","            })\n","\n","    df_long = pd.DataFrame(rows, columns=[\"Date\", \"Group\", \"Count\"])\n","\n","    # ---------------------------------------------------------\n","    # 6) Pivot => index = Group, columns = Date, values = Count\n","    # ---------------------------------------------------------\n","    df_pivoted = df_long.pivot(\n","        index='Group',\n","        columns='Date',\n","        values='Count'\n","    ).fillna(0)\n","\n","    # ---------------------------------------------------------\n","    # 7) Application d’un threshold sur les lignes (si besoin)\n","    #    => on ne garde que les groupes dont la somme >= threshold\n","    # ---------------------------------------------------------\n","    df_pivoted = df_pivoted[df_pivoted.sum(axis=1) >= threshold]\n","\n","    # Copie pour d’éventuelles normalisations\n","    df_normalized = df_pivoted.copy()\n","\n","    # ---------------------------------------------------------\n","    # 8) Division par nb_docs si apply_normalizations == True\n","    # ---------------------------------------------------------\n","    if len(df_normalized) > 1:\n","        if apply_normalizations:\n","            for col in df_normalized.columns:\n","                nb_docs = aggregated_article_counts.get(col, 1)  # éviter division par 0\n","                df_normalized[col] = df_normalized[col] / nb_docs\n","\n","    # ---------------------------------------------------------\n","    # 9) Conversion des colonnes en datetime et tri chrono\n","    # ---------------------------------------------------------\n","    df_normalized.columns = pd.to_datetime(df_normalized.columns, format='%d/%m/%Y', errors='coerce')\n","    # On enlève d’éventuelles colonnes non converties\n","    df_normalized = df_normalized.loc[:, df_normalized.columns.notnull()]\n","\n","    # Tri des dates chronologiquement\n","    df_normalized = df_normalized.reindex(sorted(df_normalized.columns), axis=1)\n","\n","    # ---------------------------------------------------------\n","    # 10) Réindexer pour inclure TOUTES les dates manquantes\n","    #     => on remplit par 0\n","    # ---------------------------------------------------------\n","    if not df_normalized.columns.empty:\n","        oldest_date = df_normalized.columns.min()\n","        newest_date = df_normalized.columns.max()\n","        date_range = pd.date_range(start=oldest_date, end=newest_date)\n","        df_normalized = df_normalized.reindex(columns=date_range, fill_value=0)\n","    else:\n","        # Pas de dates valides, on ne fait rien\n","        date_range = []\n","\n","    # ---------------------------------------------------------\n","    # 11) (Optionnel) Filtrage gaussien ici\n","    #     => si vous préférez laisser plot_custom_heatmap s'en charger, commentez.\n","    # ---------------------------------------------------------\n","    # ...\n","\n","    # ---------------------------------------------------------\n","    # 12) (Optionnel) MinMaxScaling par ligne\n","    #     => idem, si vous le faites ici, ne le refaites pas dans plot_custom_heatmap.\n","    # ---------------------------------------------------------\n","    # ...\n","\n","    # ---------------------------------------------------------\n","    # 13) Ajustement sigma si \"auto\"\n","    # ---------------------------------------------------------\n","    if sigma == 'auto':\n","        nb_cols = len(df_normalized.columns)\n","        sigma = nb_cols / 15.0 if nb_cols > 0 else 1\n","\n","    if len(df_normalized) == 1 and apply_normalizations:\n","        return\n","    else:\n","        plot_custom_heatmap(\n","            df=df_normalized,\n","            sigma=sigma,\n","            cmap=\"YlGnBu\",\n","            relative_normalizaton=apply_normalizations,\n","            with_colormap=False\n","        )\n","\n","        # ---------------------------------------------------------\n","        # 15) Sauvegarde de la figure\n","        # ---------------------------------------------------------\n","        plt.savefig(\n","            f\"{output_dir}{base_name}_groups_dynamics_heatmap_{apply_normalizations}n_{('auto' if sigma=='auto' else int(sigma))}s_\"\n","            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n","            f\"{threshold}thr_dtwcompletechdtw2.png\",\n","            dpi=DPI,\n","            bbox_inches='tight',\n","            pad_inches=0,\n","        )\n","        plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vBSjjwG90nW2"},"outputs":[],"source":["def create_results_folder(base_name):\n","    if not os.path.exists(results_path):\n","        os.makedirs(results_path)\n","\n","    name_document = f'{base_name}.csv'"]},{"cell_type":"code","source":["# =============================================================================\n","# Fonction principale pour charger les documents\n","# =============================================================================\n","def load_documents(name, source_type, minimum_caracters_nb_by_document, pbar):\n","    \"\"\"\n","    Récupère et nettoie les documents à partir d'un fichier ou dossier donné,\n","    en fonction du 'source_type' (europresse, csv, istex).\n","    Retourne :\n","      - documents : liste de textes\n","      - all_soups : liste de BeautifulSoup (pour europresse, sinon vide)\n","      - columns_dict : dictionnaire contenant d'autres colonnes (pour CSV, ISTEX...)\n","    \"\"\"\n","    documents = []\n","    all_soups = []\n","    columns_dict = {}\n","\n","    # -------------------------------------------------------------------------\n","    # CAS 1 : Fichiers issus de Europresse\n","    # -------------------------------------------------------------------------\n","    if source_type == 'europresse':\n","        document_europresse = ''\n","\n","        # Lecture du fichier HTML brut\n","        with open(name, 'r', encoding='utf-8', errors='xmlcharrefreplace') as file:\n","            for line in file:\n","                document_europresse += line\n","\n","        # Décode les entités HTML et répare la séparation entre articles\n","        document_europresse = html.unescape(document_europresse)\n","        document_europresse = document_europresse.replace('</article> <article>', '</article><article>')\n","        documents_europresse = document_europresse.split('</article><article>')\n","\n","        nb_not_occur = 0\n","        for d in documents_europresse:\n","            soup = BeautifulSoup(d, features=\"html.parser\")\n","\n","            # On met à jour la barre de progression pour chaque article\n","            pbar.update(1)\n","\n","            # On retire les paragraphes \"Lire aussi ...\" qui ne contiennent pas assez de texte\n","            for p in soup.find_all('p'):\n","                p_text = p.get_text()\n","                if (\"Lire aussi\" in p_text and (\"http\" in p_text or \"https\" in p_text) and len(p_text) <= 1000):\n","                    p.decompose()\n","\n","            # Si on trouve la div \"docOcurrContainer\", c'est là qu'est le texte\n","            if len(soup('div', {'class': 'docOcurrContainer'})) > 0:\n","                # Corrige des fins de paragraphes manquantes (ponctuation)\n","                for p in soup.find_all('p'):\n","                    # Trouver le prochain caractère alphabétique après ce paragraphe\n","                    next_char_match = re.search(\n","                        r'(?<=' + re.escape(p.text) + r')\\s*(?:<[^>]*>)*\\s*([a-zA-Z])',\n","                        str(soup)\n","                    )\n","                    # Ajoute un point si le paragraphe ne se termine pas par '.'\n","                    # et que le prochain char est une majuscule\n","                    if not p.text.endswith('.') and next_char_match and next_char_match.group(1).isupper():\n","                        p.string = p.text + '. '\n","\n","                # On recrée la soupe après modifications\n","                soup = BeautifulSoup(str(soup), features='html.parser')\n","\n","                candidate_text = soup('div', {'class': 'docOcurrContainer'})[0].get_text()\n","                if (minimum_caracters_nb_by_document <= len(candidate_text) < maximum_caracters_nb_by_document):\n","                    candidate_text = remove_urls_hashtags_emojis_mentions_emails(candidate_text)\n","                    candidate_text = transform_text(candidate_text)\n","                    documents.append(candidate_text)\n","                    all_soups.append(soup)\n","            else:\n","                nb_not_occur += 1\n","\n","    # -------------------------------------------------------------------------\n","    # CAS 2 : Fichier CSV\n","    # -------------------------------------------------------------------------\n","    elif source_type == 'csv':\n","        # 1. Vérifier la taille du fichier\n","        file_size_bytes = os.path.getsize(name)\n","        file_size_mb = file_size_bytes / (1024 * 1024)\n","        print(f\"[*] Taille du fichier CSV = {file_size_mb:.2f} Mo\")\n","\n","        if file_size_mb > 100:\n","            # 2. Si le fichier dépasse 200 Mo, on lit directement en UTF-8, sep=';'\n","            print(\"[*] Le fichier est > 200 Mo : lecture directe en UTF-8 (séparateur ';')\")\n","            try:\n","                df = pd.read_csv(name, encoding='utf-8', sep=';', on_bad_lines='skip', low_memory=False)\n","            except Exception as e:\n","                print(f\"[!] Erreur lors de la lecture du fichier >200Mo en UTF-8/';' : {e}\")\n","                return ([], [], {})  # Retourne des listes/dict vides\n","\n","        else:\n","            # 3. Fichier <= 200 Mo : détection d'encodage via charset-normalizer\n","            print(\"[*] Le fichier est <= 200 Mo : on effectue la détection d'encodage\")\n","            results = from_path(name)\n","            best_guess = results.best()\n","\n","            if best_guess is not None:\n","                detected_encoding = best_guess.encoding\n","                raw_data = best_guess.raw\n","            else:\n","                detected_encoding = None\n","                with open(name, 'rb') as f:\n","                    raw_data = f.read()\n","\n","            # On se limite à utf-8 + encodage système éventuel\n","            common_encodings = ['utf-8']\n","            system_encoding = locale.getpreferredencoding(False)\n","            if system_encoding and system_encoding.lower() not in [enc.lower() for enc in common_encodings]:\n","                common_encodings.append(system_encoding)\n","\n","            # Si charset-normalizer propose un encodage non dans la liste, on l'insère en priorité\n","            if detected_encoding and detected_encoding.lower() not in [enc.lower() for enc in common_encodings]:\n","                common_encodings.insert(0, detected_encoding)\n","\n","            best_score = None\n","            best_df = None\n","            best_sep = None\n","\n","            # On tente les encodages présents dans 'common_encodings'\n","            for enc in common_encodings:\n","                try:\n","                    # Décodage en mémoire (bytes -> str)\n","                    content_str = raw_data.decode(enc, errors='replace')\n","                    file_like = io.StringIO(content_str)\n","\n","                    # Détection du séparateur via la 1ère ligne\n","                    first_line = file_like.readline().strip()\n","                    file_like.seek(0)\n","                    if ',' in first_line:\n","                        sep = ','\n","                    elif ';' in first_line:\n","                        sep = ';'\n","                    else:\n","                        sep = None\n","\n","                    file_like.seek(0)\n","                    try:\n","                        # Lecture du CSV multi-colonnes\n","                        df_test = pd.read_csv(file_like, header=0, sep=sep, on_bad_lines='skip', low_memory=False)\n","                    except Exception:\n","                        # Si échec, on retente en \"mono-colonne\"\n","                        file_like.seek(0)\n","                        header = file_like.readline().strip()\n","                        content = [line.strip() for line in file_like]\n","                        df_test = pd.DataFrame(content, columns=[header])\n","\n","                    # Compter les caractères invalides\n","                    invalid_chars = df_test.to_string().count('�')\n","                    if best_score is None or invalid_chars < best_score:\n","                        best_score = invalid_chars\n","                        best_df = df_test\n","                        best_sep = sep\n","\n","                except Exception:\n","                    # Si l'encodage échoue, on ignore\n","                    continue\n","\n","            # À la fin, best_df est le DataFrame \"le moins corrompu\"\n","            df = best_df\n","            if df is None:\n","                print(\"[!] Impossible de lire le CSV avec les encodages testés.\")\n","                return ([], [], {})\n","\n","            # Conversion en minuscules et renommage éventuel\n","            df.columns = df.columns.str.lower()\n","            df = df.rename(columns={'post created date': 'date'})\n","            df.fillna('', inplace=True)\n","\n","        # -------------------------------------------------------------------------\n","        # À ce stade, on possède un DataFrame 'df', soit gros CSV (lecture directe),\n","        # soit petit CSV (<=200 Mo) après détection d'encodage.\n","        # Le code qui suit (choix de la colonne, filtrage, etc.) reste inchangé :\n","        # -------------------------------------------------------------------------\n","\n","        # Choix de la colonne texte\n","        if 'text' in df.columns:\n","            column_to_use = 'text'\n","        elif 'description' in df.columns:\n","            column_to_use = 'description'\n","        else:\n","            print(\"Les colonnes 'text' ou 'description' ne sont pas présentes dans le DataFrame.\")\n","            return ([], [], {})  # Retourne des listes/dict vides\n","\n","        # Filtrage par longueur\n","        df = df.loc[\n","            (df[column_to_use].str.len() >= minimum_caracters_nb_by_document) &\n","            (df[column_to_use].str.len() <= maximum_caracters_nb_by_document)\n","        ]\n","\n","        # Nettoyage de chaque document\n","        documents = df[column_to_use].tolist()\n","        for i in range(len(documents)):\n","            documents[i] = remove_urls_hashtags_emojis_mentions_emails(documents[i])\n","            documents[i] = transform_text(documents[i])\n","\n","        # Stockage des autres colonnes\n","        for column in df.columns:\n","            if column not in ['text', 'description']:\n","                columns_dict[column] = df[column].tolist()\n","\n","        # À ce stade, on a un DataFrame 'df' lisible\n","        # -------------------------------------------------------------------------\n","        # Choix de la colonne texte\n","        if 'text' in df.columns:\n","            column_to_use = 'text'\n","        elif 'description' in df.columns:\n","            column_to_use = 'description'\n","        else:\n","            print(\"Les colonnes 'text' ou 'description' ne sont pas présentes dans le DataFrame.\")\n","            return ([], [], {})  # Retourne des listes/dict vides\n","\n","        # Filtrage par longueur\n","        df = df.loc[\n","            (df[column_to_use].str.len() >= minimum_caracters_nb_by_document) &\n","            (df[column_to_use].str.len() <= maximum_caracters_nb_by_document)\n","        ]\n","\n","        # Nettoyage de chaque document\n","        documents = df[column_to_use].tolist()\n","        for i in range(len(documents)):\n","            documents[i] = remove_urls_hashtags_emojis_mentions_emails(documents[i])\n","            documents[i] = transform_text(documents[i])\n","\n","        # Stockage des autres colonnes\n","        for column in df.columns:\n","            if column not in ['text', 'description']:\n","                columns_dict[column] = df[column].tolist()\n","\n","    # -------------------------------------------------------------------------\n","    # CAS 3 : ISTEX\n","    # -------------------------------------------------------------------------\n","    elif source_type == 'istex':\n","        def get_nested(data, keys):\n","            \"\"\"Fonction utilitaire pour extraire des données imbriquées dans un dictionnaire.\"\"\"\n","            for key in keys:\n","                if isinstance(data, dict):\n","                    data = data.get(key)\n","                else:\n","                    return None\n","            return data\n","\n","        documents = []\n","        columns_dict = {}\n","\n","        # Champs ISTEX à extraire\n","        fields_to_extract = [\n","            'date', 'title', 'doi', 'journal', 'language', 'originalGenre',\n","            'accessCondition', 'pdfVersion', 'abstractCharCount', 'pdfPageCount',\n","            'pdfWordCount', 'score', 'pdfText', 'imageCount', 'refCount',\n","            'sectionCount', 'paragraphCount', 'tableCount', 'categories_scopus',\n","            'categories_scienceMetrix', 'host_volume', 'host_issue',\n","            'host_publisher', 'host_pages_first', 'host_pages_last', 'host_title',\n","            'refBibs_count',\n","        ]\n","\n","        field_mappings = {\n","            'date': ['publicationDate'],\n","            'title': ['title'],\n","            'doi': ['doi'],\n","            'journal': ['host', 'title'],\n","            'language': ['language'],\n","            'originalGenre': ['originalGenre'],\n","            'accessCondition': ['accessCondition', 'value'],\n","            'pdfVersion': ['qualityIndicators', 'pdfVersion'],\n","            'abstractCharCount': ['qualityIndicators', 'abstractCharCount'],\n","            'pdfPageCount': ['qualityIndicators', 'pdfPageCount'],\n","            'pdfWordCount': ['qualityIndicators', 'pdfWordCount'],\n","            'score': ['qualityIndicators', 'score'],\n","            'pdfText': ['qualityIndicators', 'pdfText'],\n","            'imageCount': ['qualityIndicators', 'xmlStats', 'imageCount'],\n","            'refCount': ['qualityIndicators', 'xmlStats', 'refCount'],\n","            'sectionCount': ['qualityIndicators', 'xmlStats', 'sectionCount'],\n","            'paragraphCount': ['qualityIndicators', 'xmlStats', 'paragraphCount'],\n","            'tableCount': ['qualityIndicators', 'xmlStats', 'tableCount'],\n","            'categories_scopus': ['categories', 'scopus'],\n","            'categories_scienceMetrix': ['categories', 'scienceMetrix'],\n","            'host_volume': ['host', 'volume'],\n","            'host_issue': ['host', 'issue'],\n","            'host_publisher': ['host', 'publisher'],\n","            'host_pages_first': ['host', 'pages', 'first'],\n","            'host_pages_last': ['host', 'pages', 'last'],\n","            'host_title': ['host', 'title'],\n","            'refBibs_count': ['refBibs'],\n","        }\n","\n","        # Initialiser columns_dict avec listes vides\n","        for field in fields_to_extract:\n","            columns_dict[field] = []\n","\n","        import contextlib\n","        if os.path.isdir(name):\n","            with contextlib.redirect_stdout(None):\n","                files_in_dir = os.listdir(name)\n","                txt_files = [f for f in files_in_dir if f.endswith('.txt')]\n","                json_files = [f for f in files_in_dir if f.endswith('.json')]\n","\n","                # On cherche les basenames communs\n","                txt_basenames = set(os.path.splitext(f)[0] for f in txt_files)\n","                json_basenames = set(os.path.splitext(f)[0] for f in json_files)\n","                common_basenames = txt_basenames.intersection(json_basenames)\n","\n","                if not common_basenames:\n","                    print(f\"Le répertoire '{name}' est ignoré (pas de fichiers .txt et .json correspondants).\")\n","                else:\n","                    for basename in common_basenames:\n","                        txt_file_path = os.path.join(name, basename + '.txt')\n","                        try:\n","                            with open(txt_file_path, 'r', encoding='utf-8') as f:\n","                                txt_content = f.read()\n","                        except Exception as e:\n","                            print(f\"Erreur lors de la lecture du fichier texte '{txt_file_path}': {e}\")\n","                            continue\n","\n","                        # Filtrage de longueur\n","                        if (minimum_caracters_nb_by_document <= len(txt_content) <= maximum_caracters_nb_by_document):\n","                            txt_content = remove_urls_hashtags_emojis_mentions_emails(txt_content)\n","                            txt_content = transform_text(txt_content)\n","                            documents.append(txt_content)\n","\n","                            # Lecture du JSON\n","                            json_file_path = os.path.join(name, basename + '.json')\n","                            try:\n","                                with open(json_file_path, 'r', encoding='utf-8') as f:\n","                                    json_data = json.load(f)\n","                            except Exception as e:\n","                                print(f\"Erreur lors de la lecture du fichier JSON '{json_file_path}': {e}\")\n","                                # On met des None pour chaque champ\n","                                for field in fields_to_extract:\n","                                    columns_dict[field].append(None)\n","                                continue\n","\n","                            # Extraction champs\n","                            for field in fields_to_extract:\n","                                json_keys = field_mappings.get(field)\n","                                value = None\n","                                if json_keys is not None:\n","                                    if field == 'refBibs_count':\n","                                        # Nombre de références bibliographiques\n","                                        refbibs = get_nested(json_data, json_keys)\n","                                        value = len(refbibs) if refbibs is not None else 0\n","                                    else:\n","                                        value = get_nested(json_data, json_keys)\n","                                        # Si c'est une liste, on joint par virgule\n","                                        if isinstance(value, list):\n","                                            value = ', '.join(map(str, value))\n","                                columns_dict[field].append(value)\n","                        else:\n","                            # Document trop court ou trop long\n","                            continue\n","\n","                    # Vérification des longueurs\n","                    length_documents = len(documents)\n","                    for field in columns_dict:\n","                        assert len(columns_dict[field]) == length_documents, \\\n","                            f\"Incohérence pour le champ '{field}'\"\n","            # On modifie la date si besoin\n","            columns_dict[\"date\"] = [\"01/01/\" + date for date in columns_dict[\"date\"]]\n","        else:\n","            print(f\"Le chemin '{name}' n'est pas un répertoire valide.\")\n","\n","    # -------------------------------------------------------------------------\n","    return documents, all_soups, columns_dict\n","\n","# =============================================================================\n","# Fonction pour rassembler les documents de manière simplifiée\n","# =============================================================================\n","def meta_load_documents():\n","    \"\"\"\n","    Identifie tous les fichiers/dossiers sources en fonction de 'source_type',\n","    puis appelle directement load_documents sur chacun d'entre eux.\n","    Fusionne le tout dans les variables globales :\n","      - documents\n","      - all_soups\n","      - columns_dict\n","    Gère la suppression de doublons si go_remove_duplicates == True.\n","    \"\"\"\n","    global documents\n","    global all_soups\n","    global columns_dict\n","\n","    # -------------------------------------------------------------------------\n","    # Construction de la liste des fichiers à traiter\n","    # -------------------------------------------------------------------------\n","    if source_type == 'europresse':\n","        # Tous les .html qui contiennent base_name\n","        fichiers_html = [\n","            f for f in os.listdir(f\"{folder_path}DATA/\")\n","            if f.lower().endswith('.html')\n","            and os.path.isfile(os.path.join(f\"{folder_path}DATA/\", f))\n","            and base_name in f\n","        ]\n","\n","    elif source_type == 'csv':\n","        # Tous les .csv qui contiennent base_name\n","        fichiers_html = [\n","            f for f in os.listdir(f\"{folder_path}DATA/\")\n","            if f.lower().endswith('.csv')\n","            and os.path.isfile(os.path.join(f\"{folder_path}DATA/\", f))\n","            and base_name in f\n","        ]\n","\n","    elif source_type == 'istex':\n","        # Vérifier si 'DATA' est déjà dans folder_path\n","        if folder_path.endswith(\"DATA\") or folder_path.endswith(\"DATA/\"):\n","            data_folder_path = folder_path\n","        else:\n","            data_folder_path = os.path.join(folder_path, \"DATA\")\n","\n","        # On cherche un sous-dossier dont le nom contient base_name\n","        sous_dossier_principal = None\n","        for f in os.listdir(data_folder_path):\n","            if os.path.isdir(os.path.join(data_folder_path, f)) and base_name in f:\n","                sous_dossier_principal = f\n","                break\n","\n","        if sous_dossier_principal:\n","            # Pour ISTEX, on récupère la liste des sous-sous-dossiers\n","            fichiers_html = [\n","                os.path.join(sous_dossier_principal, sub_f)\n","                for sub_f in os.listdir(os.path.join(data_folder_path, sous_dossier_principal))\n","                if os.path.isdir(os.path.join(data_folder_path, sous_dossier_principal, sub_f))\n","            ]\n","        else:\n","            fichiers_html = []\n","    else:\n","        fichiers_html = []\n","\n","    # -------------------------------------------------------------------------\n","    # Barre de progression\n","    # -------------------------------------------------------------------------\n","    pbar = tqdm(\n","        total=len(fichiers_html),\n","        desc='DOCUMENTS PROCESSÉS'\n","    )\n","\n","    # -------------------------------------------------------------------------\n","    # Récupération des documents et fusion des informations\n","    # -------------------------------------------------------------------------\n","    all_columns_dicts = []\n","    for f in fichiers_html:\n","        # Construction du chemin complet\n","        full_path = os.path.join(folder_path, 'DATA', f) \\\n","            if source_type in ['europresse', 'csv'] else os.path.join(folder_path, 'DATA', f)\n","\n","        # On appelle directement load_documents\n","        d, s, cd = load_documents(full_path, source_type, minimum_caracters_nb_by_document, pbar)\n","        documents.extend(d)\n","        all_soups.extend(s)\n","        all_columns_dicts.append(cd)\n","\n","        # Mise à jour de la barre de progression\n","        pbar.update(1)\n","\n","    # -------------------------------------------------------------------------\n","    # Fusionner les dictionnaires de colonnes\n","    # -------------------------------------------------------------------------\n","    for dico in all_columns_dicts:\n","        for cle, valeur in dico.items():\n","            if cle not in columns_dict:\n","                columns_dict[cle] = []\n","            columns_dict[cle].extend(valeur)\n","\n","    # -------------------------------------------------------------------------\n","    # Suppression des doublons si demandé\n","    # -------------------------------------------------------------------------\n","    if go_remove_duplicates:\n","        remove_duplicates_lsh()\n","\n","    # -------------------------------------------------------------------------\n","    # Affichage final\n","    # -------------------------------------------------------------------------\n","    print('\\n')\n","    print(len(documents), 'documents')"],"metadata":{"id":"Pd5bYgQKqsZU"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A6WxTZ_WzYDU"},"outputs":[],"source":["def update_candidates_for_unigram(kind, unigrams):\n","    \"\"\"\n","    Met à jour le dictionnaire `unigrams` en affectant la valeur 1\n","    aux tokens pour lesquels le POS majoritaire (mode) est `kind`\n","    et qui ont au moins 3 caractères.\n","    \"\"\"\n","\n","    # 1) Prépare la liste de travail, avec un éventuel unidecode pour les PROPN\n","    if kind == 'PROPN':\n","        all_tab_pos_for_work = copy.deepcopy(all_tab_pos)\n","        for sentence in all_tab_pos_for_work:\n","            for token_info in sentence:\n","                token_info[0] = unidecode.unidecode(token_info[0])\n","    else:\n","        all_tab_pos_for_work = all_tab_pos\n","\n","    # 2) Un dictionnaire token -> liste des POS rencontrés\n","    token_pos_map = defaultdict(list)\n","\n","    # 3) Un ensemble pour repérer vite lesquels ont déjà eu 'kind' et >= 3 caractères\n","    candidates = set()\n","\n","    # 4) Parcours unique de all_tab_pos_for_work\n","    for sentence in all_tab_pos_for_work:\n","        for token, pos in sentence:\n","            token_pos_map[token].append(pos)\n","            # Si ce token a le POS recherché et au moins 3 lettres, on le \"mark\" comme candidat\n","            if pos == kind and len(token) >= 3:\n","                candidates.add(token)\n","\n","    # 5) Calcule le POS majoritaire pour chaque candidat et met à jour unigrams\n","    for token in candidates:\n","        pos_list = token_pos_map[token]\n","        mode_pos, _ = Counter(pos_list).most_common(1)[0]\n","        if mode_pos == kind:\n","            unigrams[token] = 1\n","\n","    return unigrams"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"u8I0UxIrzYDU"},"outputs":[],"source":["def remove_duplicates_lsh(threshold=0.8, num_perm=256):\n","    \"\"\"\n","    Détecte et supprime les quasi-doublons dans la variable globale `documents`\n","    en utilisant un MinHash LSH (Locality-Sensitive Hashing).\n","\n","    Paramètres:\n","    -----------\n","    - threshold : float\n","        Seuil de similarité Jaccard en-deçà duquel on ne considère pas les documents comme doublons.\n","        (ex: 0.8 = 80% de similarité)\n","    - num_perm : int\n","        Nombre de permutations utilisées pour le MinHash. Plus ce nombre est grand,\n","        plus la précision est élevée, mais le coût de calcul augmente.\n","\n","    Effets:\n","    -------\n","    - Modifie la liste globale `documents` en supprimant les quasi-doublons.\n","    - Met à jour `columns_dict` et `all_soups` (si `source_type == 'europresse'`)\n","      pour rester cohérents avec les documents restants.\n","    \"\"\"\n","    global documents, columns_dict, all_soups\n","\n","    # 1) Construire l'index LSH\n","    # -------------------------------------------------------------------------\n","    lsh = MinHashLSH(threshold=threshold, num_perm=num_perm)\n","\n","    # Pour stocker les MinHash de chaque document\n","    doc_minhashes = []\n","\n","    # On ne garde que les 100 premiers tokens (comme dans votre code initial)\n","    # puis on crée la signature MinHash\n","    for i, doc in enumerate(documents):\n","        tokens_100 = doc.split()[:100]  # tronque à 100 tokens\n","\n","        # Construire un MinHash pour ce document\n","        m = MinHash(num_perm=num_perm)\n","        for token in tokens_100:\n","            # Pour éviter les collisions d'encodage, on encode en UTF-8\n","            m.update(token.encode('utf-8'))\n","        doc_minhashes.append(m)\n","\n","        # On insère dans la structure LSH en associant l'ID du doc\n","        lsh.insert(str(i), m)\n","\n","    # 2) Détecter les doublons via l'interrogation LSH\n","    # -------------------------------------------------------------------------\n","    # On va construire un ensemble d'indices à supprimer\n","    to_remove = set()\n","\n","    # On parcourt chaque document dans l'ordre :\n","    # si un document n'est pas déjà marqué pour suppression,\n","    # on récupère tous ses quasi-doublons et on les marque pour suppression.\n","    for i in range(len(documents)):\n","        if i in to_remove:\n","            continue  # déjà marqué, on passe\n","\n","        # Récupérer les documents similaires dans l'index\n","        candidates = lsh.query(doc_minhashes[i])  # renvoie la liste des \"keys\" insérées\n","\n","        for c in candidates:\n","            c_idx = int(c)\n","            if c_idx != i:\n","                # c_idx est jugé quasi-doublon de i\n","                to_remove.add(c_idx)\n","\n","    # 3) Supprimer les doublons en ordre décroissant d'indice\n","    # -------------------------------------------------------------------------\n","    # (pour ne pas invalider les indices suivants lors du 'del')\n","    indices_to_remove = sorted(to_remove, reverse=True)\n","\n","    for idx in indices_to_remove:\n","        del documents[idx]\n","        for key in columns_dict:\n","            del columns_dict[key][idx]\n","        if source_type == 'europresse':\n","            del all_soups[idx]\n","\n","    # 4) Éventuel affichage / log\n","    # -------------------------------------------------------------------------\n","    print(f\"[LSH] {len(indices_to_remove)} quasi-doublons supprimés parmi {len(doc_minhashes)} documents initiaux.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ipay3YovzYDU"},"outputs":[],"source":["def write_topics_unigrams():\n","    for num_topic in all_nmf_H:\n","        write_unigrams_results(100,\n","                        tfidf_feature_names,\n","                        all_nmf_H[num_topic])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EZUKeMCIzYDV"},"outputs":[],"source":["def write_documents_infos():\n","    # On va stocker nos données non plus sous forme de lignes strings,\n","    # mais en listes de valeurs. Le csv.writer se chargera d'assembler\n","    # correctement le tout.\n","\n","    for num_topic in tqdm(all_nmf_W,\n","                          desc=\"ÉCRITURE DES FICHIERS SUR LE DISQUE\"):\n","\n","        # Préparation des données\n","        rows = []\n","\n","        # Création de l'entête\n","        config_key = num_topic  # ou bien len(scores) + 1, selon votre logique\n","\n","        # Exemple : pour Europresse\n","        if source_type == 'europresse':\n","            # On définit explicitement l'ordre des colonnes\n","            header = [\n","                'title',\n","                'authors',\n","                'raw_authors',\n","                'nb_characters',\n","                'journal',\n","                'date',\n","                'main_topic'\n","            ]\n","\n","            # On ajoute les colonnes score_? où ? est le label associé\n","            for i in range(num_topic):\n","                if config_key in topic_labels_by_config and i < len(topic_labels_by_config[config_key]):\n","                    label = topic_labels_by_config[config_key][i]\n","                    # (Optionnel) Nettoyer/transformer le label pour éviter caractères spéciaux\n","                    # Par exemple :\n","                    # label_sanitized = re.sub(r'[^a-zA-Z0-9_]', '_', label)\n","                    # header.append(f\"score_{label_sanitized}\")\n","                    # Ou simplement :\n","                    header.append(f\"score_{label}\")\n","                else:\n","                    # Si jamais la clé ou l’indice n’existe pas, on met un fallback\n","                    header.append(f\"score_Unknown_{i}\")\n","\n","            rows.append(header)\n","\n","        elif source_type in ['csv', 'istex']:\n","            # On utilise columns_dict pour construire l'entête\n","            header = list(columns_dict.keys())\n","            header.extend(['nb_characters', 'main_topic'])\n","\n","            for i in range(num_topic):\n","                if config_key in topic_labels_by_config and i < len(topic_labels_by_config[config_key]):\n","                    label = topic_labels_by_config[config_key][i]\n","                    header.append(f\"score_{label}\")\n","                else:\n","                    header.append(f\"score_Unknown_{i}\")\n","\n","            rows.append(header)\n","\n","        # Remplissage des données\n","        if source_type == 'europresse':\n","            for i in range(len(all_soups)):\n","                if i < len(all_nmf_W[num_topic]):\n","                    row_data = write_info_europresse(\n","                        all_nmf_W[num_topic][i],\n","                        all_soups[i],\n","                        documents[i]\n","                    )\n","                    # Assurez-vous que write_info_europresse renvoie une liste et non une string\n","                    rows.append(row_data)\n","\n","        elif source_type in ['csv', 'istex']:\n","            for i in range(len(columns_dict['date'])):\n","                row_data = write_info_another(\n","                    all_nmf_W[num_topic][i],\n","                    columns_dict,\n","                    i,\n","                    documents[i]\n","                )\n","                # Même remarque : write_info_another doit renvoyer une liste\n","                rows.append(row_data)\n","\n","        class_suffix = \"_\".join(grammatical_classes)\n","\n","        # Écriture du CSV en utilisant le csv.writer\n","        csv_path = (\n","            f\"{results_path}{base_name}_EXPLORE_TOPICS/\"\n","            f\"{base_name}_database_{num_topic}tc_{class_suffix}_\"\n","            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp.csv\"\n","        )\n","\n","        with open(csv_path, \"w\", encoding='utf-8', newline='') as file_object:\n","            writer = csv.writer(file_object, delimiter=';', quoting=csv.QUOTE_MINIMAL)\n","            for row in rows:\n","                writer.writerow(row)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1aV2Qz11nKeZ"},"outputs":[],"source":["def process_sentiments():\n","    sentiments = []\n","    dates = []\n","    transformed_sentiments = []\n","\n","    model_name = 'nlptown/bert-base-multilingual-uncased-sentiment' # Modèle spécifique pour l'analyse de sentiments\n","    tokenizer = AutoTokenizer.from_pretrained(model_name)\n","    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n","\n","    if torch.backends.mps.is_available():\n","        device = \"mps\"\n","    elif torch.cuda.is_available():\n","        device = 0  # ou torch.device(\"cuda:0\")\n","    else:\n","        device = -1\n","\n","    print('device', device)\n","\n","    sentiment_pipeline = pipeline('sentiment-analysis',\n","                                  model=model,\n","                                  tokenizer=tokenizer,\n","                                  truncation=True,\n","                                  max_length=512,\n","                                  device=device)\n","\n","    # Imaginons que 'documents' est votre tableau de textes\n","    sentiments = [analyze_sentiment(doc, sentiment_pipeline) for doc in tqdm(documents, desc=\"Processing Documents\")]\n","\n","    if source_type == 'europresse':\n","        for soup in all_soups:\n","            header = soup\n","            date_text = extract_information(header, '.DocHeader')\n","            date_text_clean = extract_date_info(date_text)\n","            date_normalized = normalise_date(date_text_clean).replace(';', '').replace('&', '')\n","            dates.append(date_normalized)\n","    else:\n","        dates = formater_liste_dates(columns_dict['date'])\n","\n","    # Filtrer et convertir les dates\n","    new_dates = []\n","    for date_str in dates:\n","        date = extract_and_convert_date(date_str)\n","        new_dates.append(date)\n","\n","    dates = new_dates\n","\n","    # Transformer les sentiments en scores basés sur les étoiles\n","    for doc_sentiments in sentiments:\n","        if doc_sentiments:  # Assurez-vous qu'il n'est pas None ou vide\n","            doc_scores = []\n","            # doc_sentiments est maintenant une liste de dictionnaires\n","            for sentiment_dict in doc_sentiments:\n","                label = sentiment_dict['label']         # ex: '4 stars'\n","                star_rating = int(label.split()[0])     # ex: 4\n","                doc_scores.append(star_rating)\n","\n","            average_score = sum(doc_scores) / len(doc_scores)\n","            transformed_sentiments.append(average_score)\n","        else:\n","            transformed_sentiments.append(None)\n","\n","\n","\n","    # Ici, on modifie la fonction sentiments_heatmaps (ou son appel)\n","    # pour qu'elle considère les arrays comme base. Par exemple :\n","    for relative in [False, True]:\n","        sentiment_by_date_and_topic = sentiments_heatmaps(\n","            relative_normalizaton=relative,\n","            sigma='auto',\n","            transformed_sentiments=transformed_sentiments,\n","            dates=dates\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vV5LKXIdllXM"},"outputs":[],"source":["# ===================================================================\n","# 2) nouvelle : calc_positions_for_continuous_spacing\n","# ===================================================================\n","def calc_positions_for_continuous_spacing(xmin, xmax, text_width, spacing_factor):\n","    \"\"\"\n","    Calcule (sans dessiner) les positions X (en data coords)\n","    où l’on placerait chaque label, en avançant de 'text_width * spacing_factor'\n","    tant que le bord droit du label (current_x + text_width)\n","    ne dépasse pas xmax (avec une petite tolérance).\n","    \"\"\"\n","    positions = []\n","    current_x = xmin + text_width / 2\n","\n","    tolerance_max = xmax + (text_width / 2)\n","    while True:\n","        right_edge = current_x + text_width\n","        if right_edge > tolerance_max:\n","            break\n","        positions.append(current_x)\n","        current_x += text_width * spacing_factor\n","\n","    return positions\n","\n","\n","# ===================================================================\n","# 3) nouvelle : find_optimal_continuous_spacing\n","# ===================================================================\n","def find_optimal_continuous_spacing(xmin, xmax, text_width,\n","                                    spacing_factor_min=1.02,\n","                                    spacing_factor_max=1.2,\n","                                    step=0.001):\n","\n","    best_sf = spacing_factor_min\n","    best_diff = float('inf')\n","    best_positions = []\n","\n","    tolerance_max = xmax + (text_width / 2)\n","    spacing_values = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n","\n","    for sf in spacing_values:\n","        positions = calc_positions_for_continuous_spacing(\n","            xmin=xmin,\n","            xmax=xmax,\n","            text_width=text_width,\n","            spacing_factor=sf\n","        )\n","\n","        if not positions:\n","            diff = 9999\n","        else:\n","            last_x = positions[-1]\n","            right_edge = last_x + text_width\n","            diff = abs(tolerance_max - right_edge)\n","\n","        if diff < best_diff:\n","            best_diff = diff\n","            best_sf = sf\n","            best_positions = positions\n","            if diff < 1e-9:\n","                break\n","\n","    return best_sf, best_positions\n","\n","\n","# ===================================================================\n","# 4) nouvelle : manual_tick_placement_continuous\n","# ===================================================================\n","def manual_tick_placement_continuous(\n","    ax,\n","    xmin,\n","    xmax,\n","    spacing_factor_min=1.02,\n","    spacing_factor_max=1.2,\n","    step=0.001\n","):\n","    \"\"\"\n","    Place manuellement des pseudo-ticks pour l'axe X dans [xmin..xmax].\n","    On coupe les ticks officiels et on dessine les labels via ax.text.\n","    \"\"\"\n","    text_width = compute_text_width_in_data_coords(ax)\n","\n","    # 1) Désactiver les ticks \"officiels\"\n","    ax.set_xticks([])\n","    ax.set_xlim(xmin, xmax)\n","\n","    # 2) Recherche du spacing_factor optimal\n","    best_sf, _ = find_optimal_continuous_spacing(\n","        xmin=xmin,\n","        xmax=xmax,\n","        text_width=text_width,\n","        spacing_factor_min=spacing_factor_min,\n","        spacing_factor_max=spacing_factor_max,\n","        step=step\n","    )\n","    # 3) Placement effectif\n","    positions = calc_positions_for_continuous_spacing(\n","        xmin=xmin,\n","        xmax=xmax,\n","        text_width=text_width,\n","        spacing_factor=best_sf\n","    )\n","\n","    # 4) Transform \"Axes\" + offset en points\n","    offset_axes_transform = mtransforms.offset_copy(\n","        ax.transAxes,\n","        fig=ax.figure,\n","        x=0,\n","        y=-1.0,\n","        units='points'\n","    )\n","\n","    x_min, x_max = ax.get_xlim()\n","    # 5) Dessin de chaque label\n","    for x_val in positions:\n","        x_val_axes = (x_val - x_min) / (x_max - x_min)\n","        label_str = f\"{x_val:.3f}\"  # arrondi; adapter si besoin\n","\n","        ax.text(\n","            x_val_axes,\n","            0.0,   # tout en bas du subplot\n","            label_str,\n","            rotation=90,\n","            rotation_mode='anchor',\n","            ha='right',\n","            va='center',\n","            transform=offset_axes_transform,\n","            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.0')\n","        )\n","\n","    return best_sf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6lJlX8yPqlpd"},"outputs":[],"source":["# Vérification de la Multicollinéarité\n","# Calcul du VIF (Facteur d'Inflation de la Variance)\n","def calculate_vif(df):\n","    vif_data = pd.DataFrame()\n","    vif_data[\"feature\"] = df.columns\n","    vif_data[\"VIF\"] = [variance_inflation_factor(df.values, i) for i in range(len(df.columns))]\n","\n","    return vif_data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ihE7Cy5Fq40a"},"outputs":[],"source":["def analyze_sentiment(text, sentiment_pipeline):\n","    try:\n","        # Analyse de sentiments directement sur le texte complet,\n","        # en demandant explicitement la troncation à 512 tokens\n","        result = sentiment_pipeline(text, truncation=True, max_length=512)\n","        return result\n","    except Exception as e:\n","        print(f\"Error in sentiment analysis: {e}\")\n","        return None"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1AqPelFWq4zL"},"outputs":[],"source":["def compute_text_width_in_data_coords(ax):\n","    \"\"\"\n","    Mesure la largeur (en 'data coords') d'un texte donné,\n","    via un placement temporaire invisible pour récupérer la bounding box en pixels.\n","    \"\"\"\n","    temp_text = ax.text(\n","        0,\n","        0,\n","        '0123456789',\n","        rotation=90,\n","        rotation_mode='anchor',\n","        ha='right',\n","        va='center',\n","        alpha=0,\n","        transform=ax.transAxes  # On place ceci en Axes coords (peu importe où)\n","    )\n","    ax.figure.canvas.draw()\n","    renderer = ax.figure.canvas.get_renderer()\n","    bbox = temp_text.get_window_extent(renderer=renderer)\n","    pixel_width = bbox.width\n","\n","    # Convertir la largeur (en pixels) -> (en data coords sur l'axe X) :\n","    x0_data, _ = ax.transData.transform((0, 0))\n","    x1_data, _ = ax.transData.transform((1, 0))\n","    one_unit_in_pixels = x1_data - x0_data\n","    data_width = pixel_width / one_unit_in_pixels\n","\n","    temp_text.remove()\n","    return data_width\n","\n","\n","def compute_label_positions_for_spacing(ncols, text_width, spacing_factor, with_colormap):\n","    \"\"\"\n","    Calcule (sans dessiner) les positions X (en data coords)\n","    où l’on placerait chaque label, sachant que TOUS les labels\n","    ont la même 'text_width'.\n","\n","    Paramètres\n","    ----------\n","    ncols         : int\n","        Nombre total de labels (i.e. nombre de colonnes).\n","    text_width    : float\n","        Largeur fixe (en data coords) pour chaque label.\n","    spacing_factor: float\n","        Facteur d'espacement entre deux labels successifs.\n","\n","    Retourne\n","    --------\n","    list of (x, text_width)\n","        La liste des positions (x, text_width) en data coords.\n","    \"\"\"\n","    positions = []\n","    current_x = 0.0\n","\n","    while True:\n","        col_index = int(np.floor(current_x))\n","        if col_index >= ncols:\n","            # On a dépassé le nombre de colonnes, on s’arrête\n","            break\n","\n","        # Bord droit (en data coords) si on place le label à \"current_x\"\n","        right_edge = current_x + text_width\n","\n","        if with_colormap:\n","            tolerance_max = (ncols - 1) + 0.17*text_width\n","        else:\n","            tolerance_max = (ncols - 1) + 0.001*text_width\n","\n","        if right_edge <= tolerance_max:\n","            # On enregistre la position (pour info)\n","            positions.append((current_x, text_width))\n","            # On décale \"current_x\" pour le prochain label\n","            current_x += text_width * spacing_factor\n","        else:\n","            # Si on dépasse trop, on arrête\n","            break\n","\n","    return positions\n","\n","\n","def find_best_spacing_factor(\n","    ncols,\n","    text_width,\n","    spacing_factor_min=1.02,\n","    spacing_factor_max=1.2,\n","    step=0.001,\n","    with_colormap=True\n","):\n","    \"\"\"\n","    Cherche le spacing_factor qui permet d'occuper au mieux la place disponible\n","    sans trop déborder la tolérance max = (ncols - 1) pour le dernier label.\n","\n","    Paramètres\n","    ----------\n","    ncols : int\n","        Nombre total de labels.\n","    text_width : float\n","        Largeur fixe (en data coords) à utiliser pour tous les labels.\n","    spacing_factor_min : float\n","        Borne inférieure pour la recherche du spacing factor.\n","    spacing_factor_max : float\n","        Borne supérieure pour la recherche du spacing factor.\n","    step : float\n","        Pas d'incrémentation pour la recherche brute force.\n","\n","    Renvoie\n","    -------\n","    (best_sf, positions):\n","        best_sf : float\n","            Le spacing factor optimal.\n","        positions : list of (x, text_width)\n","            Liste des positions correspondant à best_sf.\n","    \"\"\"\n","    best_sf = spacing_factor_min\n","    best_diff = float('inf')\n","    best_positions = []\n","\n","    spacing_values = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n","\n","    for sf in spacing_values:\n","        positions = compute_label_positions_for_spacing(\n","            ncols=ncols,\n","            text_width=text_width,\n","            spacing_factor=sf,\n","            with_colormap=with_colormap\n","        )\n","\n","        if not positions:\n","            # Aucune position => on fixe un diff arbitraire\n","            diff = 9999\n","        else:\n","            # On regarde la position (et la largeur) du dernier label\n","            last_x, last_w = positions[-1]\n","            right_edge = last_x + last_w\n","            # Tolerance max\n","            if with_colormap:\n","                tolerance_max = (ncols - 1) + 0.17*text_width\n","            else:\n","                tolerance_max = (ncols - 1) + 0.001*text_width\n","\n","            # On calcule la différence\n","            diff = abs(tolerance_max - right_edge)\n","\n","        # Mise à jour du meilleur spacing factor\n","        if diff < best_diff:\n","            best_diff = diff\n","            best_sf = sf\n","            best_positions = positions\n","            # Si diff est vraiment très faible, on peut s'arrêter (optionnel)\n","            if diff < 1e-9:\n","                break\n","\n","    return best_sf, best_positions\n","\n","\n","def manual_tick_placement(\n","    ax,\n","    df,\n","    spacing_factor_min=1.02,\n","    spacing_factor_max=1.2,\n","    step=0.001,\n","    with_colormap=True\n","):\n","    \"\"\"\n","    Place manuellement des \"pseudo-ticks\" et leurs labels,\n","    SANS utiliser ax.set_xticks / ax.set_xticklabels / ax.tick_params.\n","\n","    Hypothèse simplifiée :\n","    - on considère que TOUS les labels ont la même largeur\n","      (calculée sur une date aléatoire par exemple).\n","\n","    Étapes :\n","    1) On désactive l'axe officiel.\n","    2) On crée un transform mixte (X en data, Y en Axes).\n","    3) On prend une date aléatoire dans df.columns -> on calcule la largeur du texte.\n","    4) On cherche le spacing_factor optimal (find_best_spacing_factor).\n","    5) On calcule les positions finales et on dessine chaque label manuellement.\n","    \"\"\"\n","    # 1) Désactiver l'axe \"officiel\"\n","    ax.set_xticks([])\n","    ax.set_xlim(0, len(df.columns) - 1)\n","\n","\n","\n","    ncols = len(df.columns)\n","    if ncols == 0:\n","        return  # Rien à faire si df n'a pas de colonnes\n","\n","    # 3) Prendre une date \"au hasard\" (ou la première), calculer sa largeur\n","    random_col = df.columns[0]   # ou n’importe quel index\n","    text_random = random_col.strftime(\"%Y-%m-%d\")\n","    text_width = compute_text_width_in_data_coords(ax)\n","\n","    # 4) Trouver le spacing_factor optimal\n","    best_sf, positions_preview = find_best_spacing_factor(\n","        ncols=ncols,\n","        text_width=text_width,\n","        spacing_factor_min=spacing_factor_min,\n","        spacing_factor_max=spacing_factor_max,\n","        step=step,\n","        with_colormap=with_colormap\n","    )\n","\n","    # 5) Placement effectif\n","    positions = compute_label_positions_for_spacing(\n","        ncols=ncols,\n","        text_width=text_width,\n","        spacing_factor=best_sf,\n","        with_colormap=with_colormap\n","    )\n","\n","    # On récupère les limites X de l'axe\n","    x_min, x_max = ax.get_xlim()\n","\n","    # 1) Construire une transform \"Axes\" + offset en points\n","    offset_axes_transform = mtransforms.offset_copy(\n","        ax.transAxes,            # on part du repère Axes (0..1)\n","        fig=ax.figure,\n","        x=0,\n","        y=-0.7,\n","        units='points'\n","    )\n","\n","    # 2) Boucle d'affichage\n","    for (x_val, _) in positions:\n","        col_index = int(np.floor(x_val))\n","        if col_index < ncols:\n","            label_str = df.columns[col_index].strftime(\"%Y-%m-%d\")\n","\n","            # Convertir x_val (data) -> x_val_axes (0..1)\n","            x_val_axes = (x_val - x_min) / (x_max - x_min)\n","\n","            # On place le texte en Axes coords\n","            ax.text(\n","                x_val_axes,   # X en [0..1]\n","                0.0,          # Y=0 en Axes coords (bas de l'axe)\n","                label_str,\n","                rotation=90,\n","                rotation_mode='anchor',\n","                ha='right',\n","                va='top',     # ancré \"en haut\" pour que le -2 pts décale vers le bas\n","                transform=offset_axes_transform\n","            )\n","\n","\n","\n","    return best_sf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fGMT8MD62vY8"},"outputs":[],"source":["def measure_text_height_axes(ax, label=\"999.99\"):\n","    \"\"\"\n","    Renvoie :\n","      - text_height_axes : la hauteur TOTALE (bottom -> top) en coords AXES\n","      - offset_axes      : la distance (baseline - bottom) de la bbox en coords AXES\n","    \"\"\"\n","    # Placement (invisible) d'un texte aligné baseline, à la position (0,0) en AXES\n","    text_baseline = ax.text(\n","        0,\n","        0,\n","        label,\n","        va='baseline',\n","        ha='left',\n","        alpha=0,  # invisible\n","        transform=ax.transAxes  # <-- On le place en Axes !\n","    )\n","\n","    # On force un rendu pour obtenir la bbox en coords pixels\n","    ax.figure.canvas.draw()\n","    renderer = ax.figure.canvas.get_renderer()\n","    bbox = text_baseline.get_window_extent(renderer=renderer)\n","    text_baseline.remove()\n","\n","    # Coordonnées (x, y) en PIXELS du point (0, 0) Axes\n","    anchor_pixel = ax.transAxes.transform((0, 0))\n","\n","    # Hauteur de la bbox en pixels\n","    text_height_pixel = bbox.height\n","\n","    # Décalage (baseline -> bottom) en pixels\n","    offset_pixel = anchor_pixel[1] - bbox.y0\n","\n","    # 1 “unité Axes” = combien de pixels ?\n","    y0_pix = ax.transAxes.transform((0, 0))[1]\n","    y1_pix = ax.transAxes.transform((0, 1))[1]\n","    one_axes_unit_in_pixels = abs(y1_pix - y0_pix)\n","\n","    # Conversion des pixels -> coords Axes\n","    text_height_axes = text_height_pixel / one_axes_unit_in_pixels\n","    offset_axes      = offset_pixel      / one_axes_unit_in_pixels\n","\n","    return text_height_axes, offset_axes\n","\n","\n","\n","\n","def data_to_axes(y_data, data_min, data_max):\n","    \"\"\"Convertit y_data (dans [data_min, data_max]) en [0,1].\"\"\"\n","    return (y_data - data_min) / (data_max - data_min)\n","\n","\n","\n","def axes_to_data(y_axes, data_min, data_max):\n","    \"\"\"Convertit y_axes (dans [0,1]) en [data_min, data_max].\"\"\"\n","    return y_axes*(data_max - data_min) + data_min\n","\n","\n","def compute_label_positions_axes(\n","    text_height_axes,\n","    offset_axes,\n","    spacing_factor,\n","    vmin_axes=0.0,\n","    vmax_axes=1.0\n","):\n","    \"\"\"\n","    Calcule les positions en Axes-coords pour placer les labels\n","    de vmin_axes à vmax_axes (en général [0,1]).\n","\n","    On part baseline = vmin_axes,\n","    et on incrémente de (text_height_axes * spacing_factor).\n","    \"\"\"\n","    positions = []\n","    current_baseline = vmin_axes + 0.0016\n","\n","    while True:\n","        top_of_bbox = current_baseline + (text_height_axes - offset_axes)\n","        if top_of_bbox > vmax_axes:\n","            break\n","\n","        # On enregistre la position en Axes-coords\n","        positions.append(current_baseline)\n","\n","        current_baseline += (text_height_axes * spacing_factor)\n","        if current_baseline > vmax_axes:\n","            break\n","\n","    return positions\n","\n","\n","\n","\n","def get_final_top_baseline_axes(baseline, text_height_axes, offset_axes):\n","    \"\"\"\n","    baseline = la baseline du texte (axes-coords)\n","    Retourne la coord \"top\" de la bbox du dernier label.\n","    \"\"\"\n","    return baseline + (text_height_axes - offset_axes)\n","\n","def find_best_spacing_factor_axes(\n","    ax,\n","    text_height_axes,\n","    offset_axes,\n","    spacing_factor_min=1.02,\n","    spacing_factor_max=1.2,\n","    step=0.001,\n","    vmin_axes=0.0,\n","    vmax_axes=1.0\n","):\n","    best_sf = spacing_factor_min\n","    best_diff = float('inf')\n","    spacing_factors = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n","\n","    for sf in tqdm(spacing_factors):\n","        positions = compute_label_positions_axes(\n","            text_height_axes, offset_axes, sf,\n","            vmin_axes, vmax_axes\n","        )\n","        if positions:\n","            last_baseline = positions[-1]\n","            last_top = get_final_top_baseline_axes(last_baseline, text_height_axes, offset_axes)\n","            diff = abs(last_top - vmax_axes)\n","        else:\n","            # Si on ne trouve aucun label => diff = 1.0\n","            diff = 1.0\n","\n","        if diff < best_diff:\n","            best_diff = diff\n","            best_sf = sf\n","            if diff == 0:\n","                break\n","\n","    return best_sf\n","\n","\n","\n","\n","def manual_colorbar_ticks(\n","    fig,\n","    ax,\n","    data_min,      # 2.60 par exemple\n","    data_max,      # 3.46 par exemple\n","    spacing_factor_min=1.02,\n","    spacing_factor_max=1.2,\n","    step=0.001\n","):\n","    \"\"\"\n","    1) Mesure la hauteur du texte en Axes-coords\n","    2) Trouve un spacing_factor optimal pour remplir [0,1] verticalement\n","    3) Calcule toutes les baselines en [0,1]\n","    4) Affiche des labels correspondant à la \"vraie\" valeur data\n","       sur la baseline Axes correspondante\n","    \"\"\"\n","    # 1) Hauteur du texte\n","    text_height_axes, offset_axes = measure_text_height_axes(ax, label=\"999.99\")\n","\n","    # 2) Spacing factor optimal\n","    sf_opt = find_best_spacing_factor_axes(\n","        ax,\n","        text_height_axes,\n","        offset_axes,\n","        spacing_factor_min,\n","        spacing_factor_max,\n","        step,\n","        vmin_axes=0.0,\n","        vmax_axes=1.0\n","    )\n","\n","    # 3) Calcul positions\n","    positions_axes = compute_label_positions_axes(\n","        text_height_axes,\n","        offset_axes,\n","        sf_opt,\n","        vmin_axes=0.0,\n","        vmax_axes=1.0\n","    )\n","\n","    # Créez un offset de 10 points vers la droite et 0 points vers le haut\n"," #   offset = transforms.ScaledTranslation(0.7/72, 0, fig.dpi_scale_trans)\n","\n","    offset_axes_transform = mtransforms.offset_copy(\n","        ax.transAxes,\n","        fig=ax.figure,\n","        x=1.5,\n","        y=0,\n","        units='points'\n","    )\n","    # 10/72 car 1 point = 1/72 inch\n","\n","    # Combinez ax.transAxes avec cet offset\n","  #  trans = ax.transAxes + offset_axes_transform\n","\n","    # 4) Dessin\n","    for baseline_axes in positions_axes:\n","        # Convertir la baseline axes -> data\n","        val_data = axes_to_data(baseline_axes, data_min, data_max)\n","        label_str = f\"{val_data:.2f}\"\n","\n","        ax.text(\n","            1,\n","            baseline_axes,\n","            label_str,\n","            va='baseline',\n","            ha='left',\n","            transform=offset_axes_transform\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"diRhYIUQ2vXH"},"outputs":[],"source":["def create_custom_colorbar(fig,\n","                           df_normalized,\n","                           cmap=plt.cm.coolwarm,\n","                           colorbar_position=[1.0, 0.0, 0.02, 0.85],\n","                           step=0.0005,\n","                           spacing_factor_min=1.02,\n","                           spacing_factor_max=1.2):\n","\n","    # Déterminer les valeurs min et max pour la normalisation\n","    vmin = df_normalized.min().min()\n","    vmax = df_normalized.max().max()\n","\n","    # Créer la normalisation et l'objet ScalarMappable\n","    norm = mcolors.Normalize(vmin=vmin, vmax=vmax)\n","    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n","    sm.set_array([])  # Nécessaire pour l'affectation de la colorbar\n","\n","    # Ajouter un axe pour la colorbar (à droite, ici)\n","    cbar_ax = fig.add_axes(colorbar_position)\n","\n","    # Créer la colorbar\n","    cbar = fig.colorbar(sm,\n","                        cax=cbar_ax,\n","                        orientation='vertical',\n","                        spacing='proportional',\n","                        extend='neither',\n","                        fraction=1.0,\n","                        pad=0.0)\n","\n","    # Personnaliser l'apparence de la colorbar\n","    cbar.outline.set_visible(False)  # Supprime la bordure\n","    cbar.set_ticks([])               # Supprime les graduations par défaut\n","\n","    # Appeler la fonction manuelle pour définir les graduations\n","    manual_colorbar_ticks(\n","        fig,\n","        cbar_ax,\n","        vmin,\n","        vmax,\n","        step=step,\n","        spacing_factor_min=spacing_factor_min,\n","        spacing_factor_max=spacing_factor_max\n","    )"]},{"cell_type":"code","source":["def remove_outliers_by_mean(df, threshold=100):\n","    \"\"\"\n","    Pour chaque (Group, Date), remplace par 0\n","    toute valeur > threshold * moyenne_des_autres (dans la même ligne).\n","\n","    Paramètres\n","    ----------\n","    df : pd.DataFrame\n","        Le DataFrame dont les lignes sont des Group et les colonnes des dates.\n","    threshold : float\n","        Facteur multiplicatif pour détecter les outliers (défaut = 100).\n","\n","    Retour\n","    ------\n","    df_out : pd.DataFrame\n","        Copie de df avec les outliers remplacés par 0.\n","    outliers_df : pd.DataFrame\n","        Tableau listant les outliers détectés (Group, Date, Value).\n","    \"\"\"\n","    # Copie pour ne pas modifier l'original\n","    df_out = df.copy()\n","\n","    # Liste pour stocker les outliers détectés\n","    outliers = []\n","\n","    # Parcours de chaque ligne (group)\n","    for group, row in df_out.iterrows():\n","        # Parcours des colonnes (dates)\n","        for date in df_out.columns:\n","            val = row[date]\n","\n","            # Moyenne des autres colonnes de la ligne\n","            avg_ignore = row.drop(labels=date).mean()\n","\n","            # Test d'outlier : val > threshold * moyenne_des_autres\n","            if (avg_ignore > 0) and (val > threshold * avg_ignore):\n","                # On remplace par 0 dans le DataFrame\n","                df_out.at[group, date] = 0\n","                # On garde trace de l'outlier\n","                outliers.append((group, date, val))\n","\n","    # Création d'un DataFrame pour les outliers détectés\n","    outliers_df = pd.DataFrame(outliers, columns=['Group', 'Date', 'Value'])\n","\n","    return df_out, outliers_df"],"metadata":{"id":"VMRbA_5SwbxO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numba\n","import numpy as np\n","\n","@numba.njit\n","def cost_sq(x, y):\n","    return (x - y)**2\n","\n","@numba.njit\n","def downsample(seq):\n","    \"\"\"\n","    Réduit la série par un facteur de 10 en moyennant chaque groupe de 10 points.\n","    Exemple : [x0, x1, ..., x9, x10, x11, ..., x19, ...] -> [(x0+x1+...+x9)/10, (x10+x11+...+x19)/10, ...]\n","    \"\"\"\n","    new_len = len(seq) // 10\n","    new_seq = np.empty(new_len, dtype=np.float64)\n","    for i in range(new_len):\n","        start = 10 * i\n","        new_seq[i] = np.mean(seq[start:start + 10])  # Moyenne des 10 points\n","    return new_seq\n","\n","\n","@numba.njit\n","def dtw_distance(seq1, seq2):\n","    \"\"\"\n","    DTW simple renvoyant uniquement la distance, sans chemin.\n","    \"\"\"\n","    n, m = len(seq1), len(seq2)\n","    dtw = np.full((n+1, m+1), np.inf)\n","    dtw[0, 0] = 0.0\n","\n","    for i in range(1, n+1):\n","        for j in range(1, m+1):\n","            cost = cost_sq(seq1[i-1], seq2[j-1])\n","            dtw[i, j] = cost + min(\n","                dtw[i-1, j],    # \"delete\" (avancer dans seq1)\n","                dtw[i, j-1],    # \"insert\" (avancer dans seq2)\n","                dtw[i-1, j-1]   # \"match\"\n","            )\n","    return dtw[n, m]\n","\n","@numba.njit\n","def _dtw_distance_and_path(seq1, seq2):\n","    \"\"\"\n","    DTW standard *uniquement* pour le 'base case' (petites séries).\n","    Renvoie (path, distance). Le path est nécessaire pour construire\n","    la fenêtre en plus haute résolution. On ne l'exposera pas en public.\n","    \"\"\"\n","    n, m = len(seq1), len(seq2)\n","    dtw = np.full((n+1, m+1), np.inf)\n","    direction = np.zeros((n+1, m+1), dtype=np.int8)\n","\n","    dtw[0, 0] = 0.0\n","    for i in range(1, n+1):\n","        for j in range(1, m+1):\n","            cost = cost_sq(seq1[i-1], seq2[j-1])\n","            # On teste 3 possibilités\n","            del_val = dtw[i-1, j]\n","            ins_val = dtw[i, j-1]\n","            match_val = dtw[i-1, j-1]\n","\n","            if del_val < ins_val and del_val < match_val:\n","                dtw[i, j] = cost + del_val\n","                direction[i, j] = 1\n","            elif ins_val < match_val:\n","                dtw[i, j] = cost + ins_val\n","                direction[i, j] = 2\n","            else:\n","                dtw[i, j] = cost + match_val\n","                direction[i, j] = 0\n","\n","    # Rétropropagation du chemin (en partant de (n,m))\n","    path = []\n","    i, j = n, m\n","    while i > 0 and j > 0:\n","        path.append((i-1, j-1))\n","        move = direction[i, j]\n","        if move == 0:\n","            i -= 1\n","            j -= 1\n","        elif move == 1:\n","            i -= 1\n","        else:\n","            j -= 1\n","\n","    path.reverse()\n","    return path, dtw[n, m]\n","\n","@numba.njit\n","def expand_window(path_lowres, len_x, len_y, radius=1):\n","    \"\"\"\n","    Projette le chemin en basse résolution (path_lowres)\n","    vers la taille (len_x, len_y), en créant un voisinage (window)\n","    autour de ce chemin. Retourne un set() de paires (i, j) autorisées.\n","    \"\"\"\n","    window = set()\n","    if len(path_lowres) == 0:\n","        # Par sécurité : si path vide, autoriser toute la matrice\n","        for i in range(len_x):\n","            for j in range(len_y):\n","                window.add((i, j))\n","        return window\n","\n","    # Indices max en basse résolution\n","    max_i_low = path_lowres[-1][0]\n","    max_j_low = path_lowres[-1][1]\n","\n","    # Facteurs d'échelle pour passer de la résolution basse à la fine\n","    scale_x = (len_x - 1) / max(1, max_i_low)\n","    scale_y = (len_y - 1) / max(1, max_j_low)\n","\n","    for (i_l, j_l) in path_lowres:\n","        i_center = int(i_l * scale_x)\n","        j_center = int(j_l * scale_y)\n","        # Autorise i_center +/- radius et j_center +/- radius\n","        for di in range(-radius, radius+1):\n","            for dj in range(-radius, radius+1):\n","                i_w = i_center + di\n","                j_w = j_center + dj\n","                if 0 <= i_w < len_x and 0 <= j_w < len_y:\n","                    window.add((i_w, j_w))\n","\n","    return window\n","\n","@numba.njit\n","def dtw_distance_constrained(seq1, seq2, window):\n","    \"\"\"\n","    Calcule la distance DTW en ne considérant que les cellules (i,j)\n","    présentes dans 'window' (un set de paires).\n","    \"\"\"\n","    n, m = len(seq1), len(seq2)\n","    dtw = np.full((n+1, m+1), np.inf)\n","    dtw[0, 0] = 0.0\n","\n","    # On trie les (i, j) par i+j croissant pour respecter\n","    # les dépendances (i-1, j), (i, j-1), etc.\n","    window_list = list(window)\n","    window_list.sort(key=lambda x: x[0] + x[1])\n","\n","    for (i, j) in window_list:\n","        i1, j1 = i+1, j+1\n","        cost = cost_sq(seq1[i], seq2[j])\n","\n","        # On prend le min parmi (i-1, j), (i, j-1), (i-1, j-1)\n","        tmp = dtw[i1, j1]\n","        if i1 > 0 and dtw[i1-1, j1] < tmp:\n","            tmp = dtw[i1-1, j1]\n","        if j1 > 0 and dtw[i1, j1-1] < tmp:\n","            tmp = dtw[i1, j1-1]\n","        if i1 > 0 and j1 > 0 and dtw[i1-1, j1-1] < tmp:\n","            tmp = dtw[i1-1, j1-1]\n","\n","        dtw[i1, j1] = cost + tmp\n","\n","    return dtw[n, m]\n","\n","@numba.njit\n","def _fastdtw_path_lowres(seq1, seq2, radius=1):\n","    \"\"\"\n","    Fonction 'privée' qui calcule le chemin + distance en basse résolution,\n","    afin de construire la fenêtre. Elle *n'est pas* appelée en dernier,\n","    juste pour connaître la zone de recherche à l'échelle fine.\n","    \"\"\"\n","    # -- Cas de base : si la série est très courte, on récupère path + distance --\n","    if len(seq1) <= radius + 2 or len(seq2) <= radius + 2:\n","        return _dtw_distance_and_path(seq1, seq2)\n","\n","    # -- Sinon, downsample --\n","    shrunk1 = downsample(seq1)\n","    shrunk2 = downsample(seq2)\n","\n","    # -- Récursif : on obtient un path_lowres à l'échelle encore plus réduite --\n","    path_lowres, _ = _fastdtw_path_lowres(shrunk1, shrunk2, radius)\n","\n","    # -- On \"projette\" ce path_lowres à l'échelle courante pour re-calculer\n","    #    un chemin \"un peu plus précis\" en se limitant à une fenêtre contrainte --\n","    window = expand_window(path_lowres, len(seq1), len(seq2), radius)\n","    # Ici, pour obtenir un *path* local, on pourrait coder un \"dtw_path_constrained\",\n","    # mais pour aller au plus simple, on recycle _dtw_distance_and_path()\n","    # *sans* repasser par un full DTW sur toute la fenêtre.\n","    # Pour la démonstration, on va \"tricher\" en faisant un DTW complet\n","    # dans la fenêtre en mode distance SEULEMENT, et on NE RECALCULE PAS le path\n","    # très précis. On s’en contente pour renvoyer un path \"approximatif\" identique.\n","\n","    # => Pour le *vrai* path, il faudrait un backtrack partiel dans la fenêtre.\n","    #    Ici, on renvoie juste le path_lowres + la distance contrainte \"fine\".\n","    dist_constrained = dtw_distance_constrained(seq1, seq2, window)\n","    return path_lowres, dist_constrained\n","\n","@numba.njit\n","def fastdtw_distance(seq1, seq2, radius=1):\n","    \"\"\"\n","    Fonction publique : renvoie uniquement la distance FastDTW\n","    entre seq1 et seq2, pour un 'radius' donné.\n","    \"\"\"\n","    # -- Cas de base : si la série est courte --\n","    if len(seq1) <= radius + 2 or len(seq2) <= radius + 2:\n","        return dtw_distance(seq1, seq2)\n","\n","    # -- On calcule un chemin approximatif en basse résolution --\n","    path_lowres, _ = _fastdtw_path_lowres(seq1, seq2, radius)\n","\n","    # -- On construit la fenêtre autour de ce chemin --\n","    window = expand_window(path_lowres, len(seq1), len(seq2), radius)\n","\n","    # -- On calcule la distance DTW dans la fenêtre --\n","    dist_final = dtw_distance_constrained(seq1, seq2, window)\n","    return dist_final"],"metadata":{"id":"GuPp-86Nl27R"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UuRdVcp92vUy"},"outputs":[],"source":["def plot_custom_heatmap(\n","    df,\n","    sigma='auto',\n","    cmap=\"coolwarm\",\n","    relative_normalizaton=True,\n","    with_colormap=True\n","):\n","    df, outliers_table = remove_outliers_by_mean(df, threshold=100000)\n","\n","    if len(outliers_table) > 1:\n","        print('Valeurs aberrantes trouvées')\n","        print(outliers_table)\n","\n","    df = df.astype(float)\n","\n","    # 1) Application du filtre gaussien, ligne par ligne\n","    list_of_series = []\n","    for index, row in df.iterrows():\n","        filtered_values = gaussian_filter(row, sigma=sigma)\n","        s = pd.Series(filtered_values, index=df.columns, name=index)\n","        list_of_series.append(s)\n","\n","    df_normalized = pd.concat(list_of_series, axis=1).T\n","\n","    if relative_normalizaton:\n","        list_of_series = []\n","        for index, row in df_normalized.iterrows():\n","            normalized_values = (row - row.min()) / (row.max() - row.min()) if (row.max() != row.min()) else row\n","            s = pd.Series(normalized_values, index=df_normalized.columns, name=index)\n","            list_of_series.append(s)\n","\n","        df_normalized = pd.concat(list_of_series, axis=1).T\n","\n","    if len(df_normalized) > 1:\n","       # if relative_normalizaton:\n","        # 1) On calcule, pour chaque série, la colonne où se situe son max\n","        max_positions = df_normalized.idxmax(axis=1)\n","\n","        # 2) On trie les lignes (les séries) selon ces positions croissantes\n","        sorted_index = max_positions.sort_values().index\n","\n","        # 3) On réordonne le DataFrame dans ce nouvel ordre\n","        df_normalized = df_normalized.loc[sorted_index]\n","   #     else:\n","            # 1. Calcul de la distance en euclidien avec pdist\n","     #       condensed_dist_matrix = pdist(df_normalized.values, metric='euclidean')\n","\n","            # 2. Classification hiérarchique (méthode \"average\")\n","      #      Z = linkage(condensed_dist_matrix, method='average', optimal_ordering=True)\n","\n","            # 3. Récupérer l'ordre des feuilles\n","       #     dendro = dendrogram(Z, no_plot=True)\n","        #    leaves_order = dendro['leaves']\n","\n","            # 4. Réordonner le DataFrame\n","         #   df_normalized = df_normalized.iloc[leaves_order]\n","\n","\n","\n","    # 8) Ajustement de la hauteur de figure\n","    figure_height_inch = (df_normalized.shape[0] * PX_PER_TOPIC) / DPI\n","\n","    # 9) Création de la figure et tracé de la heatmap\n","    fig = plt.figure(figsize=(FIGURE_WIDTH_INCH, figure_height_inch), dpi=DPI)\n","    main_ax = fig.add_axes([0.3, 0.0, 0.697, 0.85])\n","\n","    mask = pd.DataFrame(False, index=df_normalized.index, columns=df_normalized.columns)\n","\n","    # On passe à True directement via .loc,\n","    # en prenant outliers_table[\"Group\"] comme index de lignes\n","    # et outliers_table[\"Date\"] comme index de colonnes\n","    mask.loc[outliers_table[\"Group\"], outliers_table[\"Date\"]] = True\n","\n","    ax = sns.heatmap(\n","        df_normalized,\n","        cmap=cmap,\n","        ax=main_ax,\n","        cbar=False,\n","        rasterized=False,\n","        linewidths=0.0,\n","        linecolor=\"white\",\n","        mask=mask\n","    )\n","\n","\n","    # 10) Tracé des lignes de séparation horizontales\n","    for i in range(1, df_normalized.shape[0]):\n","        ax.axhline(i, color=\"white\", linewidth=1)\n","\n","    # 11) Affichage manuel des labels (index) à gauche\n","    offset_axes_transform_2 = mtransforms.offset_copy(\n","        ax.transAxes,\n","        fig=ax.figure,\n","        x=-1.5,  # Décalage en points\n","        y=0,\n","        units='points'\n","    )\n","\n","    for i, label in enumerate(df_normalized.index):\n","        # On calcule la position du haut vers le bas\n","        y_pos = (df_normalized.shape[0] - i - 0.5) / df_normalized.shape[0]\n","        ax.text(\n","            0,\n","            y_pos,\n","            label,\n","            ha='right',\n","            va='center',\n","            transform=offset_axes_transform_2\n","        )\n","\n","    # 12) Supprimer les bordures & masquer les graduations Y\n","    for spine in ax.spines.values():\n","        spine.set_visible(False)\n","\n","    ax.set_yticks([])\n","\n","    # Placement manuel des ticks X (dates ou colonnes) si besoin\n","    manual_tick_placement(\n","        ax,\n","        df_normalized,\n","        spacing_factor_min=1.02,\n","        spacing_factor_max=1.2,\n","        step=0.0001,\n","        with_colormap=with_colormap\n","    )\n","\n","    # 13) Colorbar facultative\n","    if with_colormap:\n","        create_custom_colorbar(\n","            fig,\n","            df_normalized=df_normalized,\n","            cmap=cmap,\n","            step=0.0001,\n","            spacing_factor_min=1.02,\n","            spacing_factor_max=1.2\n","        )"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"c2f3fbqW2vSb"},"outputs":[],"source":["def sentiments_heatmaps(relative_normalizaton, sigma, transformed_sentiments, dates):\n","    # Calcul du poids total de chaque topic par jour pour chaque topic_count\n","    total_weight_by_topic_count_topic_and_date = {}\n","    for topic_count, W_matrix in all_nmf_W.items():\n","        for article_num, topic_scores in enumerate(W_matrix):\n","            # Vérification pour éviter les erreurs d'index\n","            if article_num >= len(dates):\n","                continue\n","            article_date = dates[article_num]\n","\n","            # topic_scores est un array / liste de poids. Exemple: [0.2, 0.5, 0.1, ...]\n","            for topic_num, topic_weight in enumerate(topic_scores):\n","                key = (topic_count, topic_num, article_date)\n","                if key not in total_weight_by_topic_count_topic_and_date:\n","                    total_weight_by_topic_count_topic_and_date[key] = topic_weight\n","                else:\n","                    total_weight_by_topic_count_topic_and_date[key] += topic_weight\n","\n","\n","    # Initialisation d'un dictionnaire pour stocker les sentiments normalisés\n","    # par date, topic_count et topic_num\n","    sentiment_by_date_and_topic = {}\n","\n","    for topic_count, W_matrix in all_nmf_W.items():\n","        for article_num, topic_scores in enumerate(W_matrix):\n","            if article_num >= len(dates) or article_num >= len(transformed_sentiments):\n","                continue\n","            article_date = dates[article_num]\n","            sentiment_score = transformed_sentiments[article_num]\n","\n","            for topic_num, topic_weight in enumerate(topic_scores):\n","                adjusted_sentiment_score = sentiment_score * topic_weight\n","\n","                weight_key = (topic_count, topic_num, article_date)\n","                total_weight = total_weight_by_topic_count_topic_and_date.get(weight_key, 0)\n","\n","                if total_weight > 0:\n","                    normalized_sentiment_score = adjusted_sentiment_score / total_weight\n","                else:\n","                    normalized_sentiment_score = 0\n","\n","                combined_key = (topic_count, topic_num, article_date)\n","                if combined_key not in sentiment_by_date_and_topic:\n","                    sentiment_by_date_and_topic[combined_key] = [normalized_sentiment_score]\n","                else:\n","                    sentiment_by_date_and_topic[combined_key].append(normalized_sentiment_score)\n","\n","\n","    # Calcul de la moyenne pour chaque combinaison de (topic_count, topic_num, date)\n","    for key, normalized_sentiments in sentiment_by_date_and_topic.items():\n","        average_sentiment = sum(normalized_sentiments)  # / len(normalized_sentiments) si besoin\n","        sentiment_by_date_and_topic[key] = average_sentiment\n","\n","    # Filtrer les combinaisons avec un score de 0\n","    sentiment_by_date_and_topic = {\n","        k: v for k, v in sentiment_by_date_and_topic.items() if v != 0\n","    }\n","\n","    # Création du dossier de résultats si nécessaire\n","    if not os.path.exists(f\"{results_path}{base_name}_TOPICS_SENTIMENTS_DYNAMICS_HEATMAPS/\"):\n","        os.makedirs(f\"{results_path}{base_name}_TOPICS_SENTIMENTS_DYNAMICS_HEATMAPS/\")\n","\n","    # Boucle principale : on génère la heatmap pour chaque topic_count\n","    for topic_count in all_nmf_W:\n","        # 1) On construit d'abord un dictionnaire (topic_num, date) -> sentiment\n","        filtered_data = {\n","            (topic_num, date): sentiment\n","            for (count, topic_num, date), sentiment in sentiment_by_date_and_topic.items()\n","            if count == topic_count\n","        }\n","\n","        # 2) Conversion en DataFrame\n","        #    On sépare (topic_num, date) en deux colonnes distinctes : Topic et Date\n","        df = pd.DataFrame(list(filtered_data.items()), columns=['Topic_Date', 'Sentiment'])\n","        df[['Topic', 'Date']] = pd.DataFrame(df['Topic_Date'].tolist(), index=df.index)\n","\n","        # 3) On crée un DataFrame de mapping \"Topic -> Label\"\n","        #    (en s'appuyant sur la liste topic_labels_by_config[topic_count])\n","        df_labels = pd.DataFrame({\n","            'Topic': range(len(topic_labels_by_config[topic_count])),\n","            'Topic_label': topic_labels_by_config[topic_count]\n","        })\n","\n","        # 4) On fusionne df et le mapping pour obtenir le label de chaque topic_num\n","        df = df.merge(df_labels, on='Topic', how='left')\n","\n","        # 5) On fait le pivot en utilisant le **label** comme index\n","        df = df.pivot(index=\"Topic_label\", columns=\"Date\", values=\"Sentiment\")\n","\n","        # 6) Interpolation et imputation\n","        df_imputed = df.ffill(axis=1).bfill(axis=1)\n","        df = df_imputed.interpolate(method='linear', axis=1)\n","\n","        # 7) Comme dans le code original, on transpose pour gérer la chronologie\n","        df_transposed = df.T\n","\n","        start_date = df_transposed.index.min()\n","        end_date = df_transposed.index.max()\n","        all_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n","\n","        # 8) On réindexe pour ne rien perdre, puis on interpole\n","        df_reindexed = df_transposed.reindex(all_dates)\n","        df_interpolated = df_reindexed.interpolate(method='linear')\n","\n","        # 9) Re-transposer : chaque ligne correspond désormais à un label (Topic_label)\n","        df = df_interpolated.T\n","\n","        # NOTE IMPORTANTE :\n","        # À ce stade, l'index de df est constitué des \"Topic_label\", et non plus des topic_num.\n","        # Tu n'as donc plus besoin de faire un `labels_for_my_df = [...]`.\n","        # Les labels SONT déjà dans df.index, donc si on veut un array/list pour le plotting :\n","        labels_for_my_df = df.index.to_list()\n","\n","        # Ajustement éventuel de sigma si c'est 'auto'\n","        if sigma == 'auto':\n","            sigma = len(df.columns) / 15\n","\n","        # Plot de la heatmap\n","        plot_custom_heatmap(\n","            df,\n","            cmap='coolwarm',\n","            sigma=sigma,\n","            relative_normalizaton=relative_normalizaton,\n","            with_colormap=True\n","        )\n","\n","        # Sauvegarde de la figure\n","        plt.savefig(\n","            f\"{results_path}{base_name}_TOPICS_SENTIMENTS_DYNAMICS_HEATMAPS/\"\n","            f\"{base_name}_topics_sentiments_dynamics_heatmap_{topic_count}tc_{relative_normalizaton}n_{int(sigma)}s_\"\n","            f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","            f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_dtwcompletechdtw2.png\",\n","            dpi=DPI,\n","            bbox_inches='tight',\n","            pad_inches=0\n","        )\n","        plt.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMlv2EWW2vQE"},"outputs":[],"source":["def compute_text_height_in_data_coords(ax):\n","    \"\"\"\n","    Mesure la hauteur (en 'data coords') d'un texte donné,\n","    via un placement temporaire invisible pour récupérer la bounding box en pixels.\n","    \"\"\"\n","    # 1) On place du texte \"invisible\" (alpha=0) dans l'axe,\n","    #    peu importe où (ici en Axes coords = transAxes).\n","    #    Note : rotation=0 pour mesurer une hauteur \"verticale\" classique.\n","    temp_text = ax.text(\n","        0,\n","        0,\n","        '0123456789',       # Exemple de chaîne un peu longue\n","        rotation=0,\n","        rotation_mode='anchor',\n","        ha='left',\n","        va='bottom',\n","        alpha=0,\n","        transform=ax.transAxes  # On le place en Axes coords\n","    )\n","\n","    # 2) On force un dessin pour que la bounding box soit calculée\n","    ax.figure.canvas.draw()\n","    renderer = ax.figure.canvas.get_renderer()\n","\n","    # 3) On récupère la bounding box en pixels\n","    bbox = temp_text.get_window_extent(renderer=renderer)\n","    pixel_height = bbox.height  # Hauteur en pixels\n","\n","    # 4) Convertir la hauteur (en pixels) -> (en 'data coords' sur l'axe Y) :\n","    #    On regarde le décalage vertical en pixels pour \"1\" unité sur l'axe Y\n","    x0_data, y0_data = ax.transData.transform((0, 0))\n","    x1_data, y1_data = ax.transData.transform((0, 1))\n","    one_unit_in_pixels = y1_data - y0_data\n","\n","    data_height = pixel_height / one_unit_in_pixels\n","\n","    # 5) Nettoyage : on supprime le texte temporaire\n","    temp_text.remove()\n","\n","    return data_height\n","\n","\n","def calc_positions_for_continuous_spacing_Y(ymin, ymax, text_height, spacing_factor):\n","    \"\"\"\n","    Calcule (sans dessiner) les positions Y (en coordonnées 'data')\n","    où l’on placerait chaque label, en avançant de 'text_height * spacing_factor'\n","    tant que le bord \"supérieur\" du label (current_y + text_height)\n","    ne dépasse pas ymax (avec une petite tolérance).\n","    \"\"\"\n","    positions = []\n","    # On démarre de ymin + text_height/2 pour centrer le label sur cette position\n","    current_y = ymin + text_height / 2\n","\n","    # Tolérance permettant de s'assurer que le label complet reste dans [ymin..ymax]\n","    tolerance_max = ymax + text_height / 2\n","\n","    while True:\n","        top_edge = current_y + text_height\n","        if top_edge > tolerance_max:\n","            break\n","        positions.append(current_y)\n","        current_y += text_height * spacing_factor\n","\n","    return positions\n","\n","\n","def find_optimal_continuous_spacing_Y(ymin, ymax, text_height,\n","                                      spacing_factor_min=1.02,\n","                                      spacing_factor_max=1.2,\n","                                      step=0.001):\n","    \"\"\"\n","    Cherche le \"spacing_factor\" optimal (entre spacing_factor_min et spacing_factor_max)\n","    pour maximiser le remplissage de [ymin..ymax] par des labels\n","    espacés de manière continue.\n","    \"\"\"\n","    best_sf = spacing_factor_min\n","    best_diff = float('inf')\n","    best_positions = []\n","\n","    # Comme dans la fonction X, on définit une tolérance similaire\n","    tolerance_max = ymax + text_height / 2\n","    spacing_values = np.arange(spacing_factor_min, spacing_factor_max + step, step)\n","\n","    for sf in spacing_values:\n","        positions = calc_positions_for_continuous_spacing_Y(\n","            ymin=ymin,\n","            ymax=ymax,\n","            text_height=text_height,\n","            spacing_factor=sf\n","        )\n","\n","        # Si aucune position n'est retournée, c'est que sf est trop grand\n","        if not positions:\n","            diff = 9999\n","        else:\n","            last_y = positions[-1]\n","            top_edge = last_y + text_height\n","            diff = abs(tolerance_max - top_edge)\n","\n","        if diff < best_diff:\n","            best_diff = diff\n","            best_sf = sf\n","            best_positions = positions\n","            # Si on est extrêmement proche de la limite, on arrête\n","            if diff < 1e-9:\n","                break\n","\n","    return best_sf, best_positions\n","\n","\n","def manual_tick_placement_continuous_Y(\n","    ax,\n","    ymin,\n","    ymax,\n","    spacing_factor_min=1.02,\n","    spacing_factor_max=1.2,\n","    step=0.001\n","):\n","    \"\"\"\n","    Place manuellement des pseudo-ticks pour l'axe Y dans [ymin..ymax].\n","    On coupe les ticks officiels et on dessine les labels via ax.text.\n","    \"\"\"\n","\n","    # 1) On suppose que vous avez une fonction qui calcule la hauteur\n","    #    d'un label en 'data coords' :\n","    text_height = compute_text_height_in_data_coords(ax)  # À adapter\n","\n","    # 2) Désactiver les ticks \"officiels\" de l'axe Y\n","    ax.set_yticks([])\n","    ax.set_ylim(ymin, ymax)\n","\n","    # 3) Recherche du spacing_factor optimal\n","    best_sf, _ = find_optimal_continuous_spacing_Y(\n","        ymin=ymin,\n","        ymax=ymax,\n","        text_height=text_height,\n","        spacing_factor_min=spacing_factor_min,\n","        spacing_factor_max=spacing_factor_max,\n","        step=step\n","    )\n","\n","    # 4) Placement effectif des positions optimisées\n","    positions = calc_positions_for_continuous_spacing_Y(\n","        ymin=ymin,\n","        ymax=ymax,\n","        text_height=text_height,\n","        spacing_factor=best_sf\n","    )\n","\n","    # 5) Création d'un offset transform pour décaler légèrement le texte\n","    #    vers la gauche (x<0) ou la droite (x>0) en points\n","    offset_axes_transform = mtransforms.offset_copy(\n","        ax.transAxes,\n","        fig=ax.figure,\n","        x=-2.0,   # Décalage à gauche en points (ajustez selon vos besoins)\n","        y=0,\n","        units='points'\n","    )\n","\n","    # Récupération pour la conversion data -> coords Axe\n","    y_min, y_max = ax.get_ylim()\n","\n","    # 6) Dessin de chaque label\n","    for y_val in positions:\n","        # Convertir la coordonnée data -> coordonnée \"Axes\" (entre 0 et 1)\n","        y_val_axes = (y_val - y_min) / (y_max - y_min)\n","\n","        label_str = f\"{y_val:.3f}\"  # Format de l'étiquette (à adapter si besoin)\n","\n","        ax.text(\n","            0.0,               # On place le texte \"à gauche\" du subplot\n","            y_val_axes,\n","            label_str,\n","            rotation=0,        # On peut mettre 0 ou toute autre rotation\n","            rotation_mode='anchor',\n","            ha='right',        # Alignement horizontal à droite\n","            va='center',       # Alignement vertical centré\n","            transform=offset_axes_transform,\n","            bbox=dict(facecolor='white', edgecolor='none', boxstyle='round,pad=0.0')\n","        )\n","\n","    return best_sf"]},{"cell_type":"code","source":["def create_box_plots(group_column=None):\n","    # Ce dictionnaire contiendra pour chaque \"n_components\" (nombre de topics),\n","    # la distribution des scores de chaque topic par journal\n","    distri_topics_by_journal_by_num_topic = {}\n","\n","    # Parcours de chaque nombre de composantes (chaque clé de all_nmf_W)\n","    for n_components, W_matrix in all_nmf_W.items():\n","        # Initialiser un dictionnaire pour stocker la distribution des sujets par journal pour ce n_components\n","        distri_topics_by_journal = {}\n","\n","        # W_matrix est une matrice de taille (nb_articles, n_components).\n","        # num_article correspond ici à l'index de la ligne (document) dans la matrice.\n","        for num_article, row_values in enumerate(W_matrix):\n","            # Récupération du \"journal\" selon la source\n","            if source_type == 'europresse':\n","                header = all_soups[num_article].header\n","                journal_text = extract_information(header, '.rdp__DocPublicationName')\n","                journal_text = normalize_journal(journal_text)\n","\n","            elif source_type == 'istex':\n","                journal_text = columns_dict['journal'][num_article]\n","\n","            elif source_type == 'csv':\n","                # Vérification de l'existence de la colonne\n","                if group_column not in columns_dict:\n","                    print(f\"La colonne '{group_column}' n'a pas été trouvée dans le fichier CSV.\")\n","                    return\n","\n","                journal_text = columns_dict[group_column][num_article]\n","\n","            # row_values est un vecteur de scores de longueur n_components,\n","            # chaque \"topic\" est l'index dans ce vecteur.\n","            for topic, score in enumerate(row_values):\n","                # On initialise le sous-dictionnaire si nécessaire\n","                if topic not in distri_topics_by_journal:\n","                    distri_topics_by_journal[topic] = {}\n","\n","                if journal_text not in distri_topics_by_journal[topic]:\n","                    distri_topics_by_journal[topic][journal_text] = []\n","\n","                # Ajout du score dans la liste correspondant à ce journal et ce topic\n","                distri_topics_by_journal[topic][journal_text].append(score)\n","\n","        # On stocke ensuite cette distribution pour le n_components courant\n","        distri_topics_by_journal_by_num_topic[n_components] = distri_topics_by_journal\n","\n","    \"\"\"\n","    Remplace le test de Kruskal-Wallis par un test bootstrap sur les moyennes.\n","    \"\"\"\n","\n","    # Création du dossier principal\n","    if not os.path.exists(f\"{results_path}{base_name}_BOX_PLOTS/\"):\n","        os.makedirs(f\"{results_path}{base_name}_BOX_PLOTS/\")\n","\n","    for num_topic in distri_topics_by_journal_by_num_topic:\n","\n","        # Sous-dossier spécifique au nombre de topics\n","        if not os.path.exists(f\"{results_path}{base_name}_BOX_PLOTS/{base_name}_{num_topic}TC_BOX_PLOTS/\"):\n","            os.makedirs(f\"{results_path}{base_name}_BOX_PLOTS/{base_name}_{num_topic}TC_BOX_PLOTS/\")\n","\n","        for topic in tqdm(distri_topics_by_journal_by_num_topic[num_topic], desc=\"Processing topics\"):\n","            topic_data = distri_topics_by_journal_by_num_topic[num_topic][topic]\n","\n","            # Collecte des données de score pour chaque journal\n","            data = []\n","            journals = []  # Pour les étiquettes\n","            for journal, scores in topic_data.items():\n","                data.extend(scores)\n","                journals.extend([journal] * len(scores))\n","\n","            # Création d'un DataFrame pour Seaborn\n","            df = pd.DataFrame({'Journal': journals, 'Score': data})\n","\n","            # Filtrer les journaux (ceux qui ont au moins \"threshold\" valeurs)\n","            journal_counts = df['Journal'].value_counts()\n","            journals_to_keep = journal_counts[journal_counts >= threshold].index\n","            df = df[df['Journal'].isin(journals_to_keep)]\n","\n","            # Test bootstrap (si au moins 2 groupes)\n","            if len(set(df['Journal'])) < 2:\n","                print(\"Pas assez de groupes pour effectuer le test bootstrap pour ce sujet et topic\")\n","                continue\n","\n","            # Journaux uniques\n","            unique_journals = df['Journal'].unique()\n","\n","            # Calcul de la hauteur de la figure\n","            figure_height_inch = (len(unique_journals) * PX_PER_TOPIC) / DPI\n","\n","            # Figure avec un sous-axe par journal\n","            fig, axes = plt.subplots(len(unique_journals), 1,\n","                                     figsize=(FIGURE_WIDTH_INCH, figure_height_inch),\n","                                     dpi=DPI, sharex=True)\n","\n","            # On trie les journaux par moyenne\n","            mean_scores = df.groupby('Journal')['Score'].mean().sort_values(ascending=False)\n","            sorted_journals = mean_scores.index.tolist()\n","\n","            # Plot de chaque boxplot\n","            # Si un seul journal, axes n'est pas un array => on le transforme en liste\n","            if len(unique_journals) == 1:\n","                axes = [axes]\n","\n","            for i, journal in enumerate(sorted_journals):\n","                # Boxplot\n","                sns.boxplot(\n","                    x='Score',\n","                    data=df[df['Journal'] == journal],\n","                    ax=axes[i],\n","                    whis=[0, 100],\n","                    showmeans=True,\n","                    width=0.98,\n","                    meanprops={\n","                        'marker': '|',\n","                        'markeredgecolor': 'red',\n","                        'markeredgewidth': 5,\n","                        'markersize': 16\n","                    },\n","                    boxprops={\n","                        'facecolor': (0.0, 0.2, 0.8),\n","                        'edgecolor': (0.0, 0.2, 0.8)\n","                    },\n","                    medianprops={\n","                        'color': 'none',\n","                        'linewidth': 10\n","                    },\n","                    whiskerprops={\n","                        'color': 'black',\n","                        'linewidth': 2\n","                    },\n","                    capprops={\n","                        'color': 'none',\n","                        'linewidth': 0\n","                    },\n","                )\n","\n","                # Ajustement des x-lims\n","                axes[i].set_xlim(left=0, right=df['Score'].max())\n","\n","                # Masque l'axe X pour tous sauf le dernier\n","                if i < len(unique_journals) - 1:\n","                    axes[i].xaxis.set_visible(False)\n","                else:\n","                    # Placement manuel des ticks (exemple de fonction custom que vous aviez)\n","                    manual_tick_placement_continuous(\n","                        ax=axes[i],\n","                        xmin=0,\n","                        xmax=df['Score'].max(),\n","                        spacing_factor_min=1.02,\n","                        spacing_factor_max=1.2,\n","                        step=0.001\n","                    )\n","\n","                # Retirer y-label et y-ticks\n","                axes[i].set_ylabel('')\n","                axes[i].set_yticks([])\n","                axes[i].set_xticks([])\n","\n","                offset_axes_transform = mtransforms.offset_copy(\n","                    axes[i].transAxes,\n","                    fig=axes[i].figure,\n","                    x=-3.0,\n","                    y=0.0,\n","                    units='points'\n","                )\n","\n","                # Petit label à gauche (nom du journal)\n","                axes[i].text(\n","                    0,\n","                    0.5,\n","                    journal,\n","                    ha='right',\n","                    va='center',\n","                    transform=offset_axes_transform\n","                )\n","\n","            sns.despine(left=True, bottom=True)\n","\n","            # Sauvegarde de la figure\n","            plt.savefig(\n","                f\"{results_path}{base_name}_BOX_PLOTS/{base_name}_{num_topic}TC_BOX_PLOTS/\"\n","                f\"{base_name}_{num_topic}tc_{topic_labels_by_config[num_topic][topic]}_\"\n","                f\"{minimum_caracters_nb_by_document}minc_{maximum_caracters_nb_by_document}maxc_\"\n","                f\"{go_remove_duplicates}dup_{web_paper_differentiation}wp_\"\n","                f\"{threshold}thr_journals_boxplots.png\",\n","                bbox_inches='tight',\n","                pad_inches=0\n","            )\n","            plt.close()"],"metadata":{"id":"xi1SPlBnLB3A"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"}},"nbformat":4,"nbformat_minor":0}